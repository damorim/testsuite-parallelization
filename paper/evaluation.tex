\section{Evaluation}
\label{sec:eval}

%% We are interested in understanding the prevalence of time-consuming
%% test suites and main sources of execution cost. We want to understand
%% how the execution cost is distributed on test cases within a test
%% suite and how developers approach test execution. Based on that, we
%% study parallelization of testing frameworks and build systems.  More
%% precisely, we investigate how prevalent test parallelization is, the
%% potential for improving execution cost, issues of flakiness that
%% hinders the use of parallelization, and how to address those issues.
%% More specifically, we pose the following research questions:

%% The first research question addresses the prevalence of long-running
%% test suites. We are interested to know if costly test suites are
%% common in open-source projects.  The second research question
%% addresses the relationship of test cases and the overall execution
%% cost: we are interested to investigate how the execution time is
%% distributed among test cases.  In the third research question, we
%% investigate if developers consider low-level parallelism features
%% available out-of-the-box to amortize test execution (see
%% Section~\ref{sec:modes}). In addition, we want to identify what
%% configurations are often used and why they are more popular (if any).
%% The fourth research question addresses the impact of low-level
%% parallelism on test execution from projects in our sample set. We want
%% to identify subjects that already use test parallelization and compare
%% their performance in contrast to sequential execution. In addition, we
%% are interested in evaluating the performance of sequential test suites
%% with different parallelization settings.
%% Finally, the fifth research question discusses the limitations and
%% insights to overcome the pitfalls of parallelization.

%% \Comment{
%%     \Fix{distribution of execution time per test case. For each subject
%%     identified in the first research question, we investigate how
%%     balanced is the cost of the test suite in contrast to the cost of
%%     test cases and if there are subjects where the time cost is mostly
%%     dominated by a small fraction of test cases.} \Fix{The third research
%%     question addresses the distribution of regression tests according
%%     to the use of computational resources.  We are interested in
%%     investigating if regression test suites are CPU intensive and if there
%%     are opportunities to improve performance. The RQ4 addresses}
%%     \Fix{...elaborate...}

%%     The rationale is that if the time cost of a regression test is equally
%%     distributed among test cases, the execution cost could be potentially
%%     improved by running tests in parallel (in contrast to the scenario
%%     where only one test case dominates most of the execution time).
%% }

We pose the following research questions, organized by the dimensions
of analysis we presented in Section~\ref{sec:intro}.

\newcommand{\numRQA}{RQ1}
\newcommand{\numRQB}{RQ2}
\newcommand{\numRQD}{RQ3}
\newcommand{\numRQC}{RQ4}
\newcommand{\numRQE}{RQ5}
\newcommand{\numRQF}{RQ6}

\newcommand{\RQA}{How prevalent is the occurence of time-consuming
  test suites\Comment{ in open-source projects}?}
\newcommand{\RQB}{How time is distributed across test cases?}
\newcommand{\RQC}{How prevalent is the use of test suite
  parallelization\Comment{ in open-source projects}?}
\newcommand{\RQD}{What are the speedups obtained with parallelization
  (in projects that actually use it) on cost?}
\newcommand{\RQE}{What are the main reasons that prevent developers
  from using test suite parallelization?}
\newcommand{\RQF}{To which extent parallel execution configurations affect testing
  cost and flakiness?}

\begin{itemize}
\item Potential
  \begin{itemize}
  \item \textbf{\numRQA.} \RQA
  \item \textbf{\numRQB.} \RQB    
  \end{itemize}
\item Effectiveness
  \begin{itemize}
  \item \textbf{\numRQD.} \RQD
  \end{itemize}  
\item Adoption
  \begin{itemize}
  \item \textbf{\numRQC.} \RQC    
  \item \textbf{\numRQE.} \RQE
  \end{itemize}
\item Tradeoff
  \begin{itemize}
  \item \textbf{\numRQF.} \RQF    
  \end{itemize}          
\end{itemize}

%%\newcommand{\RQB}{What is the distribution of CPU and IO bound
%%regression test suites from the sample set?}
%%
%%\newcommand{\RQC}{How uniformly distributed is the execution time
%%across test cases in costly projects?}
%%
%%\newcommand{\RQD}{How often developers use the parallelism features
%%from build systems to improve runtime performance?}


\subsection{Subjects}
\label{sec:subjects}

%% We evaluated the characteristics of test suites in open-source
%% development from a sample set of Java projects from \github{}.  We are
%% interested to evaluate non-trivial test suites from popular projects
%% that are in activity.

%% , which is used on
%% \github{} to indicate appreciation of a user to a
%% project~\cite{github-stars},

We used \github{}'s search API~\cite{githubsearch} to identify
projects that satisfy the following criteria: (1) the primary language
is Java\footnote{In case of projects in multiple languages, the
  \github{} API considers the predominant language as the primary
  language.}, (2) the project has at least 100 stars, (3) the latest
update was on or after January 1st, 2016, and (4) the \emph{readme}
file contains the string \emph{mvn}\footnote{As of now, \github{}'s
  search API can only reflect contents from README file (not other
  code elements).}.  We focused on Java for its popularity.  Although
there is no clearcut limit on number of \github{}
stars~\cite{github-stars} to define an interesting project, we
observed that 100 was enough to eliminate trivial subjects. The third
criteria is a constraint to skip projects without recent activity. The
fourth criteria is an approximation to find Maven projects. The
rationale is that if the string \emph{mvn} exists in the \emph{readme}
file, it may represent a Maven call (\eg, to compile or to test the
project). We used Maven as a reference due to its popularity on Java
projects and to assist one of our experiments. A Maven project may
contain several sub-modules with multiple \pomf{} files but we only
considered projects with a \pomf{} file located in the root directory.
Figure~\ref{fig:subject-query} illustrates the query to the \github{}
API as an HTTP request.

\vspace{1ex}
\begin{figure}[h!]
\centering
\scriptsize
\lstset{
    escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c, numberstyle=\tiny,
    deletekeywords={true}
}
\begin{lstlisting}
https://api.github.com/search/repositories?q=language:java
        +stars:>=100+pushed:>=2016-01-01
        +mvn%20in:readme+sort:stars
\end{lstlisting}
    \caption{\label{fig:subject-query} Query to the \github{} API for
    projects with the following criteria: (1) Java, (2) at least 100
    stars, (3) updated on January 1st, 2016 (or later), (4) contains
    the string \emph{mvn} in the \emph{readme} file. Output is
    paginated in descending order of stars.}
\end{figure}

As of March 25th 2017, our search criteria returned \SubjectsGithub{}
subjects. Figure~\ref{fig:subjects} summarizes our sample set. From
\SubjectsGithub{} downloaded projects, \SubjectsGithubNotMaven{}
projects were not Maven or did not have a \pomf{} in the root
directory and \SubjectsGithubNotTestable{} projects were untestable
(because of missing dependencies or incompatible testing environment,
for example). To ensure that our sample set was stable, we retested
the remaining subjects to eliminate projects with flaky tests (total of
\SubjectsGithubFlaky{} subjects). Our final set consists of
\numSubjs{} testable subjects.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{plots/subjs.pdf}
    \caption{\label{fig:subjects}We fetched \SubjectsGithub{} popular
    projects hosted on \github{}. From this initial sample, we ignored
    \SubjectsGithubNotMaven{} projects without Maven support,
    \SubjectsGithubNotTestable{} untestable projects, and
    \SubjectsGithubFlaky{} projects with flaky tests. We considered
    \numSubjs{} projects to conduct our study.}
\end{figure}

\subsection{Setup and Replication}
\label{sec:setup}

To run our experiments, we used a Core i7-4790 (3.60 GHz) Intel
processor machine with eight virtual CPUs (four cores with two native
threads each) and 16GB of memory, running Ubuntu 14.04 LTS Trusty Tahr
(64-bit version).  Software settings include \Comment{the Linux
  \emph{sysstat} package to measure performance, }git to fetch
subjects, Java 8, and Maven 3.3.9 to build and test subjects. We used
Python 3.4, Bash, R and Ruby 2.3 to process the data and generate plots.  All
source artifacts are publicly available for replication (on
request)\Comment{ at \Fix{create gh-pages}}.  This includes supporting
scripts (\eg, the script that test subjects and generates raw analysis
data) and the full list of projects. \Comment{, and a
  \emph{Vagrantfile} to emulate our hardware and all software
  dependencies.}

\subsection{Potential}
\label{sec:rqA}
\label{sec:rqB}

\begin{itemize}
    \item \textbf{\RQA}
\end{itemize}
%\Jbc{The following steps may change $\rightarrow$}

To evaluate prevalence of projects with costly test suites, we
considered the \numSubjs{} testable subjects from
Figure~\ref{fig:subjects}.  Figure~\ref{fig:mvn-execution} illustrates
the script we used to measure time.  The main loop (lines 5-11)
iterates over the list of subjects and invokes Maven multiple
times\Comment{ to isolate cost of running tests} (lines 7-9).  We
first compiled the source and test files (line 7), made all
dependencies available locally (line 8), and then we ran the tests in
offline mode to skip the package update task (line 9). After
execution, we used a regular expression on the output log to extract
the elapsed time (line 10).

Prior to running this script, we inspected Maven execution logs from a
sample inclduing a hundred projects.  The goal of this inspection step
is to find tasks unrelated to testing (\eg, \emph{javadoc} generation
and static analyses) that could interfere in our measurements.  The
tasks we found were ignored from execution (lines 1-3).  Furthermore,
to avoid noise from operating system events, we configured our
workstation to run only essential services.  The machine was dedicated
to our experiments and we accessed it via SSH. In addition, we
configured the \CodeIn{isolcpus} option from the Linux Kernel
\cite{linux-kernel} to isolate six virtual CPUs to execute our
experiment, leaving the remaining CPUs to run OS
processes~\cite{isolcpus-use}.  The rationale for this decision is to
prevent context-switching between user processes (running the
experiment) and OS-related processes.  Finally, to make sure our
measurements were fair, we compared the sequential execution of tests
using Maven with JUnit's default \CodeIn{JUnitCore} runner, invoked
from the command line.  We observed that results were very similar.

%% We executed each project's test suite for
%% \Fix{5} times through Maven and directly through JUnit each project

\input{codes/evaluation}

We ran the test suite for each subject three times, reporting averaged
execution times in three ranges: tests that run within a minute
(\shortg{} group), tests that run in one to five minutes (\medg{}
group), and tests that run in five or more minutes (\longg{}
group). We followed a similar methodology to group projects by time as
Gligoric~\etal{}~\cite{gligoric-etal-issta2015} in their work on
regression test selection.\Comment{ and added the \medg{} group due to
  the variability of the time cost from subjects out of the \shortg{}
  group} Figure~\ref{fig:rq1-barplot} shows the number of projects in
each group.  As it is to be expected, \longg{} and \medg{} projects do
not occur as frequently as \shortg{} projects.  However, they do ocur
in relatively high numbers.  Figure~\ref{fig:rq1-boxplot} shows cost
distribution of test suites in each group as boxplots.  The
distribution associated with the \shortg{} group is the most
unbalanced (right skewed)\Comment{ with outliers closed to the \medg{}
  group}.  The test suites in this group run in 15 or less seconds for
over 75\% of the cases.  Such scenarios constitute the majority of the
cases we analyzed.  Considering the groups \medg{} and \longg{},
however, we found many costly executions.  Nearly 75\% of the projects
from the \medg{} group take over 3.5 minutes to run and nearly 75\% of
the projects from the \longg{} group take $\sim$20 minutes to run.  We
found cases in the \longg{} group were execution takes over 50 minutes
to complete, as can be observed from the outliers (dots) in the plot.

%% the median from the
%% \medg{} group is nearly two minutes and most of the subjects run in
%% less than four minutes; most of the \longg{} group runs in less than
%% 25 minutes but has outliers that require more than 50 minutes to
%% execute.



\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.182\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/barplot-timecost.pdf}
        \caption{\label{fig:rq1-barplot}}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplot-timecost.pdf}
        \caption{\label{fig:rq1-boxplot}}
    \end{subfigure}%
    \caption{(a) Subjects grouped by time cost ($t$): short run ($t <
    1m$), medium run ($1m \le t < 5m$), and long run ($5m \le t$); (b)
    Distribution of time cost by group.}
\end{figure}

%% Figure~\ref{fig:rq1-barplot} is a lower bound estimation of cost
%% because some tests may finish earlier than expected due to existing
%% test failures in the revision we downloaded.

It is important to note that we under-estimated cost in our
experiments for two main reasons.  First, some tests may finish
earlier than expected due to observed test failures in some of the
revisions we downloaded.  From the \numSubjs{} testable projects,
\numSubjsPass{} successfully executed all tests and \numSubjsFail{}
reported some test failures.  Second, some projects may omit
long-running tests on their default execution. For instance, the
project \CodeIn{apache.maven-surefire} runs all unit tests in a few
seconds in our environment.  According to our criteria, this project
would be classified as \shortg{} but we noticed that only smoke test
run by default.  The more expensive integration and system tests are
only accessible via custom parameters.  When running the test suite
with integration tests, the testing time goes to nearly 30 minutes.
For simplicity, we considered only the tests executed by default.

\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQA{}:}~\emph{We conclude that
      time-consuming test suites are relatively frequent in
      open-source projects.  We found that \percentMedLongRunning{} of
      the \numSubjs{} projects we analyzed (\ie{}, over 1 in every 5
      projects) take at least 3 minutes to run and
      \percentLongRunning{} take at least 5 minutes to run.\Comment{
        (\ie, \numMedLong{} projects from \medg{} and \longg{}).}}
\end{minipage}
}
\end{center}


\vspace{1ex}
\begin{itemize}
    \item \textbf{\RQB}
\end{itemize}

Section~\ref{sec:rqA} showed that medium and long-running projects are
not uncommon; they account to nearly \percentMedLongRunning{} of the
\numSubjs{} projects we analyzed.  However, parallelism by itself does
not assure speedups on those projects.  One sensible factor that is
important to the effectiveness of low-level parallelism is the
distribution of test costs in the test suite.  In the limit, if cost
is dominated by a single test, it is unlikely that parallelization
will be beneficial as a test method is the smallest working unit in
test frameworks (see Section~\ref{sec:frameworks}).

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-long.pdf}
      \caption{\label{fig:longtcost}Long group.}
    \end{subfigure}\\
    \vspace{2ex}
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-medium.pdf}
      \caption{\label{fig:medtcost}Medium group.}
    \end{subfigure}\\
    \vspace{2ex}
    \begin{subfigure}{0.15\textwidth}
      \centering
      \includegraphics[width=.85\textwidth]{plots/boxplots-testcases.pdf}
        \caption{\label{fig:medtcost}Distribution of test cases.}
    \end{subfigure}~
    \begin{subfigure}{0.3\textwidth}
      \centering
      \includegraphics[width=.95\textwidth]{plots/scatter-testcost.pdf}
        \caption{\label{fig:scattercost}\Fix{legend}}
     \end{subfigure}\\
    \vspace{2ex}
    \begin{subfigure}{0.5\textwidth}
      \centering
      \begin{tabular}{rrrr}
        \toprule
        & $\sigma\leq1$ & $1<\sigma\leq5$ & $\sigma\ge5$ \\
        \midrule    
        Long   &  7 & 15 & 12 \\
        Medium & 22 & 19 & 7 \\
        \bottomrule
      \end{tabular}
      \caption{\label{fig:sd}Standard deviantion of test cases}
    \end{subfigure}%
    \caption{\label{fig:time-distributions}Distribution of test case
    execution cost per project.}%
\end{figure}

\sloppy Figures~\ref{fig:longtcost} and~\ref{fig:medtcost} show the
time distribution of tests per project.  We observed that the average
median value of execution cost for a test was relatively small (dashed
horizontal red lines), namely 0.31s for \medg{} projects and
0.23s for \longg{} projects.  Note that correlation between cost
of test suites and cost of their individual tests is not to be
expected.  The standard deviation ($\sigma$) associated with each
distribution was relatively low.  Figure~\ref{fig:sd} shows the number
of projects within specific ranges of $\sigma$ values.  We also
observed that projects with very high standard deviations may or may
not be indicative of monopolization of execution by a small number of
tests.  For example, the highest value of $\sigma$ occurred in project
\CodeIn{uber\_chaperone}, whose test suite was classified as
medium-running.  This project contains only \Fix{65} tests, \Fix{62} of
which take less than \Fix{0.5s} to run, \Fix{1} of which takes nearly
\Fix{3s} to run, and \Fix{2} of which take \Fix{40m} to run.  This is
an example where test suite parallelization may not be very beneficial
as \Fix{$>$99\%} execution cost is dominated by only \Fix{3\%} of the
tests.  Without these two tests this project would have been
classified as short-running.  A closer inspection in the data
indicates that the project \CodeIn{uber\_chaperone} was an extreme case;
we did not find projects with similar characteristics of time
monopolization.  Project \CodeIn{facebookarchive\_linkbench} is
classified as long-running and has the second highest value of
$\sigma$.  For this project, however, cost is distributed more
smoothly across \Fix{529} tests, of which \Fix{119} take more than
\Fix{1s} to run.  

\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQB{}:}~\emph{Overall, results indicate that
    projects with a very small number of tests monopolizing end-to-end
    execution time were rare.}
\end{minipage}
}
\end{center}

%% We are interested to know whether
%% most of the execution cost of a subject is dominated by a small subset
%% of test cases or if the cost is nearly equally distributed. 

%% We also evaluated the dispersion of time distributions (one
%% distribution per project) to answer research question \numRQB{}.  To
%% measure dispersion \emph{across} projects we used Relative Standard
%% Deviation (RSD)~\cite{everitt-book-stats-2010}.  Note that, if we were
%% to analyze each project in isolation, the standard deviation of a
%% distribution ($\sigma$) would suffice to quantify how dispersed the
%% (time) distribution is.  However, in our case, we would like to be
%% able to compare and summarize dispersion across projects.  The RSD,
%% which is obtained dividing the standard deviation by the mean ($\mu$)
%% of a distribution, provides such normalization effect.  This metric
%% provides a lower bound (zero) but not an upper bound (somewhere close
%% to 1).  The smaller (larger) the value of RSD the more (less) uniform
%% the distribution is.  Consequently, the lower the value of RSD the
%% more parallelizable a test suite should be.



%% \begin{figure}[h!]
%%   \centering
%%   \includegraphics[width=0.5\textwidth]{R/testcost.pdf}  
%%   \caption{\label{fig:relativesd}Distribution of RSD ($\sigma/\mu$)
%%     across projects.}
%% \end{figure}

%% Figure~\ref{fig:relativesd} shows the distribution of RSD across
%% medium and long-running projects.  Results show that the distribution
%% is skewed to the right indicating that test costs are relatively well
%% distributed in most costly projects we analyzed \Fix{$\leftarrow$
%%   confirm}.

%% analyzed the execution time
%% for the \numMedLong{} projects from the \longg{} and \medg{} groups
%% (see Section~\ref{sec:rqA}).
%% For each subject we calculated the
%% relative standard deviation of the test cases: we collected the
%% elapsed time of each individual test, calculated the standard
%% deviation, and divided by the mean. \Jbc{I need to clarify the
%%   relationship "well/bad-balanced" regression test and relative
%%   standard deviation}

%% Results indicated that \Fix{...elaborate...}. \Jbc{We may identify
%% different groups of subjects}\Fix{TODO: collect data + compute the
%% statistic, create a scatter plot to identify groups of subjects}

%% \Jbc{I shouldn't introduce parallelization arguments here but we have
%% to address this at some point:
%% Regression tests that are well distributed may benefit from
%% parallelism since more tests executes at the same time while the
%% opposite scenario may require a different approach. In the later
%% scenario, executing tests in parallel may have insignificant impact
%% since a small subset of test cases dominates the execution.}

\subsection{Effectiveness}
\label{sec:rqD}

\begin{itemize}
    \item \textbf{\RQD}
\end{itemize}

To answer \numRQD{}, we considered \numProjectsPar{} subjects that we
tested and confirmed parallelization activated by default, and
compared their time cost with sequential execution. We are interested
in measuring the execution speedup (if any) when the parallelization
settings are activated.

%% To answer \numRQD{}, we considered the \numProjectsPar{} subjects with
%% parallelization settings enabled by default identified in \numRQC{}
%% and compared their time cost with sequential execution. We are
%% interested in measuring the execution speedup (if any) when the
%% parallelization settings are activated. It is important to mention
%% that we ignored the remaining \Fix{??} subjects identified in
%% Section~\ref{sec:rqC-2} because it would be impractical to execute
%% each subject on specific conditions to measure the impact of
%% parallelization.

To enforce sequential execution, we analyzed the \numProjectsPar{}
subjects looking for the parallelism configuration inside the \pomf{} 
of each module. To run the subjects sequentially, we manually modified 
the Maven Surefire configuration in order to force the subjects to
execute tests in sequential mode.

\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQD{}:}~\emph{\Jbc{summarize
      findings.....}}
  \end{minipage}
}
\end{center}

\subsection{Adoption}
\label{sec:rqC}
\label{sec:rqE}

\begin{itemize}
    \item \emph{\RQC}
\end{itemize}

To evaluate the support of parallel execution of test suites, we
considered the \numMedLong{} projects that ran in at least one minute
identified in Section~\ref{sec:rqA} (\ie, \medg{} and \longg{}
groups). We are interested to know if developers consider low-level
parallelism features to amortize the execution cost. More precisely,
we investigate if there are projects with parallelism configuration
enabled by default or if developers configure this feature to run only
on specific scenarios (\eg, dedicated server).  We describe our
approach as follows:


\subsubsection{Test execution with parallelism enabled by default}
\label{sec:rqC-1}

Initially, we configured Maven to output all configuration variables.
The rationale is that, for each project's module, we can capture
relevant configurations and classify their parallelization level (see
Section \ref{sec:modes}) based on their values. To avoid waiting all
tests to terminate, we just run the bootstrap phase and we skip the
actual test execution.

According to Maven's documentation~\cite{maven-surefire-plugin}, any
parallelization setting depends either on (1) \CodeIn{parallel} to
define the parallelism level within a JVM followed by
\CodeIn{threadCount} or (2) \CodeIn{forkCount} (or \CodeIn{forkMode}
in older versions) to define the number of forked JVMs. For each test
execution, we look for theses parameters and their values to classify
them.  \Jbc{Missing: explain how we mapped the configurations to
parallel level.} Figure~\ref{fig:freqmodes-dynamic} summarizes our
findings.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.32\textwidth]{plots/barplot-modes-dynamic.pdf}
    \caption{\label{fig:freqmodes-dynamic}\Fix{fix
    caption}Distribution of parallel modes identified dynamically in a
    subset of \numProjectsPar{} projects.  A project may have support
    to more than one parallel mode. Also, a project may run only a
    subset tests in parallel by default.}
\end{figure}

\Fix{Elaborate findings....}

\subsubsection{Support of parallelization configuration in build files}
\label{sec:rqC-2}

To extend our initial investigation, we mined several \pomf{} files
from all \numMedLong{} projects classified as the \medg{} and
\longg{}-running.
To reduce the number of build files
to inspect, we consider only \pomf{} files that contain
\CodeIn{plugin} tags with a child tag \CodeIn{artifactId} with the
value \CodeIn{maven-surefire-plugin} and a \CodeIn{configuration}
node. Figure~\ref{fig:surefire-configuration} shows an example of
configuration node mined. This mining step, however, is not sound as a matching project may not 
indicate a project with a valid configuration for running tests in parallel.

%False positive can happen because of comments, for instance.  
%To eliminate the cases of false positives and also to categorize 
%true positive cases, we complemented the initial mining step with a 
%manual inspection of files.
%% settings); the second step (inspection) consists in a manual
%% inspection to confirm the presence of parallelism settings in the
%% build file and classify them according to the parallelism level.
%% Figure \Fix{removed} describes the discovery step: we list the paths
%% of all build files and filter only the files that contain any of the

Figure~\ref{tab:inspection-table} summarizes our results. The first
column indicates the group of projects according to their time cost.
The second column indicates the number of build files per group. 
The last column indicates the ratio of projects with parallelization settings.

From the \numMedLong{} subjects, we found \pomMedLong{} \pomf{} files.
The \numPomMatched{} configurations are distributed across \numProjectsPar{} projects from our sample.
% 
% \emph{From these results we found that $\sim$51\% of medium and
% long-running projects do not use parallel features to run test
% suites.}\Mar{please make it consistent with research
% question}\Mar{explain this is over(under)-estimated...}

\begin{figure}[ht!]
    \centering
    \begin{tabular*}{0.48\textwidth}{@{\extracolsep{\fill}}ccc}
        \toprule
        \multirow{2}{*}{Group} %1st row, 1st cell
            & \multirow{2}{*}{\# \pomf{}}
	    & \# \pomf{} matched\\
        % 2nd row - empty cell
            & % empty cell
            & / total\\%
        \midrule%
	Long   & \numPomLong{} & 4 / \numLong{}\\%
	Medium & \numPomMed{} & 6 / \numMed{}\\%
        \midrule%
        Total % last row, first cell
            & \pomMedLong{}
            & \numProjectsPar{} / \numMedLong{}\\%
        \bottomrule%
    \end{tabular*}
    \caption{Presence of parallelization settings in build files: the
    first column indicates the group of projects according to their
    time cost; the second column is the subset of files with parallelization
    keywords; the last column indicates the ratio of projects with
    parallelism support.}
    \label{tab:inspection-table} 
\end{figure}

\Jbc{rework this... $\rightarrow$} From the \numProjectsPar{} projects
identified above, we investigated further the \numPomMatched{}
build files with parallel settings.  We analyzed the support and
distribution of parallel modes from this subset of projects. To
calculate the distribution of parallel modes, we considered only the
presence of the mode in at least one of the project settings.  Recall
that a build file may contain more than one parallel setting and a
project may contain several sub-modules with build files.  In case the
value of a parallel option is resolved dynamically (\eg, via
command-line argument or system variable) we compute all modes related
to the option. For instance, depending on the value, the
\CodeIn{parallel} option can be \Seq{} (\CodeIn{none}),
\ParClassSeqMeth{} (\CodeIn{classes}), \SeqClassParMeth{},
(\CodeIn{methods}), and \ParClassParMeth{} (\CodeIn{all}).
Figure~\ref{fig:freqmodes-static} summarizes our findings.
\Fix{Missing conclusion: Fork the most used configuration}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.32\textwidth]{plots/barplot-modes-static.pdf}
	\caption{\label{fig:freqmodes-static}\Luis{This is wrong, it
	should be \textbf{CF0} instead of \textbf{CL0}}Distribution of parallel modes
    identified statically in a subset of \numProjectsPar{} projects.
    A project may have support to more than one parallel mode.}
\end{figure}

\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQC{}:}~\emph{Overall, results indicate
      that parallelism features are underused by defailt given that only
	  \percentParallel{}
      of projects supports parallel execution of test suites without
	  any extra parameter.}
  \end{minipage}
}
\end{center}

\begin{itemize}
	\item \textbf{\RQE{}}
\end{itemize}

To understand why developers prefer not use parallel test execution,
we considered the \numNonParallel{} subjects that do not use any
parallel mode from Section~\ref{sec:rqC-1} (\ie{}, \medg{} and
\longg{}
projects that does not run in parallel by default) and elaborated a
quick survey to extent the comprehension about why developers do not
use parallel configuration. \Luis{describe how we created the survey}.
We first retrieve the last 20 commits from the project repository and
got the developers e-mails. In total, we sent \emailsSent{} e-mails to
different developers that recently contributed to the project.
There were \emailsAnswered{} replies, and \emailsTrueAnswers{} of
those were answering the survey.

The survey sent was composed by three open questions and one closed
question: (I) How long does it take for test to run in your
environment? Can you briefly define your setup? (II) Do you confirm
that your project does not run in parallel? (III) Select a reason for
not using parallelization: (a) I did not know it was possible; (b) I
was concerned with concurrency issues; (c) I use a continuous
integration server; (d) Some other reason. Please elaborate.

\Comment{
\begin{enumerate}
	\item How long does it take for test to run in your
		environment?
	\item Can you briefly define your setup?
	\item Do you confirm that your project does not run in
		parallel?
	\item Select a reason for not using paralellization:
		\begin{enumarate}
			\item I did not know it was possible;
			\item I was concerned with concurrency issues;
			\item I use a continuous integration server;
			\item Some other reason.
		\end{enumerate}
\end{enumerate}
}
\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQE{}:~\emph{\Luis{summarize
	  findings...}}}
  \end{minipage}
}
\end{center}


\Luis{these values need macro $\rightarrow$}
From the valid replies, we noticed that \Fix{55.2\%
(16/\emailsTrueAnswers{})} of developers use an external continuous
integration server to run tests, \Fix{27.6\% (8/\emailsTrueAnswers{})} run tests in the
development machine, \Fix{13.8\% (4/\emailsTrueAnswers{})} did
not specified the environment.


%%To evaluate the distribution of execution time per project, we sorted
%%the test cases by decreasing order of elapsed time and calculated the
%%number of tests executed in 90\% of the total time. Later, we reported
%%the \Fix{balance} of execution time by dividing the number of tests
%%that represents 90\% of the execution time by the number of tests
%%cases. For instance, a balance of 50\% indicates \Fix{...}.  \Fix{We
%%collected the elapsed time from test cases for each generated report.
%%Maven Surefire generates an XML report with execution information
%%(\eg, number of skipped tests and elapsed time) per test suite
%%\Jbc{Should I use the previous sentence as a footnote or should I
%%delete it?}. We noticed that some test cases reported an elapsed time
%%of zero: since the reported time is in milliseconds, some tests may
%%execute in a shorter time. \Fix{..to be continued...}}. Results
%%indicate that \Fix{...}.
%%
%%\begin{figure}[h!]
%%    \centering
%%    \includegraphics[width=0.4\textwidth]{results/plots/balance.pdf}
%%    \caption{\Fix{balance}}
%%\end{figure}

%% \subsection{Answering research question RQ3}
%% \label{sec:rqThree}
%% 
%% \begin{itemize}
%%     \item \RQB
%% \end{itemize}
%% 
%% To evaluate the distribution of CPU and IO intensive test suites from
%% the sample set, we used the command \emph{sar} to monitor the system
%% activity in background while tests ran. \emph{Sar} is a command that
%% collects and reports statistics (\eg, percentage of IO waiting and
%% usage of CPU in user mode) based on the kernel activity and it is
%% highly configurable to collect detailed information (\eg, usage of a
%% specific processor core or percentage of network interface
%% utilization). We configured \emph{sar} to report \Fix{...explain how
%% we executed and what fields we are interested}. \Fix{explain fields}.
%% Figure \Fix{A} shows the distribution of subjects grouped in intervals
%% of \Fix{W}\% of CPU utilization. Results indicates that \Fix{...}
%% 
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.4\textwidth]{results/plots/cpuness.pdf}
%%     \caption{\Fix{cpu usage}}
%% \end{figure}
%% 
%% \Comment{we proposed the definition of \emph{cpuness}
%% computed as the follow: $((user\_t + system\_t) / elapsed\_t) * 100$,
%% where \emph{user\_t} is the elapsed time of execution in \emph{user
%% mode}, \emph{system\_t} is the elapsed in \emph{kernel mode}, and
%% \emph{elapsed\_t} is the elapsed time to finish the execution. We
%% measured the \emph{cpuness} of each regression test \Fix{...elaborate
%% the meaning of cpuness} \Fix{Describe how I measured user, system and
%% "wall" time}.  \Fix{Explain results}.  \Fix{show plots}}
%% 
