\section{Evaluation}
\label{sec:eval}

We are interested in understanding the prevalence of time-consuming
test suites and main sources of cost. We want to understand how the
execution cost is distributed on test cases withing a test suite and
how developers approach test execution. Based on that, we study
parallelization of testing frameworks.  More precisely, we investigate
how prevalent test parallelization is, the potential for improving
execution cost, issues of flakiness that hinders the use of
parallelization, and how to address those issues.  More specifically,
we pose the following research questions:

\newcommand{\RQA}{How prevalent is the occurence of time-consuming
regression test suites in open-source projects?}
\newcommand{\RQB}{How the time cost from a test suite is distributed
among the test cases?}
\newcommand{\RQC}{How often parallelization features appear in
build files?}
\newcommand{\RQD}{What is the impact of parallelization?}
\newcommand{\RQE}{What factors contribute to improve performance
through parallelization?\Jbc{Previously as "When does parallelization
works and when it does not so great?"}}
\newcommand{\RQF}{What are the limitations of parallelization?}

\newcommand{\rqA}{\textbf{RQ1.} \RQA}
\newcommand{\rqB}{\textbf{RQ2.} \RQB}
\newcommand{\rqC}{\textbf{RQ3.} \RQC}
\newcommand{\rqD}{\textbf{RQ4.} \RQD}
\newcommand{\rqE}{\textbf{RQ5.} \RQE}
\newcommand{\rqF}{\textbf{RQ6.} \RQF}

\begin{itemize}
    \item \rqA
    \item \rqB
    \item \rqC
    \item \rqD
    \item \rqE
    \item \rqF
\end{itemize}

%%\newcommand{\RQB}{What is the distribution of CPU and IO bound
%%regression test suites from the sample set?}
%%
%%\newcommand{\RQC}{How uniformly distributed is the execution time
%%across test cases in costly projects?}
%%
%%\newcommand{\RQD}{How often developers use the parallelism features
%%from build systems to improve runtime performance?}

The first research question addresses the prevalence of long-running
test suites. We are interested to know if costly test suites are
common in open-source projects. The second research question addresses
the balance of regression tests: we want to investigate how the time
cost is distributed among test cases. The third
research question addresses our interest in understanding if
developers are aware of parallelization features from testing
frameworks available out-of-the-box through build systems (\eg, run
JUnit test cases in parallel using Maven Surefire) and what settings
are more often used.  The fourth research question addresses the
impact of parallelism on the regression tests from our sample set. For
subjects that use parallelization settings, we are interested in
compare the execution performance when these settings are activated
and deactivated (\ie, sequential execution) to evaluate the benefits.
\Jbc{This RQ has to be reworked:} \Fix{The fifth research question
addresses the characteristics of regression tests from the previous
experiment.  More specifically, we want to investigate if there is a
relation between the balance of test execution (\ie, how uniformly
distributed is the execution time across tests cases) and the usage of
computational resources (\ie, if tests are mostly CPU or IO intense)
that impacts the effectiveness of parallelization}.  Finally, the
sixth research question discusses the limitations and insights to
overcome the pitfalls of parallelization.

\Comment{
    \Fix{distribution of execution time per test case. For each subject
    identified in the first research question, we investigate how
    balanced is the cost of the test suite in contrast to the cost of
    test cases and if there are subjects where the time cost is mostly
    dominated by a small fraction of test cases.} \Fix{The third research
    question addresses the distribution of regression tests according
    to the use of computational resources.  We are interested in
    investigating if regression test suites are CPU intensive and if there
    are opportunities to improve performance. The RQ4 addresses}
    \Fix{...elaborate...}

    The rationale is that if the time cost of a regression test is equally
    distributed among test cases, the execution cost could be potentially
    improved by running tests in parallel (in contrast to the scenario
    where only one test case dominates most of the execution time).
}

\subsection{Subjects}
\label{sec:subjects}

We evaluated the characteristics of regression tests in open-source
development from a sample set of Java projects from \github{}.  We are
interested to evaluate non-trivial test suites on popular projects
that are in activity. We used the \github{}'s search
API~\cite{githubsearch} to fetch Java projects according to the
following criteria: (1) the primary language must be Java\footnote{In
case of projects in multiple languages, the \github{} API considers
the predominant language as the primary language.}; (2) the project
has at least 100 stars; (3) latest update on (or after) January 1st,
2016; (4) the \emph{readme} file contains the string \emph{mvn}.

On \github{}, when a user adds a star to a project, s/he demonstrated
appreciation and bookmarked it for later
reference~\cite{github-stars}.  Although there is not a specific range
for the number of stars, our criteria is an estimation to avoid
trivial subjects: we assumed that \github{} users are likely to
demonstrate interest on well-tested projects. The third criteria is a
constraint to skip projects without recent activity. The fourth
criteria is an approximation to find projects with Maven support. The
rationale is that if the string \emph{mvn} exists in the \emph{readme}
file, it may represent a Maven call (\eg, to compile or test the
project). We used Maven as a reference due to its popularity on Java
projects and to automate our evaluation scripts.
Figure~\ref{fig:subject-query} illustrates the query as an HTTP
request.

\begin{figure}[h!]
\centering
\scriptsize
\lstset{
    escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c, numberstyle=\tiny,
    deletekeywords={true}
}
\begin{lstlisting}
https://api.github.com/search/repositories?q=
    stars:>=100+language:java+mvn%20in:readme+
    pushed:>=2016-01-01+sort:stars

\end{lstlisting}
    \caption{\label{fig:subject-query} Query to the \github{} API for
    projects with the following criteria: (1) Java, (2) at least 100
    stars, (3) updated on January 1st, 2016 (or later), (4) contains
    the string \emph{mvn} in the \emph{readme} file. Output is
    paginated in descending order of stars.}
\end{figure}

A Maven project may contain several submodules with multiple
\emph{pom.xml} files. We considered only projects with a
\emph{pom.xml} file located in the root directory.  As of December
30th 2016, our search criteria returned 615 subjects. From the 615
downloaded projects, 46 were not Maven projects and 159 were in a
unstable revision. From these 159 unstable subjects, \Jbc{elaborate
what is unstable (eg, (1) how many could not resolve dependencies? (2)
how many could not compile sources and tests? (3) could not run tests
due to incompatible testing environment?}. Our final set consists in
\numSubjs{} testable subjects.

\subsection{Setup and Replication}
\label{sec:setup}

To run our experiments, we used a \emph{Core i7-4790} (3.60 GHz) Intel
processor machine with 8 virtual CPUs (4 cores with 2 native threads
each) and 16GB of memory, running \emph{Ubuntu 14.04 LTS Trusty Tahr}
(64-bit version). Software settings include \Comment{the Linux
\emph{sysstat} package to measure performance, }\emph{git} to fetch
subjects, \emph{Java 8} and \emph{Maven 3.3.9} to build and test
subjects. Our evaluation scripts depends on \emph{Python 3.4} and
\emph{Bash} to execute.  For replication, all source artifacts are
publicly available at \Fix{create gh-pages}, including supporting
scripts (\eg, the script that test subjects and generates the raw
data), and the full list of projects. \Comment{, and a
\emph{Vagrantfile} to emulate our hardware and all software
dependencies.}

\subsection{Answering research question RQ1}
\label{sec:rqA}

\begin{itemize}
    \item \emph{\RQA}
\end{itemize}

To evaluate the frequency of time-consuming regression tests, we
considered the \numSubjs{} subjects from Section~\ref{sec:subjects} and
compared the elapsed time to run their tests.
Figure~\ref{fig:mvn-execution} shows the commands used in our script
to test each subject. The main loop (lines 5-11) iterates over the
list of subjects and invokes Maven in separated steps (lines 7-9). To
avoid inflating the measured time we executed Maven in different
steps: we first compiled the source and test files (line 7), made all
dependencies available locally (line 8) and later, we ran the tests in
offline mode (line 9) to bypass package updates. After the execution,
we used a regular expression on the output log to extract the elapsed
time (line 10). Before collecting the time cost, we executed all
subjects and randomly selected 100 logs to inspect and identify
non-related tasks (\eg, \emph{javadoc} generation and static analysis)
to ignore during the experiment (lines 1-3).  To avoid noise from
operating system events, we used a dedicated server remotely via SSH
with the operating system running only essential services (\eg, the
SSH server). In addition, we \Fix{describe CPU isolation with Kernel
settings}.

\input{codes/evaluation}

\Jbc{Ensure to describe that: subjects that could not run the tests
due to incompatible testing environment were removed (likely to happen
for subjects that rely on database systems and other external
factors), the exit status for all executions are consistent and I
removed flaky projs}

We ran each subject's tests three times and grouped them according to
the time cost (in average): tests that run in less than a minute
(\emph{short} group), tests than run in one minute and less than five
minutes (\emph{medium} group), and tests that run in five (or more)
minutes (\emph{long} group). We based our classification from a
previous work \cite{gligoric-etal-issta2015} and added the
\emph{medium} group due to the variability of the time cost from
subjects out of the \emph{short} group. Figure~\ref{fig:rq1-boxplot}
shows the time cost distribution of each group: the \emph{short} group
is the most right skewed distribution with outliers closed to the
\emph{medium} group and 50\% of the subjects run in less than 15
seconds; the median from the \emph{medium} group is nearly two minutes
and most of the subjects run in less than four minutes; most of the
\emph{long} group runs in less than 25 minutes but has outliers that
require more than 50 minutes to execute. Figure~\ref{fig:rq1-barplot}
summarizes the proportion of subjects from each group.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.21\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/barplot-timecost.pdf}
        \caption{\label{fig:rq1-barplot}}
    \end{subfigure}%
    ~ 
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplot-timecost.pdf}
        \caption{\label{fig:rq1-boxplot}}
    \end{subfigure}%
    \caption{(a) Subjects grouped by time cost ($t$): short run ($t <
    1m$), medium run ($1m \le t < 5m$), and long run ($5m \le t$); (b)
    Distribution of time cost by group.}
\end{figure}

Given that 19.3\% of the projects (\ie, 61 projects) are costly,
we concluded that time-consuming regression test suites are relatively
frequent in open-source projects.  It is important to mention that
Figure~\ref{fig:rq1-barplot} represents a lower bound for time cost:
some tests may finish early due to a failure since the subjects were
tested in a potentially unstable revision. \Jbc{How many executed
completely? From the failed subjects, how much of the test suite was
executed?}

\subsection{Answering research question RQ2}
\label{sec:rqB}

\begin{itemize}
    \item \emph{\RQB}
\end{itemize}

We analyzed the distribution of time execution to investigate if most
of the cost is dominated by a small subset of test cases or if the
cost is nearly equally distributed. \Jbc{I shouldn't introduce
parallelization arguments here... May I just proceed to the
methodology?} \Fix{Regression tests that are well distributed may
benefit from parallelism since more tests executes at the same time
while the opposite scenario may require a different approach. In the
later scenario, executing tests in parallel may have insignificant
impact since a small subset of test cases dominates the execution.}

To evaluate how uniformly distribution the execution cost is among
test cases, we \Fix{...elaborate methodology...} 

Results indicated that \Fix{...elaborate...}. \Jbc{We may identify
different groups of subjects (\eg, test suites that are more equally
distributed than others) I think the results from this RQ should be
towards this direction}

\subsection{Answering research question RQ3}
\label{sec:rqC}

\begin{itemize}
    \item \emph{\RQC}
\end{itemize}

To evaluate the presence of parallelization settings, we considered
the \numSubjsRqTwo{} subjects from the groups \emph{long} and
\emph{medium} identified in Section~\ref{sec:rqA}. We focused on
these two groups because given that costly test suites exist, we
wanted to analyze if their maintainers are aware of parallelization
settings and how they use these settings.  We computed the
frequency of the parallelism modes (see Section \ref{sec:modes}) using
a semi-automatic approach: we first discover automatically the
relevant build files (\ie, \emph{pom.xml} files) and then we manually
inspect them to eliminate false-positives (\eg, commented
configuration).  Figure~\ref{fig:discovery-step} describes the
discovery step: first, we list the paths of all build file and then we
filter only the build files that contain any of the keywords,
\CodeIn{forkMode}, \CodeIn{forkCount}, or \CodeIn{parallel}. Notice
that, for Maven projects, any of these keywords exist in any possible
parallelism configuration \Fix{cite surefire doc}.

\input{codes/discovery}

From the \numSubjsRqTwo{} subjects, the discovery step checked \Fix{X}
files and yielded \Fix{Y} files to inspect (distributed in \Fix{A}
subjects). \Jbc{How many were valid configurations? Show a contingency
table summarizing results for Medium/Long when we have the results}
\Jbc{Show a histogram with the distribution of parallel modes}.

\subsection{Answering research question RQ4}
\label{sec:rqD}

\begin{itemize}
    \item \emph{\RQD}
\end{itemize}

\Jbc{describe}.\Jbc{relevance}. \Jbc{methodology}.

\Fix{---------------------}

%%To evaluate the distribution of execution time per project, we sorted
%%the test cases by decreasing order of elapsed time and calculated the
%%number of tests executed in 90\% of the total time. Later, we reported
%%the \Fix{balance} of execution time by dividing the number of tests
%%that represents 90\% of the execution time by the number of tests
%%cases. For instance, a balance of 50\% indicates \Fix{...}.  \Fix{We
%%collected the elapsed time from test cases for each generated report.
%%Maven Surefire generates an XML report with execution information
%%(\eg, number of skipped tests and elapsed time) per test suite
%%\Jbc{Should I use the previous sentence as a footnote or should I
%%delete it?}. We noticed that some test cases reported an elapsed time
%%of zero: since the reported time is in milliseconds, some tests may
%%execute in a shorter time. \Fix{..to be continued...}}. Results
%%indicate that \Fix{...}.
%%
%%\begin{figure}[h!]
%%    \centering
%%    \includegraphics[width=0.4\textwidth]{results/plots/balance.pdf}
%%    \caption{\Fix{balance}}
%%\end{figure}

%% \subsection{Answering research question RQ3}
%% \label{sec:rqThree}
%% 
%% \begin{itemize}
%%     \item \RQB
%% \end{itemize}
%% 
%% To evaluate the distribution of CPU and IO intensive test suites from
%% the sample set, we used the command \emph{sar} to monitor the system
%% activity in background while tests ran. \emph{Sar} is a command that
%% collects and reports statistics (\eg, percentage of IO waiting and
%% usage of CPU in user mode) based on the kernel activity and it is
%% highly configurable to collect detailed information (\eg, usage of a
%% specific processor core or percentage of network interface
%% utilization). We configured \emph{sar} to report \Fix{...explain how
%% we executed and what fields we are interested}. \Fix{explain fields}.
%% Figure \Fix{A} shows the distribution of subjects grouped in intervals
%% of \Fix{W}\% of CPU utilization. Results indicates that \Fix{...}
%% 
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.4\textwidth]{results/plots/cpuness.pdf}
%%     \caption{\Fix{cpu usage}}
%% \end{figure}
%% 
%% \Comment{we proposed the definition of \emph{cpuness}
%% computed as the follow: $((user\_t + system\_t) / elapsed\_t) * 100$,
%% where \emph{user\_t} is the elapsed time of execution in \emph{user
%% mode}, \emph{system\_t} is the elapsed in \emph{kernel mode}, and
%% \emph{elapsed\_t} is the elapsed time to finish the execution. We
%% measured the \emph{cpuness} of each regression test \Fix{...elaborate
%% the meaning of cpuness} \Fix{Describe how I measured user, system and
%% "wall" time}.  \Fix{Explain results}.  \Fix{show plots}}
%% 
