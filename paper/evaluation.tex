\section{Evaluation}
\label{sec:eval}

%% We are interested in understanding the prevalence of time-consuming
%% test suites and main sources of execution cost. We want to understand
%% how the execution cost is distributed on test cases within a test
%% suite and how developers approach test execution. Based on that, we
%% study parallelization of testing frameworks and build systems.  More
%% precisely, we investigate how prevalent test parallelization is, the
%% potential for improving execution cost, issues of flakiness that
%% hinders the use of parallelization, and how to address those issues.
%% More specifically, we pose the following research questions:

%% The first research question addresses the prevalence of long-running
%% test suites. We are interested to know if costly test suites are
%% common in open-source projects.  The second research question
%% addresses the relationship of test cases and the overall execution
%% cost: we are interested to investigate how the execution time is
%% distributed among test cases.  In the third research question, we
%% investigate if developers consider low-level parallelism features
%% available out-of-the-box to amortize test execution (see
%% Section~\ref{sec:modes}). In addition, we want to identify what
%% configurations are often used and why they are more popular (if any).
%% The fourth research question addresses the impact of low-level
%% parallelism on test execution from projects in our sample set. We want
%% to identify subjects that already use test parallelization and compare
%% their performance in contrast to sequential execution. In addition, we
%% are interested in evaluating the performance of sequential test suites
%% with different parallelization settings.
%% Finally, the fifth research question discusses the limitations and
%% insights to overcome the pitfalls of parallelization.

%% \Comment{
%%     \Fix{distribution of execution time per test case. For each subject
%%     identified in the first research question, we investigate how
%%     balanced is the cost of the test suite in contrast to the cost of
%%     test cases and if there are subjects where the time cost is mostly
%%     dominated by a small fraction of test cases.} \Fix{The third research
%%     question addresses the distribution of regression tests according
%%     to the use of computational resources.  We are interested in
%%     investigating if regression test suites are CPU intensive and if there
%%     are opportunities to improve performance. The RQ4 addresses}
%%     \Fix{...elaborate...}

%%     The rationale is that if the time cost of a regression test is equally
%%     distributed among test cases, the execution cost could be potentially
%%     improved by running tests in parallel (in contrast to the scenario
%%     where only one test case dominates most of the execution time).
%% }

We pose the following research questions, organized by the dimensions
of analysis we presented in Section~\ref{sec:intro}.

\newcommand{\numRQA}{RQ1}
\newcommand{\numRQB}{RQ2}
\newcommand{\numRQD}{RQ3}
\newcommand{\numRQC}{RQ4}
\newcommand{\numRQE}{RQ5}
\newcommand{\numRQF}{RQ6}

\newcommand{\RQA}{How prevalent is the occurence of time-consuming
  test suites\Comment{ in open-source projects}?}
\newcommand{\RQB}{How time is distributed across test cases?}
\newcommand{\RQC}{How prevalent is the use of test suite
  parallelization\Comment{ in open-source projects}?}
\newcommand{\RQD}{What are the speedups obtained with parallelization
  (in projects that actually use it) on cost?}
\newcommand{\RQE}{What are the main reasons that prevent developers
  from using test suite parallelization?}
\newcommand{\RQF}{To which extent parallel execution configurations affect testing
  cost and flakiness?}

\begin{itemize}
\item Potential
  \begin{itemize}
  \item \textbf{\numRQA.} \RQA
  \item \textbf{\numRQB.} \RQB    
  \end{itemize}
\item Effectiveness
  \begin{itemize}
  \item \textbf{\numRQD.} \RQD
  \end{itemize}  
\item Adoption
  \begin{itemize}
  \item \textbf{\numRQC.} \RQC    
  \item \textbf{\numRQE.} \RQE
  \end{itemize}
\item Tradeoffs
  \begin{itemize}
  \item \textbf{\numRQF.} \RQF    
  \end{itemize}          
\end{itemize}

%%\newcommand{\RQB}{What is the distribution of CPU and IO bound
%%regression test suites from the sample set?}
%%
%%\newcommand{\RQC}{How uniformly distributed is the execution time
%%across test cases in costly projects?}
%%
%%\newcommand{\RQD}{How often developers use the parallelism features
%%from build systems to improve runtime performance?}


\subsection{Subjects}
\label{sec:subjects}

%% We evaluated the characteristics of test suites in open-source
%% development from a sample set of Java projects from \github{}.  We are
%% interested to evaluate non-trivial test suites from popular projects
%% that are in activity.

%% , which is used on
%% \github{} to indicate appreciation of a user to a
%% project~\cite{github-stars},

We used \github{}'s search API~\cite{githubsearch} to identify
projects that satisfy the following criteria: (1) the primary language
is Java\footnote{In case of projects in multiple languages, the
  \github{} API considers the predominant language as the primary
  language.}, (2) the project has at least 100 stars, (3) the latest
update was on or after January 1st, 2016, and (4) the \emph{readme}
file contains the string \emph{mvn}.  We focused on Java for its
popularity.  Although there is no clearcut limit on the number of
\github{} stars~\cite{github-stars} to define relevant projects, we
observed that 100 was enough to eliminate trivial subjects. The third
criteria serves to skip projects without recent activity. The fourth
criteria is an approximation to find Maven projects.\Comment{ The
  rationale is that if the string \emph{mvn} exists in the
  \emph{readme} file, it may represent a Maven call (\eg, to compile
  or to test the project).} We focused on Maven for its popularity on
Java projects.  Important to highlight that, as of now, the
\github{}'s search API can only reflect contents from README file (not
other code elements); it does not provide a feature to search for
projects containing certain files in the dir structure (\eg{},
\emph{pom.xml}).  Figure~\ref{fig:subject-query} illustrates the query
to the \github{} API as an HTTP request.  After obtaining a list of
potential projects, we filtered those containing a \pomf{} file in the
root directory.  A Maven project may contain several sub-modules with
multiple \pomf{} files.  We based our methodology to select
experimental subjects on other recent studies in testing and
debugging~\cite{gligoric-etal-issta2015,perez-etal-icst2017}.

\vspace{1ex}
\begin{figure}[h!]
\centering
\scriptsize
\lstset{
    escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c, numberstyle=\tiny,
    deletekeywords={true}
}
\begin{lstlisting}
https://api.github.com/search/repositories?q=language:java
        +stars:>=100+pushed:>=2016-01-01
        +mvn%20in:readme+sort:stars
\end{lstlisting}
    \caption{\label{fig:subject-query} Query to the \github{} API for
    projects with the following criteria: (1) Java, (2) at least 100
    stars, (3) updated on January 1st, 2016 (or later), (4) contains
    the string \emph{mvn} in the \emph{readme} file. Output is
    paginated in descending order of stars.}
\end{figure}

As of March 25th 2017, our search criteria returned \SubjectsGithub{}
subjects. Figure~\ref{fig:subjects} summarizes our sample set. From
\SubjectsGithub{} downloaded projects, \SubjectsGithubNotMaven{}
projects were not Maven or did not have a \pomf{} in the root
directory, \SubjectsGithubNotTestable{} projects were untestable
(because of missing dependencies or incompatible testing environment,
for example), and \SubjectsGithubFlaky{} projects were ``flaky''.  We
executed each project's test suite for three times to identify
projects containing flaky tests that could introduce noise in our
experiments.  Our final set of subjects consists of \numSubjs{}
projects.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{plots/subjs.pdf}
    \caption{\label{fig:subjects}We fetched \SubjectsGithub{} popular
    projects hosted on \github{}. From this initial sample, we ignored
    \SubjectsGithubNotMaven{} projects without Maven support,
    \SubjectsGithubNotTestable{} untestable projects, and
    \SubjectsGithubFlaky{} projects with flaky tests. We considered
    \numSubjs{} projects to conduct our study.}
\end{figure}

\subsection{Setup and Replication}
\label{sec:setup}

To run our experiments, we used a Core i7-4790 (3.60 GHz) Intel
processor machine with eight virtual CPUs (four cores with two native
threads each) and 16GB of memory, running Ubuntu 14.04 LTS Trusty Tahr
(64-bit version).  Software settings include \Comment{the Linux
  \emph{sysstat} package to measure performance, }git to fetch
subjects, Java 8, and Maven 3.3.9 to build and test subjects. We used
Python\Comment{ 3.4}, Bash, R and Ruby\Comment{ 2.3} to process the
data and generate plots.  All source artifacts are publicly available
for replication (on request)\Comment{ at \Fix{create gh-pages}}.  This
includes supporting scripts (\eg, the script that test subjects and
generates raw analysis data) and the full list of projects. \Comment{,
  and a \emph{Vagrantfile} to emulate our hardware and all software
  dependencies.}

\subsection{Potential}
\label{sec:rqA}
\label{sec:rqB}

\begin{itemize}
    \item \numRQA{}. \textbf{\RQA}
\end{itemize}
%\Jbc{The following steps may change $\rightarrow$}

To evaluate prevalence of projects with costly test suites, we
considered the \numSubjs{} testable subjects from
Figure~\ref{fig:subjects}.  Figure~\ref{fig:mvn-execution} illustrates
the script we used to measure time.

We took the following actions to isolate our environment from
measurement noise.  First, we observed that some test tasks called
test-unrelated tasks (\eg, \emph{javadoc} generation and static
analyses) that could interfere in our time measurements.  To address
that potential issue, we inspected Maven execution logs from a sample
including a hundred projects prior to running the script from
Figure~\ref{fig:mvn-execution}.  The tasks we found were ignored from
execution (lines 1-3).  Furthermore, to avoid noise from operating
system events, we configured our workstation to run only essential
services.  The machine was dedicated to our experiments and we
accessed it via SSH. In addition, we configured the \CodeIn{isolcpus}
option from the Linux Kernel \cite{linux-kernel} to isolate six
virtual CPUs to run our experiments, leaving the remaining CPUs to run
OS processes~\cite{isolcpus-use}.  The rationale for this decision is
to prevent context-switching between user processes (running the
experiment) and OS-related processes.  Finally, to make sure our
measurements were fair, we compared timings corresponding to the
sequential execution of tests using Maven with that obtained with
JUnit's default \CodeIn{JUnitCore} runner, invoked from the command
line.  Results were very close.

The main loop (lines 5-11) of the script in
Figure~\ref{fig:mvn-execution} iterates over the list of subjects and
invokes Maven multiple times\Comment{ to isolate cost of running
  tests} (lines 7-9).  It first compiles the source and test files
(line 7), make all dependencies available locally (line 8), and then
runs the tests in offline mode as to skip the package update task,
enabled by default (line 9). After execution, we used a regular
expression on the output log to extract elapsed time (line 10).

%% We executed each project's test suite for
%% \Fix{5} times through Maven and directly through JUnit each project

\input{codes/evaluation}

We ran the test suite for each subject three times, reporting averaged
execution times in three ranges: tests that run within a minute
(\shortg{} group), tests that run in one to five minutes (\medg{}
group), and tests that run in five or more minutes (\longg{}
group). We followed a similar methodology to group projects by time as
Gligoric~\etal{}~\cite{gligoric-etal-issta2015} in their work on
regression test selection.\Comment{ and added the \medg{} group due to
  the variability of the time cost from subjects out of the \shortg{}
  group} Figure~\ref{fig:rq1-barplot} shows the number of projects in
each group.  As expected, \longg{} and \medg{} projects do not occur
as frequently as \shortg{} projects.  However, they do occur in
relatively high numbers.

Figure~\ref{fig:rq1-boxplot} shows cost distribution of test suites in
each group as boxplots.  Note that the y-ranges are different.  The
distribution associated with the \shortg{} group is the most
unbalanced (right skewed)\Comment{ with outliers closed to the \medg{}
  group}.  The test suites in this group ran in 15 or less seconds for
over 75\% of the cases.  Such scenarios constitute the majority of the
cases we analyzed.  Considering the groups \medg{} and \longg{},
however, we found many costly executions.  Nearly 75\% of the projects
from the \medg{} group take over 3.5 minutes to run and nearly 75\% of
the projects from the \longg{} group take $\sim$20 minutes to run.  We
found cases in the \longg{} group were execution takes over 50 minutes
to complete, as can be observed from the outliers (dots) in the plot.

%% the median from the
%% \medg{} group is nearly two minutes and most of the subjects run in
%% less than four minutes; most of the \longg{} group runs in less than
%% 25 minutes but has outliers that require more than 50 minutes to
%% execute.



\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.182\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/barplot-timecost.pdf}
        \caption{\label{fig:rq1-barplot}}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplot-timecost.pdf}
        \caption{\label{fig:rq1-boxplot}}
    \end{subfigure}%
    \caption{(a) Subjects grouped by time cost ($t$): short run ($t <
    1m$), medium run ($1m \le t < 5m$), and long run ($5m \le t$); (b)
    Distribution of time cost by group.}
\end{figure}

%% Figure~\ref{fig:rq1-barplot} is a lower bound estimation of cost
%% because some tests may finish earlier than expected due to existing
%% test failures in the revision we downloaded.

It is important to note that we under-estimated cost in our
experiments for two main reasons.  First, some tests may finish
earlier than expected due to observed test failures in some of the
revisions we downloaded.  From the \numSubjs{} testable projects,
\numSubjsPass{} successfully executed all tests and \numSubjsFail{}
reported some test failures.  Second, some projects may omit
long-running tests on their default execution. For instance, the
project \CodeIn{apache.maven-surefire} runs all unit tests in a few
seconds.  According to our criteria, this project is to be classified
as \shortg{} but a closer look reveals that only smoke tests are run
by default in this project.  Time-consuming integration and system
tests are only accessible via custom parameters, which we do not
handle in our experimental setup.  We enabled such parameters for this
specific project and observed that testing time goes to nearly 30
minutes.  For simplicity, we considered only the tests executed by
default.

\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQA{}:}~\emph{We conclude that
      time-consuming test suites are relatively frequent in
      open-source projects.  We found that \percentMedLongRunning{} of
      the \numSubjs{} projects we analyzed (\ie{}, over 1 in every 5
      projects) take at least 3 minutes to run and
      \percentLongRunning{} take at least 5 minutes to run.\Comment{
        (\ie, \numMedLong{} projects from \medg{} and \longg{}).}}
\end{minipage}
}
\end{center}


\vspace{1ex}
\begin{itemize}
    \item \numRQB. \textbf{\RQB}
\end{itemize}

Section~\ref{sec:rqA} showed that medium and long-running projects are
not uncommon, accounting to nearly \percentMedLongRunning{} of the
\numSubjs{} projects we analyzed.  It is therefore important to
speedup regressing testing in open-source projects.\Comment{not only
  to huge projects as those from Google~\cite{google-tap,google-ci}
  and Microsoft~\cite{prasad-shulte-ieee-microsoft-ci}.}

Research question \numRQB{} assesses whether parallelism can help in
those cases.  One important factor to the effectiveness of parallelism
is the distribution of test costs in the test suite.  In the limit, if
cost is dominated by a single test from a large test suite, it is
unlikely that parallelization will be beneficial as a test method is
the smallest working unit in test frameworks.  However, avoiding
frequent context switches is another factor to consider.  For example,
assuming there are at least two CPUs available for execution, cost can
be cut in half if two tests in a large test suite dominate execution
time. For the case where cost is distributed more evenly across test
cases, one expects that speedups will be a function of the number of
cores.

%% These contradictory forces, pushing number of tests and cost
%% of each test up and down, make prediction of effectiveness challenging.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-long.pdf}
      \caption{\label{fig:longtcost}Long group.}
    \end{subfigure}\\
    \vspace{2ex}
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-medium.pdf}
      \caption{\label{fig:medtcost}Medium group.}
    \end{subfigure}
    %% \vspace{2ex}
    %% \begin{subfigure}{0.5\textwidth}
    %%   \centering
    %%   \begin{tabular}{rrrr}
    %%     \toprule
    %%     & $\sigma\leq1$ & $1<\sigma\leq5$ & $\sigma\ge5$ \\
    %%     \midrule    
    %%     Long   &  7 & 15 & 12 \\
    %%     Medium & 22 & 19 & 7 \\
    %%     \bottomrule
    %%   \end{tabular}
    %%   \caption{\label{fig:sd}Standard deviation ($\sigma$) of test case running times.}
    %% \end{subfigure}
    \caption{\label{fig:time-distributions}Time distributions.}%
\end{figure}

\sloppy Figures~\ref{fig:longtcost} and~\ref{fig:medtcost} show the
time distribution of individual test cases per project.  We observed
that the average median value of execution cost for a test was
relatively small (dashed horizontal red lines), namely 0.31s for
\medg{} projects and 0.23s for \longg{} projects.  The standard
deviations associated with each distribution were relatively
low.\Comment{ Figure~\ref{fig:sd} shows the number of projects within
  specific ranges of $\sigma$ values.}  We noted a small number of
cases of CPU monopolization.  For example, the highest value of
$\sigma$ occurred in \CodeIn{uber\_chaperone}, a project from the
medium group.  This project contains only 65 tests, 62 of which take
less than 0.5s to run, one of which takes nearly 3s to run, and two of
which take $\sim$40m to run.  For this project, 99.2\% of the
execution cost is dominated by only 3\% of the tests; without these
two costly tests this project would have been classified as
short-running.  A closer inspection in the data indicates that the
project \CodeIn{uber\_chaperone} was a corner case: we did not find
projects with such extreme time monopolization profile.  Project
\CodeIn{facebookarchive\_linkbench} is also classified as long-running
and has the second highest value of $\sigma$.  For this project,
however, cost is distributed more smoothly across \Fix{529} tests, of
which \Fix{119 (23\%)} take more than \Fix{1s} to run with the rest of
the tests running faster.

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.15\textwidth}
    \centering
    \includegraphics[width=.85\textwidth]{plots/boxplots-testcases.pdf}
    \caption{\label{fig:size-testsuites}Size of test suites.}
  \end{subfigure}
  ~
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{plots/scatter-testcost.pdf}
    \caption{\label{fig:scattercost}Size versus running time of
      test suites.}
  \end{subfigure}
  \caption{\label{fig:time-versus-size}Relating size and time.}%
\end{figure}

%\Mar{$\leftarrow$ show stats to indicate discrimination of
%  two distributions}

Figure~\ref{fig:time-distributions} showed that the average median
times were similar.  A closer inspection on results indicates that the
difference in overall running times across projects in these groups is
mainly justified by the number of test cases as opposed to the
individual costs of test cases.  Figure~\ref{fig:size-testsuites}
shows the difference in the distribution of test suite sizes across
groups.  This figure indicates that long projects, albeit having a
wider inter-quartile range (middle 50\% projects in this group are
less predictable), have a higher median and much higher averages.
Furthermore, we noted a strong positive correlation between running
time and number of test on projects in the \longg{} group.  The
positive correlation between these two variables was, however, weak in
the \medg{} group, suggesting that saving time in this group with test
suite parallelization may be more challenging as relatively fewer
tests dominate overall execution time.  Figure~\ref{fig:scattercost}
shows these results.

%% This 
%% indication that it is more beneficial to parallelize long projects as
%% cost is spread across many

\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQB{}:}~\emph{Overall, results indicate that
    projects with a very small number of tests monopolizing end-to-end
    execution time were rare.}
\end{minipage}
}
\end{center}

%% We are interested to know whether
%% most of the execution cost of a subject is dominated by a small subset
%% of test cases or if the cost is nearly equally distributed. 

%% We also evaluated the dispersion of time distributions (one
%% distribution per project) to answer research question \numRQB{}.  To
%% measure dispersion \emph{across} projects we used Relative Standard
%% Deviation (RSD)~\cite{everitt-book-stats-2010}.  Note that, if we were
%% to analyze each project in isolation, the standard deviation of a
%% distribution ($\sigma$) would suffice to quantify how dispersed the
%% (time) distribution is.  However, in our case, we would like to be
%% able to compare and summarize dispersion across projects.  The RSD,
%% which is obtained dividing the standard deviation by the mean ($\mu$)
%% of a distribution, provides such normalization effect.  This metric
%% provides a lower bound (zero) but not an upper bound (somewhere close
%% to 1).  The smaller (larger) the value of RSD the more (less) uniform
%% the distribution is.  Consequently, the lower the value of RSD the
%% more parallelizable a test suite should be.

%% \begin{figure}[h!]
%%   \centering
%%   \includegraphics[width=0.5\textwidth]{R/testcost.pdf}  
%%   \caption{\label{fig:relativesd}Distribution of RSD ($\sigma/\mu$)
%%     across projects.}
%% \end{figure}

%% Figure~\ref{fig:relativesd} shows the distribution of RSD across
%% medium and long-running projects.  Results show that the distribution
%% is skewed to the right indicating that test costs are relatively well
%% distributed in most costly projects we analyzed \Fix{$\leftarrow$
%%   confirm}.

%% analyzed the execution time
%% for the \numMedLong{} projects from the \longg{} and \medg{} groups
%% (see Section~\ref{sec:rqA}).
%% For each subject we calculated the
%% relative standard deviation of the test cases: we collected the
%% elapsed time of each individual test, calculated the standard
%% deviation, and divided by the mean. \Jbc{I need to clarify the
%%   relationship "well/bad-balanced" regression test and relative
%%   standard deviation}

%% Results indicated that \Fix{...elaborate...}. \Jbc{We may identify
%% different groups of subjects}\Fix{TODO: collect data + compute the
%% statistic, create a scatter plot to identify groups of subjects}

%% Regression tests that are well distributed may benefit from
%% parallelism since more tests executes at the same time while the
%% opposite scenario may require a different approach. In the later
%% scenario, executing tests in parallel may have insignificant impact
%% since a small subset of test cases dominates the execution.}

\subsection{Effectiveness}
\label{sec:rqD}

\begin{itemize}
    \item \textbf{\RQD}
\end{itemize}

To answer \numRQD{}, we considered \numProjectsPar{} subjects that we
tested and confirmed parallelization activated by default (see
Section~\ref{sec:rqC-1}). To measure the overall speedup obtained from
parallel execution, we compared the elapsed time in the default
execution (parallelization enabled) and re-executed the tests
sequentially.  To enforce sequential execution, we modified the
existing configurations. Also, we verified that, for each project,
both executions (\ie, default and sequential) are consistent and
produce the same outcome. To compute the speedup, we divide the
sequential time by the default execution. For instance, for a project
that runs in $10m$ sequentially and runs in $5m$ in the default execution,
the speedup is two.

\begin{figure}[h!]
\centering
\resizebox{.48\textwidth}{!}{%
\begin{tabular}{llrrl}
\toprule
Group & Subject & T$_{\text{d}}$ & T$_{\text{s}}$ & Speedup\\%
\midrule%
Medium & Jcabi GitHub & 0.30m & 2.75m & x9.01\\%
Medium & Apache Flink & 2.57m & 10.02m & x3.90\\%
Medium & Spotify Helios & 1.63m & 3.70m & x2.28\\%
Medium & Hazelcast Jet & 3.67m & 8.26m & x2.25\\%
  Long & Jenkins CI Gerrit Plugin & 40.31m & 76.32m & x1.89\\%
Medium & Jankotek MapDB & 8.58m & 10.51m & x1.23\\%
Medium & Javaslang & 1.82m & 2.12m & x1.16\\%
Medium & BounceStorage Chaos HTTP Proxy & 1.47m & 1.47m & x1.00\\%
Medium & Apache Log4J2 & 8.21m & 4.73m & x0.58\\%
\bottomrule%
\end{tabular}}
\caption{\Fix{Comparacao de tempo sequencial e paralelo ordenado por
    speedup}}
\label{tab:speedup}
\end{figure}

Figure~\ref{tab:speedup} summarizes our findings.
\Fix{falar sobre o resultado geral dos speedups - elaborar menor e
maior speedup}

\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQD{}:}~\emph{\Jbc{summarize
      findings.....}}
  \end{minipage}
}
\end{center}

\subsection{Adoption}
\label{sec:rqC}
\label{sec:rqE}

\begin{itemize}
    \item \emph{\RQC}
\end{itemize}

To evaluate the support of parallel execution of test suites, we
considered the \numMedLong{} projects that ran in at least one minute
identified in Section~\ref{sec:rqA} (\ie, \medg{} and \longg{}
groups). We are interested to know if developers consider low-level
parallelism modes to amortize the execution cost (see
Section~\ref{sec:modes}). More precisely, we investigate if there are
projects with parallelization modes enabled by default or if
developers configure this feature to run only on specific scenarios
(\eg, dedicated server).  We describe our approach as follows:


\subsubsection{Test execution with parallelism enabled by default}
\label{sec:rqC-1}

We re-executed the tests from the \numMedLong{} projects and we
configured Maven to output all runtime parameters and their current
values. We run only the test bootstrap step as the actual test
execution is unnecessary to output the runtime parameters.
According to Maven's documentation~\cite{maven-surefire-plugin}, any
parallelism configuration depends either on (1) the parameter
\CodeIn{parallel} to define the parallelism mode within a JVM followed
by the parameter \CodeIn{threadCount} or (2) the parameter
\CodeIn{forkCount}\footnote{This parameter is named \CodeIn{forkMode}
in old versions of Maven Surefire} to define the number of forked
JVMs. For each execution, we capture theses parameters and their
values to map to one of the possible parallelization modes.  For
instance, if a given project contains a module with the parameter
\CodeIn{<forkCount>1C</forkCount>}, the possible classifications are
\ForkSeq{} or \ForkParMeth{} depending on the presence and the value
of the parameter \CodeIn{parallel} (\ie, the presence of
\CodeIn{<parallel>methods</parallel>} results in the parallel mode
\ForkParMeth{}).
Large projects may contain several test suites distributed on
different Maven modules with different configurations. In this case,
we collect the Maven output from each module and we consider only the
distinct configurations. For instance, if a project contains two
modules with the same parallelism mode, we count only one occurrence
to not inflate our results.
From the \numMedLong{} projects, we identified \numProjectsPar{}
projects with the parallel execution modes \ForkSeq{} and
\ParClassParMeth{} enabled by default.
Figure~\ref{tab:freqmodes-dynamic} summarizes our findings. The column
``Group'' indicates the test cost group (see
Figure~\ref{fig:rq1-barplot}), the column ``Subject'' indicates the
name of the project, the column ``\# of modules'' indicates the number
of modules with tests in the default execution, and the column
``Mode'' indicates the corresponding parallelization mode.
%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.32\textwidth]{plots/barplot-modes-dynamic.pdf}
%    \caption{\label{fig:freqmodes-dynamic}\Fix{fix
%    caption}Distribution of parallel modes identified dynamically in a
%    subset of \numProjectsPar{} projects.  A project may have support
%    to more than one parallel mode. Also, a project may run only a
%    subset tests in parallel by default.}
%\end{figure}
\begin{figure}[ht!]
    \centering
    \resizebox{.48\textwidth}{!}{%
    \begin{tabular}{llcl}
        \toprule
        Group & Subject & \# of modules & Mode\\%
        \midrule%
        Long   & JenkinsCI Gerrit Trigger Plugin & 1 & \ForkSeq{}\\%
        Medium & Apache Logging Log4J2 & 25 & \ForkSeq{}\\%
        Medium & Apache Flink & - & - \\%
        Medium & BounceStorage Chaos HTTP Proxy & - & -\\%
        Medium & Hazelcast Jet & - & -\\%
        Medium & Jankotek MapDB & 1 & \ParClassParMeth{}\\%
        Medium & Javaslang & - & \ParClassParMeth{}\\%
        Medium & Jcabi Github & - & \ParClassParMeth{}\\%
        Medium & Spotify Helios & - & - \\%
        \bottomrule%
    \end{tabular}}
    \caption{Subjects with parallel test execution enabled by
    default.}
    \label{tab:freqmodes-dynamic}
\end{figure}

\subsubsection{Presence of parallelism configurations in build files}
\label{sec:rqC-2}

To extend our previous investigation, we evaluated the presence of
parallelism configuration in several build files (\ie, \pomf{} files)
from the same sample set of \numMedLong{} projects. The rationale is
that some projects may have support to parallel execution but it is
deactivated by default (\ie, require additional arguments before the
test execution). For instance, on Maven, it is possible to create
multiple configurations in the same build file and activate any of
them dynamically.  To reduce the space of \Fix{x} build files, we
considered a subset of \Fix{y} build files with the following
criteria: the given \pomf{} file has \CodeIn{maven-surefire-plugin}
plugin declaration and it has an explicit \CodeIn{configuration}
declaration. We automatically parsed these \Fix{y} files and we used
the same approach to classify the parallelization mode from the parsed
configurations as in our previous analysis. In an initial analysis, we
noticed that some projects do not declare a static value in some
parameters, instead, these projects inform the actual value in the
command line. To handle this case, we computed all valid values for
the parameter.

%False positive can happen because of comments, for instance.  
%To eliminate the cases of false positives and also to categorize 
%true positive cases, we complemented the initial mining step with a 
%manual inspection of files.
%% settings); the second step (inspection) consists in a manual
%% inspection to confirm the presence of parallelism settings in the
%% build file and classify them according to the parallelism level.
%% Figure \Fix{removed} describes the discovery step: we list the paths
%% of all build files and filter only the files that contain any of the

Figure~\ref{tab:inspection-table} summarizes our results.
\Fix{The first column indicates the group of projects according to
their time cost.  The second column indicates the number of build
files per group.  The last column indicates the ratio of projects with
parallelization settings.  From the \numMedLong{} subjects, we found
\pomMedLong{} \pomf{} files.  The \numPomMatched{} configurations are
distributed across \numProjectsPar{} projects from our sample.}

% \emph{From these results we found that $\sim$51\% of medium and
% long-running projects do not use parallel features to run test
% suites.}\Mar{please make it consistent with research
% question}\Mar{explain this is over(under)-estimated...}
\begin{figure}[ht!]
    \centering
    \resizebox{.48\textwidth}{!}{%
    \begin{tabular}{llcl}
        \toprule
        Group & Subject & \# of modules & Mode\\%
        \midrule%
        Long   & JenkinsCI Gerrit Trigger Plugin & 1 & \ForkSeq{}\\%
        Medium & Apache Logging Log4J2 & 25 & \ForkSeq{}\\%
        Medium & Jankotek MapDB & 1 & \ParClassParMeth{}\\%
        Medium & Javaslang & 3 & \ParClassParMeth{}\\%
        Medium & Jcabi Github & 1 & \ParClassParMeth{}\\%
        \bottomrule%
    \end{tabular}}
    \caption{\Fix{This is a COPY from the previous table!}}
    \label{tab:inspection-table}
\end{figure}
%% \begin{figure}[ht!]
%%     \centering
%%     \begin{tabular*}{0.48\textwidth}{@{\extracolsep{\fill}}ccc}
%%         \toprule
%%         \multirow{2}{*}{Group} %1st row, 1st cell
%%             & \multirow{2}{*}{\# \pomf{}}
%% 	    & \# \pomf{} matched\\
%%         % 2nd row - empty cell
%%             & % empty cell
%%             & / total\\%
%%         \midrule%
%% 	Long   & \numPomLong{} & 4 / \numLong{}\\%
%% 	Medium & \numPomMed{} & 6 / \numMed{}\\%
%%         \midrule%
%%         Total % last row, first cell
%%             & \pomMedLong{}
%%             & \numProjectsPar{} / \numMedLong{}\\%
%%         \bottomrule%
%%     \end{tabular*}
%%     \caption{Presence of parallelization settings in build files: the
%%     first column indicates the group of projects according to their
%%     time cost; the second column is the subset of files with parallelization
%%     keywords; the last column indicates the ratio of projects with
%%     parallelism support.}
%%     \label{tab:inspection-table} 
%% \end{figure}
%% \Jbc{rework this... $\rightarrow$} From the \numProjectsPar{} projects
%% identified above, we investigated further the \numPomMatched{}
%% build files with parallel settings.  We analyzed the support and
%% distribution of parallel modes from this subset of projects. To
%% calculate the distribution of parallel modes, we considered only the
%% presence of the mode in at least one of the project settings.  Recall
%% that a build file may contain more than one parallel setting and a
%% project may contain several sub-modules with build files.  In case the
%% value of a parallel option is resolved dynamically (\eg, via
%% command-line argument or system variable) we compute all modes related
%% to the option. For instance, depending on the value, the
%% \CodeIn{parallel} option can be \Seq{} (\CodeIn{none}),
%% \ParClassSeqMeth{} (\CodeIn{classes}), \SeqClassParMeth{},
%% (\CodeIn{methods}), and \ParClassParMeth{} (\CodeIn{all}).
%% Figure~\Fix{fig:freqmodes-static} summarizes our findings.
%% \Fix{Missing conclusion: Fork the most used configuration}
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.32\textwidth]{plots/barplot-modes-static.pdf}
%% 	\caption{\label{fig:freqmodes-static}\Luis{This is wrong, it
%% 	should be \textbf{CF0} instead of \textbf{CL0}}Distribution of parallel modes
%%     identified statically in a subset of \numProjectsPar{} projects.
%%     A project may have support to more than one parallel mode.}
%% \end{figure}

\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQC{}:}~\emph{Overall, results indicate
      that parallelism features are underused by default given that
      only \percentParallel{} of projects  (5 out of \numMedLong)
      supports parallel execution of test suites by default and
      \Fix{?\%} (\Fix{y} out of \numMedLong{}) require additional
      parameters to enable these features.}
  \end{minipage}
}
\end{center}

\begin{itemize}
	\item \textbf{\RQE{}}
\end{itemize}

To understand why developers prefer not use parallel test execution,
we considered the \numNonParallel{} subjects that do not use any
parallel mode from Section~\ref{sec:rqC-1} and elaborated a
quick survey to extent the comprehension about why developers do not
use parallel configuration. \Luis{describe how we created the survey}.
We first retrieve the last 20 commits from the project repository and
got the developers e-mails. In total, we sent an e-mail to
\emailsSent{} developers that recently contributed to the
\numNonParallel{} projects.
There were \emailsAnswered{} replies from \emailsProjectsAnswered{}, 
and \emailsFalseAnswers{} was not able to explain how the testing works.

The survey sent was composed by three open questions and one closed
question: (I) How long does it take for test to run in your
environment? Can you briefly define your setup? (II) Do you confirm
that your project does not run in parallel? (III) Select a reason for
not using parallelization: (a) I did not know it was possible; (b) I
was concerned with concurrency issues; (c) I use a continuous
integration server; (d) Some other reason. Please elaborate.

\Comment{
\begin{enumerate}
	\item How long does it take for test to run in your
		environment?
	\item Can you briefly define your setup?
	\item Do you confirm that your project does not run in
		parallel?
	\item Select a reason for not using paralellization:
		\begin{enumarate}
			\item I did not know it was possible;
			\item I was concerned with concurrency issues;
			\item I use a continuous integration server;
			\item Some other reason.
		\end{enumerate}
\end{enumerate}
}

From the valid replies, we confirmed that the projects takes more than
\Fix{1m} to run tests and \emailsSeq{} confirmed that those projects do not use 
parallel testing using Maven Surefire. The other projects that we do not 
confirm, need and extra parameter to execute in parallel mode \Luis{make 
sure that they are inside the static analysis sheets made before}.

We noticed that \pctEmailsCI{} of developers use an continuous integration 
server to run tests, \pctEmailsLocal{} run tests in the
development machine, \pctEmailsDidNotSpecify{} did not specified their
testing setup. We also observed that one project has testing enabled 
without parallelization. However the maintainers do not execute the tests.
Considering developers did not know about the existence of the
parallelism configuration \pctEmailsDoNotKnow{} of the replies
confirmed that. Three of the developers also confirmed that they use
another method of parallelization with a distributed build machines.

\begin{center}
\fbox{
	\begin{minipage}{8cm}
		\textit{Answering \numRQE{}:~\emph{\Luis{summarize
		findings...}}}
	\end{minipage}
}
\end{center}

%%To evaluate the distribution of execution time per project, we sorted
%%the test cases by decreasing order of elapsed time and calculated the
%%number of tests executed in 90\% of the total time. Later, we reported
%%the \Fix{balance} of execution time by dividing the number of tests
%%that represents 90\% of the execution time by the number of tests
%%cases. For instance, a balance of 50\% indicates \Fix{...}.  \Fix{We
%%collected the elapsed time from test cases for each generated report.
%%Maven Surefire generates an XML report with execution information
%%(\eg, number of skipped tests and elapsed time) per test suite
%%\Jbc{Should I use the previous sentence as a footnote or should I
%%delete it?}. We noticed that some test cases reported an elapsed time
%%of zero: since the reported time is in milliseconds, some tests may
%%execute in a shorter time. \Fix{..to be continued...}}. Results
%%indicate that \Fix{...}.
%%
%%\begin{figure}[h!]
%%    \centering
%%    \includegraphics[width=0.4\textwidth]{results/plots/balance.pdf}
%%    \caption{\Fix{balance}}
%%\end{figure}

%% \subsection{Answering research question RQ3}
%% \label{sec:rqThree}
%% 
%% \begin{itemize}
%%     \item \RQB
%% \end{itemize}
%% 
%% To evaluate the distribution of CPU and IO intensive test suites from
%% the sample set, we used the command \emph{sar} to monitor the system
%% activity in background while tests ran. \emph{Sar} is a command that
%% collects and reports statistics (\eg, percentage of IO waiting and
%% usage of CPU in user mode) based on the kernel activity and it is
%% highly configurable to collect detailed information (\eg, usage of a
%% specific processor core or percentage of network interface
%% utilization). We configured \emph{sar} to report \Fix{...explain how
%% we executed and what fields we are interested}. \Fix{explain fields}.
%% Figure \Fix{A} shows the distribution of subjects grouped in intervals
%% of \Fix{W}\% of CPU utilization. Results indicates that \Fix{...}
%% 
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.4\textwidth]{results/plots/cpuness.pdf}
%%     \caption{\Fix{cpu usage}}
%% \end{figure}
%% 
%% \Comment{we proposed the definition of \emph{cpuness}
%% computed as the follow: $((user\_t + system\_t) / elapsed\_t) * 100$,
%% where \emph{user\_t} is the elapsed time of execution in \emph{user
%% mode}, \emph{system\_t} is the elapsed in \emph{kernel mode}, and
%% \emph{elapsed\_t} is the elapsed time to finish the execution. We
%% measured the \emph{cpuness} of each regression test \Fix{...elaborate
%% the meaning of cpuness} \Fix{Describe how I measured user, system and
%% "wall" time}.  \Fix{Explain results}.  \Fix{show plots}}
%% 

%%  LocalWords:  RQ occurence parallelization Tradeoffs API readme th
%%  LocalWords:  mvn clearcut escapeinside xleftmargin untestable LTS
%%  LocalWords:  framexleftmargin CPUs Tahr sysstat gh Vagrantfile
%%  LocalWords:  javadoc isolcpus JUnit's JUnitCore Gligoric boxplots
%%  LocalWords:  outliers apache uber chaperone facebookarchive
%%  LocalWords:  linkbench
