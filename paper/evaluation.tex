\section{Evaluation}
\label{sec:eval}

%% We are interested in understanding the prevalence of time-consuming
%% test suites and main sources of execution cost. We want to understand
%% how the execution cost is distributed on test cases within a test
%% suite and how developers approach test execution. Based on that, we
%% study parallelization of testing frameworks and build systems.  More
%% precisely, we investigate how prevalent test parallelization is, the
%% potential for improving execution cost, issues of flakiness that
%% hinders the use of parallelization, and how to address those issues.
%% More specifically, we pose the following research questions:

%% The first research question addresses the prevalence of long-running
%% test suites. We are interested to know if costly test suites are
%% common in open-source projects.  The second research question
%% addresses the relationship of test cases and the overall execution
%% cost: we are interested to investigate how the execution time is
%% distributed among test cases.  In the third research question, we
%% investigate if developers consider low-level parallelism features
%% available out-of-the-box to amortize test execution (see
%% Section~\ref{sec:modes}). In addition, we want to identify what
%% configurations are often used and why they are more popular (if any).
%% The fourth research question addresses the impact of low-level
%% parallelism on test execution from projects in our sample set. We want
%% to identify subjects that already use test parallelization and compare
%% their performance in contrast to sequential execution. In addition, we
%% are interested in evaluating the performance of sequential test suites
%% with different parallelization settings.
%% Finally, the fifth research question discusses the limitations and
%% insights to overcome the pitfalls of parallelization.

%% \Comment{
%%     \Fix{distribution of execution time per test case. For each subject
%%     identified in the first research question, we investigate how
%%     balanced is the cost of the test suite in contrast to the cost of
%%     test cases and if there are subjects where the time cost is mostly
%%     dominated by a small fraction of test cases.} \Fix{The third research
%%     question addresses the distribution of regression tests according
%%     to the use of computational resources.  We are interested in
%%     investigating if regression test suites are CPU intensive and if there
%%     are opportunities to improve performance. The RQ4 addresses}
%%     \Fix{...elaborate...}

%%     The rationale is that if the time cost of a regression test is equally
%%     distributed among test cases, the execution cost could be potentially
%%     improved by running tests in parallel (in contrast to the scenario
%%     where only one test case dominates most of the execution time).
%% }

We pose the following research questions, organized by the dimensions
of analysis we presented in Section~\ref{sec:intro}.

\newcommand{\numRQA}{RQ1}
\newcommand{\numRQB}{RQ2}
\newcommand{\numRQD}{RQ3}
\newcommand{\numRQC}{RQ4}
\newcommand{\numRQE}{RQ5}
\newcommand{\numRQF}{RQ6}

\newcommand{\RQA}{How prevalent is the occurence of time-consuming
  test suites\Comment{ in open-source projects}?}
\newcommand{\RQB}{How time is distributed across test cases?}
\newcommand{\RQC}{How prevalent is the use of test suite
  parallelization\Comment{ in open-source projects}?}
\newcommand{\RQD}{What are the speedups obtained with parallelization
  (in projects that actually use it) on cost?}
\newcommand{\RQE}{What are the main reasons that prevent developers
  from using test suite parallelization?}
\newcommand{\RQF}{To which extent parallel execution configurations affect testing
  cost and flakiness?}

\begin{itemize}
\item Potential
  \begin{itemize}
  \item \textbf{\numRQA.} \RQA
  \item \textbf{\numRQB.} \RQB    
  \end{itemize}
\item Effectiveness
  \begin{itemize}
  \item \textbf{\numRQD.} \RQD
  \end{itemize}  
\item Adoption
  \begin{itemize}
  \item \textbf{\numRQC.} \RQC    
  \item \textbf{\numRQE.} \RQE
  \end{itemize}
\item Tradeoffs
  \begin{itemize}
  \item \textbf{\numRQF.} \RQF    
  \end{itemize}          
\end{itemize}

%%\newcommand{\RQB}{What is the distribution of CPU and IO bound
%%regression test suites from the sample set?}
%%
%%\newcommand{\RQC}{How uniformly distributed is the execution time
%%across test cases in costly projects?}
%%
%%\newcommand{\RQD}{How often developers use the parallelism features
%%from build systems to improve runtime performance?}


\subsection{Subjects}
\label{sec:subjects}

%% We evaluated the characteristics of test suites in open-source
%% development from a sample set of Java projects from \github{}.  We are
%% interested to evaluate non-trivial test suites from popular projects
%% that are in activity.

%% , which is used on
%% \github{} to indicate appreciation of a user to a
%% project~\cite{github-stars},

We used \github{}'s search API~\cite{githubsearch} to identify
projects that satisfy the following criteria: (1) the primary language
is Java\footnote{In case of projects in multiple languages, the
  \github{} API considers the predominant language as the primary
  language.}, (2) the project has at least 100 stars, (3) the latest
update was on or after January 1st, 2016, and (4) the \emph{readme}
file contains the string \emph{mvn}.  We focused on Java for its
popularity.  Although there is no clearcut limit on the number of
\github{} stars~\cite{github-stars} to define relevant projects, we
observed that 100 was enough to eliminate trivial subjects. The third
criteria serves to skip projects without recent activity. The fourth
criteria is an approximation to find Maven projects.\Comment{ The
  rationale is that if the string \emph{mvn} exists in the
  \emph{readme} file, it may represent a Maven call (\eg, to compile
  or to test the project).} We focused on Maven for its popularity on
Java projects.  Important to highlight that, as of now, the
\github{}'s search API can only reflect contents from README file (not
other code elements); it does not provide a feature to search for
projects containing certain files in the dir structure (\eg{},
\emph{pom.xml}).  Figure~\ref{fig:subject-query} illustrates the query
to the \github{} API as an HTTP request.  After obtaining a list of
potential projects, we filtered those containing a \pomf{} file in the
root directory.  A Maven project may contain several sub-modules with
multiple \pomf{} files.  We based our methodology to select
experimental subjects on other recent studies in testing and
debugging~\cite{gligoric-etal-issta2015,perez-etal-icst2017}.

\vspace{1ex}
\begin{figure}[h!]
\centering
\scriptsize
\lstset{
    escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c, numberstyle=\tiny,
    deletekeywords={true}
}
\begin{lstlisting}
https://api.github.com/search/repositories?q=language:java
        +stars:>=100+pushed:>=2016-01-01
        +mvn%20in:readme+sort:stars
\end{lstlisting}
    \caption{\label{fig:subject-query} Query to the \github{} API for
    projects with the following criteria: (1) Java, (2) at least 100
    stars, (3) updated on January 1st, 2016 (or later), (4) contains
    the string \emph{mvn} in the \emph{readme} file. Output is
    paginated in descending order of stars.}
\end{figure}

As of March 25th 2017, our search criteria returned \SubjectsGithub{}
subjects. Figure~\ref{fig:subjects} summarizes our sample set. From
\SubjectsGithub{} downloaded projects, \SubjectsGithubNotMaven{}
projects were not Maven or did not have a \pomf{} in the root
directory, \SubjectsGithubNotTestable{} projects were untestable
(because of missing dependencies or incompatible testing environment,
for example), and \SubjectsGithubFlaky{} projects were ``flaky''.  We
executed each project's test suite for three times to identify
projects containing flaky tests that could introduce noise in our
experiments.  Our final set of subjects consists of \numSubjs{}
projects.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{plots/subjs.pdf}
    \caption{\label{fig:subjects}We fetched \SubjectsGithub{} popular
    projects hosted on \github{}. From this initial sample, we ignored
    \SubjectsGithubNotMaven{} projects without Maven support,
    \SubjectsGithubNotTestable{} untestable projects, and
    \SubjectsGithubFlaky{} projects with flaky tests. We considered
    \numSubjs{} projects to conduct our study.}
\end{figure}

\subsection{Setup and Replication}
\label{sec:setup}

To run our experiments, we used a Core i7-4790 (3.60 GHz) Intel
processor machine with eight virtual CPUs (four cores with two native
threads each) and 16GB of memory, running Ubuntu 14.04 LTS Trusty Tahr
(64-bit version).  Software settings include \Comment{the Linux
  \emph{sysstat} package to measure performance, }git to fetch
subjects, Java 8, and Maven 3.3.9 to build and test subjects. We used
Python\Comment{ 3.4}, Bash, R and Ruby\Comment{ 2.3} to process the
data and generate plots.  All source artifacts are publicly available
for replication (on request)\Comment{ at \Fix{create gh-pages}}.  This
includes supporting scripts (\eg, the script that test subjects and
generates raw analysis data) and the full list of projects. \Comment{,
  and a \emph{Vagrantfile} to emulate our hardware and all software
  dependencies.}

\subsection{Potential}
\label{sec:rqA}
\label{sec:rqB}

\begin{itemize}
    \item \numRQA{}. \textbf{\RQA}
\end{itemize}
%\Jbc{The following steps may change $\rightarrow$}

To evaluate prevalence of projects with costly test suites, we
considered the \numSubjs{} testable subjects from
Figure~\ref{fig:subjects}.  Figure~\ref{fig:mvn-execution} illustrates
the script we used to measure time.

We took the following actions to isolate our environment from
measurement noise.  First, we observed that some test tasks called
test-unrelated tasks (\eg, \emph{javadoc} generation and static
analyses) that could interfere in our time measurements.  To address
that potential issue, we inspected Maven execution logs from a sample
including a hundred projects prior to running the script from
Figure~\ref{fig:mvn-execution}.  The tasks we found were ignored from
execution (lines 1-3).  Furthermore, to avoid noise from operating
system events, we configured our workstation to run only essential
services.  The machine was dedicated to our experiments and we
accessed it via SSH. In addition, we configured the \CodeIn{isolcpus}
option from the Linux Kernel \cite{linux-kernel} to isolate six
virtual CPUs to run our experiments, leaving the remaining CPUs to run
OS processes~\cite{isolcpus-use}.  The rationale for this decision is
to prevent context-switching between user processes (running the
experiment) and OS-related processes.  Finally, to make sure our
measurements were fair, we compared timings corresponding to the
sequential execution of tests using Maven with that obtained with
JUnit's default \CodeIn{JUnitCore} runner, invoked from the command
line.  Results were very close.

The main loop (lines 5-11) of the script in
Figure~\ref{fig:mvn-execution} iterates over the list of subjects and
invokes Maven multiple times\Comment{ to isolate cost of running
  tests} (lines 7-9).  It first compiles the source and test files
(line 7), make all dependencies available locally (line 8), and then
runs the tests in offline mode as to skip the package update task,
enabled by default (line 9). After execution, we used a regular
expression on the output log to extract elapsed time (line 10).

%% We executed each project's test suite for
%% \Fix{5} times through Maven and directly through JUnit each project

\input{codes/evaluation}

We ran the test suite for each subject three times, reporting averaged
execution times in three ranges: tests that run within a minute
(\shortg{} group), tests that run in one to five minutes (\medg{}
group), and tests that run in five or more minutes (\longg{}
group). We followed a similar methodology to group projects by time as
Gligoric~\etal{}~\cite{gligoric-etal-issta2015} in their work on
regression test selection.\Comment{ and added the \medg{} group due to
  the variability of the time cost from subjects out of the \shortg{}
  group} Figure~\ref{fig:rq1-barplot} shows the number of projects in
each group.  As expected, \longg{} and \medg{} projects do not occur
as frequently as \shortg{} projects.  However, they do occur in
relatively high numbers.

Figure~\ref{fig:rq1-boxplot} shows cost distribution of test suites in
each group as boxplots.  Note that the y-ranges are different.  The
distribution associated with the \shortg{} group is the most
unbalanced (right skewed)\Comment{ with outliers closed to the \medg{}
  group}.  The test suites in this group ran in 15 or less seconds for
over 75\% of the cases.  Such scenarios constitute the majority of the
cases we analyzed.  Considering the groups \medg{} and \longg{},
however, we found many costly executions.  Nearly 75\% of the projects
from the \medg{} group take over 3.5 minutes to run and nearly 75\% of
the projects from the \longg{} group take $\sim$20 minutes to run.  We
found cases in the \longg{} group were execution takes over 50 minutes
to complete, as can be observed from the outliers (dots) in the plot.

%% the median from the
%% \medg{} group is nearly two minutes and most of the subjects run in
%% less than four minutes; most of the \longg{} group runs in less than
%% 25 minutes but has outliers that require more than 50 minutes to
%% execute.



\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.182\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/barplot-timecost.pdf}
        \caption{\label{fig:rq1-barplot}}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplot-timecost.pdf}
        \caption{\label{fig:rq1-boxplot}}
    \end{subfigure}%
    \caption{(a) Subjects grouped by time cost ($t$): short run ($t <
    1m$), medium run ($1m \le t < 5m$), and long run ($5m \le t$); (b)
    Distribution of time cost by group.}
\end{figure}

%% Figure~\ref{fig:rq1-barplot} is a lower bound estimation of cost
%% because some tests may finish earlier than expected due to existing
%% test failures in the revision we downloaded.

It is important to note that we under-estimated cost in our
experiments for two main reasons.  First, some tests may finish
earlier than expected due to observed test failures in some of the
revisions we downloaded.  From the \numSubjs{} testable projects,
\numSubjsPass{} successfully executed all tests and \numSubjsFail{}
reported some test failures.  Second, some projects may omit
long-running tests on their default execution. For instance, the
project \CodeIn{apache.maven-surefire} runs all unit tests in a few
seconds.  According to our criteria, this project is to be classified
as \shortg{} but a closer look reveals that only smoke tests are run
by default in this project.  Time-consuming integration and system
tests are only accessible via custom parameters, which we do not
handle in our experimental setup.  We enabled such parameters for this
specific project and observed that testing time goes to nearly 30
minutes.  For simplicity, we considered only the tests executed by
default.

\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQA{}:}~\emph{We conclude that
      time-consuming test suites are relatively frequent in
      open-source projects.  We found that \percentMedLongRunning{} of
      the \numSubjs{} projects we analyzed (\ie{}, over 1 in every 5
      projects) take at least 3 minutes to run and
      \percentLongRunning{} take at least 5 minutes to run.\Comment{
        (\ie, \numMedLong{} projects from \medg{} and \longg{}).}}
\end{minipage}
}
\end{center}


\vspace{1ex}
\begin{itemize}
    \item \numRQB. \textbf{\RQB}
\end{itemize}

Section~\ref{sec:rqA} showed that medium and long-running projects are
not uncommon, accounting to nearly \percentMedLongRunning{} of the
\numSubjs{} projects we analyzed.  It is therefore important to
speedup regressing testing in open-source projects.\Comment{not only
  to huge projects as those from Google~\cite{google-tap,google-ci}
  and Microsoft~\cite{prasad-shulte-ieee-microsoft-ci}.}

Research question \numRQB{} assesses whether parallelism can help in
those cases.  One important factor to the effectiveness of parallelism
is the distribution of test costs in the test suite.  In the limit, if
cost is dominated by a single test from a large test suite, it is
unlikely that parallelization will be beneficial as a test method is
the smallest working unit in test frameworks.  However, avoiding
frequent context switches is another factor to consider.  For example,
assuming there are at least two CPUs available for execution, cost can
be cut in half if two tests in a large test suite dominate execution
time. For the case where cost is distributed more evenly across test
cases, one expects that speedups will be a function of the number of
cores.

%% These contradictory forces, pushing number of tests and cost
%% of each test up and down, make prediction of effectiveness challenging.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-long.pdf}
      \caption{\label{fig:longtcost}Long group.}
    \end{subfigure}\\
    \vspace{2ex}
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-medium.pdf}
      \caption{\label{fig:medtcost}Medium group.}
    \end{subfigure}
    %% \vspace{2ex}
    %% \begin{subfigure}{0.5\textwidth}
    %%   \centering
    %%   \begin{tabular}{rrrr}
    %%     \toprule
    %%     & $\sigma\leq1$ & $1<\sigma\leq5$ & $\sigma\ge5$ \\
    %%     \midrule    
    %%     Long   &  7 & 15 & 12 \\
    %%     Medium & 22 & 19 & 7 \\
    %%     \bottomrule
    %%   \end{tabular}
    %%   \caption{\label{fig:sd}Standard deviation ($\sigma$) of test case running times.}
    %% \end{subfigure}
    \caption{\label{fig:time-distributions}Time distributions.}%
\end{figure}

\sloppy Figures~\ref{fig:longtcost} and~\ref{fig:medtcost} show the
time distribution of individual test cases per project.  We observed
that the average median value of execution cost for a test was
relatively small (dashed horizontal red lines), namely 0.31s for
\medg{} projects and 0.23s for \longg{} projects.  The standard
deviations associated with each distribution were relatively
low.\Comment{ Figure~\ref{fig:sd} shows the number of projects within
  specific ranges of $\sigma$ values.}  We noted a small number of
cases of CPU monopolization.  For example, the highest value of
$\sigma$ occurred in \CodeIn{uber\_chaperone}, a project from the
medium group.  This project contains only 65 tests, 62 of which take
less than 0.5s to run, one of which takes nearly 3s to run, and two of
which take $\sim$40m to run.  For this project, 99.2\% of the
execution cost is dominated by only 3\% of the tests; without these
two costly tests this project would have been classified as
short-running.  A closer inspection in the data indicates that the
project \CodeIn{uber\_chaperone} was a corner case: we did not find
projects with such extreme time monopolization profile.  Project
\CodeIn{facebookarchive\_linkbench} is also classified as long-running
and has the second highest value of $\sigma$.  For this project,
however, cost is distributed more smoothly across \Fix{529} tests, of
which \Fix{119 (23\%)} take more than \Fix{1s} to run with the rest of
the tests running faster.

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.15\textwidth}
    \centering
    \includegraphics[width=.85\textwidth]{plots/boxplots-testcases.pdf}
    \caption{\label{fig:size-testsuites}Size of test suites.}
  \end{subfigure}
  ~
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{plots/scatter-testcost.pdf}
    \caption{\label{fig:scattercost}Size versus running time of
      test suites.}
  \end{subfigure}
  \caption{\label{fig:time-versus-size}Relating size and time.}%
\end{figure}

%\Mar{$\leftarrow$ show stats to indicate discrimination of
%  two distributions}

Figure~\ref{fig:time-distributions} showed that the average median
times were similar.  A closer inspection on results indicates that the
difference in overall running times across projects in these groups is
mainly justified by the number of test cases as opposed to the
individual costs of test cases.  Figure~\ref{fig:size-testsuites}
shows the difference in the distribution of test suite sizes across
groups.  This figure indicates that long projects, albeit having a
wider inter-quartile range (middle 50\% projects in this group are
less predictable), have a higher median and much higher averages.
Furthermore, we noted a strong positive correlation between running
time and number of test on projects in the \longg{} group.  The
positive correlation between these two variables was, however, weak in
the \medg{} group, suggesting that saving time in this group with test
suite parallelization may be more challenging as relatively fewer
tests dominate overall execution time.  Figure~\ref{fig:scattercost}
shows these results.

%% This 
%% indication that it is more beneficial to parallelize long projects as
%% cost is spread across many

\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQB{}:}~\emph{Overall, results indicate that
    projects with a very small number of tests monopolizing end-to-end
    execution time were rare.}
\end{minipage}
}
\end{center}

%% We are interested to know whether
%% most of the execution cost of a subject is dominated by a small subset
%% of test cases or if the cost is nearly equally distributed. 

%% We also evaluated the dispersion of time distributions (one
%% distribution per project) to answer research question \numRQB{}.  To
%% measure dispersion \emph{across} projects we used Relative Standard
%% Deviation (RSD)~\cite{everitt-book-stats-2010}.  Note that, if we were
%% to analyze each project in isolation, the standard deviation of a
%% distribution ($\sigma$) would suffice to quantify how dispersed the
%% (time) distribution is.  However, in our case, we would like to be
%% able to compare and summarize dispersion across projects.  The RSD,
%% which is obtained dividing the standard deviation by the mean ($\mu$)
%% of a distribution, provides such normalization effect.  This metric
%% provides a lower bound (zero) but not an upper bound (somewhere close
%% to 1).  The smaller (larger) the value of RSD the more (less) uniform
%% the distribution is.  Consequently, the lower the value of RSD the
%% more parallelizable a test suite should be.

%% \begin{figure}[h!]
%%   \centering
%%   \includegraphics[width=0.5\textwidth]{R/testcost.pdf}  
%%   \caption{\label{fig:relativesd}Distribution of RSD ($\sigma/\mu$)
%%     across projects.}
%% \end{figure}

%% Figure~\ref{fig:relativesd} shows the distribution of RSD across
%% medium and long-running projects.  Results show that the distribution
%% is skewed to the right indicating that test costs are relatively well
%% distributed in most costly projects we analyzed \Fix{$\leftarrow$
%%   confirm}.

%% analyzed the execution time
%% for the \numMedLong{} projects from the \longg{} and \medg{} groups
%% (see Section~\ref{sec:rqA}).
%% For each subject we calculated the
%% relative standard deviation of the test cases: we collected the
%% elapsed time of each individual test, calculated the standard
%% deviation, and divided by the mean. \Jbc{I need to clarify the
%%   relationship "well/bad-balanced" regression test and relative
%%   standard deviation}

%% Results indicated that \Fix{...elaborate...}. \Jbc{We may identify
%% different groups of subjects}\Fix{TODO: collect data + compute the
%% statistic, create a scatter plot to identify groups of subjects}

%% Regression tests that are well distributed may benefit from
%% parallelism since more tests executes at the same time while the
%% opposite scenario may require a different approach. In the later
%% scenario, executing tests in parallel may have insignificant impact
%% since a small subset of test cases dominates the execution.}

\subsection{Effectiveness}
\label{sec:rqD}

\begin{itemize}
    \item \numRQD{}. \textbf{\RQD}
\end{itemize}

To answer \numRQD{}, we considered \emph{all} \numProjectsPar{}
subjects from our benchmark that use parallelization by default in
Maven to run tests.  Section~\ref{sec:rqC-1} describes in detail the
methodology we used to find these subjects. For each project, the
speedup (or slowdown) corresponds to the ratio ($T_s/T_p$) between the
elapsed time of a sequential execution ($T_s$) and the elapsed time of
a corresponding parallel execution ($T_p$), obtained as described in
the script from Figure~\ref{fig:mvn-execution}.  As usual, a ratio
above 1x denotes speedup and a ratio below 1x denotes slowdown.

%% For each project, we modified the existing configurations to disable
%% the parallel test execution. In addition, we verified that both
%% executions produce the same outcome to eliminate noise from failing
%% tests.  To compute the speedup, we divide the time obtained in the
%% sequential execution by the time obtained from the default execution.
%% For instance, if a project runs the tests sequentially in $10m$ and
%% the same execution runs in $5m$ with parallelization enabled (default
%% execution), the speedup is two.

\begin{figure}[h!]
\centering
\resizebox{.48\textwidth}{!}{%
\begin{tabular}{llrrl}
\toprule
\emph{Group} & \emph{Subject} & \multicolumn{1}{c}{$T_s$} & \multicolumn{1}{c}{$T_p$} & $T_s/T_p$ \\%
\midrule%
Medium & BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.47m & 1.00x\\%
Medium & Apache \textbf{Flink} & 10.02m & 2.57m & 3.90x\\%
Long & Jenkins CI \textbf{Gerrit} Plugin & 76.32m & 40.31m &  1.89x\\%
Medium & Spotify \textbf{Helios} & 3.70m & 1.63m & 2.28x\\%  
Medium & \textbf{Javaslang} & 2.12m & 1.82m & 1.16x\\%
Medium & \textbf{Jcabi} GitHub & 2.75m & 0.30m & \cellcolor{lightgray}9.01x\\%
Medium & Hazelcast \textbf{Jet} & 8.26m & 3.67m & 2.25x\\%
Medium & Apache \textbf{Log4J2} & 4.73m & 8.21m & \cellcolor{lightgray}0.58x\\%
Medium & Jankotek \textbf{MapDB} & 10.51m & 8.58m & 1.23x\\%
\midrule
\textbf{total} &  &  &  & \avgSpeedup{}x\\
\bottomrule%
\end{tabular}}
\caption{Speedup (or slowdown) of parallel execution ($T_p$) over
  sequential execution ($T_s$).  Default parallel configuration of
  Maven is used.  Abbreviation of project names appear in bold face
  and highest slowdown/speedup appear in gray color.}
\label{tab:speedup}
\end{figure}

Figure~\ref{tab:speedup} summarizes results.  Lines are sorted by
abbreviated project names, which appear in bold.  Columns
``\emph{Group}'' and ``\emph{Name}'' indicate, respectively, the
project group and the name of the subject.  Column ``$T_s$'' shows
sequential execution time whereas column ``$T_p$'' shows parallel
execution time. Column ``$T_s/T_p$'' shows the speedup or slowdown
associated with parallel execution.

Results indicate that, on average, parallel execution was
\avgSpeedup{} times faster compared to sequential execution.  We noted
two extreme cases: \CodeIn{Jcabi} and \CodeIn{Log4J2}.
The first when using the default parallel configuration, runs a
\textbf{C3} configuration with small test classes and big test
methods, allowing to run multiple methods and classes at once. The
high slowdown happens when big methods run sequentially.
The second when using the default parallel configuration, runs a
\textbf{FC0} configuration with small test classes and small test
methods, which is slowdown by the JVM creation overhead. This
configuration is useful to isolate each test method from each other.
\Luis{$\leftarrow$ rework this} \Fix{falar
  sobre o resultado geral dos speedups - elaborar menor e maior
  speedup... acho que so vale a pena discutir quando tiver conviccao
  dos 2 casos}

\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQD{}:}~\emph{Overall, results indicate
      that the speedup obtained with parallelization is
      \Fix{$\sim$x\%} (in average) for \Fix{\%} of the projects
      (\Fix{x} out of \Fix{w}). }
  \end{minipage}
}
\end{center}

\subsection{Adoption}
\label{sec:rqC}
\label{sec:rqE}

Research questions \numRQC{} and \numRQE{} investigate to which extent
parallelization is used in practice.  It is important to investigate
adoption as it is possible that developers may use alternative
mechanisms to execute tests.

\begin{itemize}
    \item \numRQC. \emph{\RQC}
\end{itemize}

To answer \numRQC{}, we selected all projects that ran in at least one
minute, \ie, projects from the \medg{} and \longg{} groups.  This set
includes \numMedLong{} projects (see Section~\ref{sec:rqA}).  We
looked for dynamic and static manifestations of parallelism.
Section~\ref{sec:rqC-1} and~\ref{sec:rqC-2} report results for each of
these cases.

\vspace{1ex}
\subsubsection{Test execution with parallelism enabled by default}
\label{sec:rqC-1}

To find dynamic evidence of parallelism, we ran the test suites from
our set of \numMedLong{} projects to output all key-value pairs of
Maven parameters.  To that end, we used the option~\CodeIn{-X} to
produce debug output and the option~\CodeIn{-DskipTests} to skip
execution of tests.  We observed that only bootstrapping execution
suffices to discover which parallel configuration modes Maven will
actually use.  According to Maven's
documentation~\cite{maven-surefire-plugin}, any parallelism
configuration depends either on (1) the parameter \CodeIn{parallel} to
define the parallelism mode within a JVM followed by the parameter
\CodeIn{threadCount} or (2) the parameter
\CodeIn{forkCount}\footnote{This parameter is named \CodeIn{forkMode}
  in old versions of Maven Surefire} to define the number of forked
JVMs. For each execution, we capture theses parameters and their
values to map to one of the possible parallelization modes.  For
instance, if a given project contains a module with the parameter
\CodeIn{<forkCount>1C</forkCount>}, the possible classifications are
\ForkSeq{} or \ForkParMeth{} depending on the presence and the value
of the parameter \CodeIn{parallel} (\ie, the presence of
\CodeIn{<parallel>methods</parallel>} results in the parallel mode
\ForkParMeth{}).  Large projects may contain several test suites
distributed on different Maven modules with different
configurations. In this case, we collect the Maven output from each
module discarding duplicates. For instance, if a project contains two
modules with the same parallelism mode, we count only one occurrence
to not inflate our results.

%% Column
%% ``Group'' indicates the test cost group (see
%% Figure~\ref{fig:rq1-barplot}), the

%\begin{figure}[t!]
\begin{wrapfigure}{r}[0pt]{0pt}%0.525\linewidth
    \footnotesize
    \centering
    \setlength{\tabcolsep}{2.5pt}
%    \resizebox{.48\textwidth}{!}{%
    \begin{tabular}{lrr}
        \toprule
        \emph{Subject} & \emph{\# of modules} & \emph{Mode}\\%
        \midrule%
        \Comment{BounceStorage }Chaos\Comment{ HTTP Proxy} & 1 &  \ParClassSeqMeth{}\\%
        \Comment{Apache }Flink & 74 & \ForkSeq{} \\%        
        \Comment{JenkinsCI }Gerrit\Comment{ Trigger Plugin} & 1 & \ForkSeq{}\\%
        \Comment{Spotify }Helios & 8 & \ForkSeq{}\\%
        Javaslang & 3 & \ParClassParMeth{}\\%
        Jcabi\Comment{ Github} & 1 & \ParClassParMeth{}\\%        
        \Comment{Hazelcast }Jet & 7 & \ForkSeq{}\\%
        \Comment{Apache Logging }Log4J2 & 25 & \ForkSeq{}\\%
        \Comment{Jankotek }MapDB & 1 & \ParClassParMeth{}\\%        
        \bottomrule%
    \end{tabular}
%    }
    \caption{Subjects with parallel test execution enabled by
    default.}
    \label{tab:freqmodes-dynamic}
    %\end{figure}
\end{wrapfigure}      
Figure~\ref{tab:freqmodes-dynamic} shows the projects we idendified
where parallelism is enabled by default.  Column ``\emph{Subject}''
indicates the name of the project, column ``\emph{\# of modules}''
indicates the number of detected modules containing tests, and column
``\emph{Mode}'' shows the parallel configuration mode used in those
modules (see Section~\ref{sec:modes}).  It came as a surprise the
observation that no project used distinct configurations in their
modules. Considering our set of \numMedLong{} projects, we found that
only \textbf{\numProjectsPar{}} of those projects had parallelism
enabled by default, with only configurations \ParClassSeqMeth{},
\ParClassParMeth{}, and \ForkSeq{} being used.  Configurations
\ParClassParMeth{} and \ForkSeq{} were the most popular among these
cases.  Note that these results under-approximate real usage of
parallelism as we used default parameters in our scripts to spawn the
Maven process.  That decision could prevent execution of particular
test modules.  
%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.32\textwidth]{plots/barplot-modes-dynamic.pdf}
%    \caption{\label{fig:freqmodes-dynamic}\Fix{fix
%    caption}Distribution of parallel modes identified dynamically in a
%    subset of \numProjectsPar{} projects.  A project may have support
%    to more than one parallel mode. Also, a project may run only a
%    subset tests in parallel by default.}
%\end{figure}


\subsubsection{Presence of parallelism configurations in build files}
\label{sec:rqC-2}
Given the inherent limitation of dynamic monitoring to find evidence
of parallelism, we also looked for indication of parallelism in
\emph{pom.xml} files\Comment{ in the same sample set of \numMedLong{}
  projects}. In principle, some projects can use parallel execution
although that is only activated when developers pass certain
parameters to the build process. For instance, it is possible to
create in Maven multiple configurations in the same build file and
select dynamically which one should be used.

\Mar{I have no clue what you mean here... Why should I care about
  space on build files?  I thought we did this check
  for every project and module.  $\rightarrow$}\Fix{To reduce the
  space of \Fix{x} build files, we considered a subset of \Fix{y}
  build files with the following criteria}: the given \pomf{} file has
\CodeIn{maven-surefire-plugin} plugin declaration and it has an
explicit \CodeIn{configuration} declaration. We automatically parsed
these \Fix{y} files and we used the same approach to classify the
parallelization mode from the parsed configurations as in our previous
analysis. In an initial analysis, we noticed that some projects do not
declare a static value in some parameters, instead, these projects
inform the actual value in the command line. To handle this case, we
computed all valid values for the parameter.

We found ten projects that use parallelism using this methodology.
Compared to the set of projects listed in Figure~\ref{tab:speedup}, we
found two new projects, namely: \CodeIn{Google Cloud\Comment{
    Platform} DataflowJavaSDK} (using configuration C3) and
\CodeIn{Mapstruct} (using configuration \ForkSeq{}).  Curiously, we
also found that project \CodeIn{Jcabi} was not detected using this
methodology.  That happened because \Mar{please, explain why}.
Considering the static and dynamic methods together, we found a total
of 11 distinct projects using parallelism, corresponding to the union
of the two subject sets.

\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQC{}:}~\emph{Results indicate that test
        suite parallelization is underused.  Overall, only
        \percentParallel{} of costly projects (11 out of \numMedLong)
        use parallelism.}
  \end{minipage}
}
\end{center}

%False positive can happen because of comments, for instance.  
%To eliminate the cases of false positives and also to categorize 
%true positive cases, we complemented the initial mining step with a 
%manual inspection of files.
%% settings); the second step (inspection) consists in a manual
%% inspection to confirm the presence of parallelism settings in the
%% build file and classify them according to the parallelism level.
%% Figure \Fix{removed} describes the discovery step: we list the paths
%% of all build files and filter only the files that contain any of the

%% Figure~\ref{tab:inspection-table} summarizes our results.
%% \Fix{The first column indicates the group of projects according to
%% their time cost.  The second column indicates the number of build
%% files per group.  The last column indicates the ratio of projects with
%% parallelization settings.  From the \numMedLong{} subjects, we found
%% \pomMedLong{} \pomf{} files.  The \numPomMatched{} configurations are
%% distributed across \numProjectsPar{} projects from our sample.}

%% % \emph{From these results we found that $\sim$51\% of medium and
%% % long-running projects do not use parallel features to run test
%% % suites.}\Mar{please make it consistent with research
%% % question}\Mar{explain this is over(under)-estimated...}
%% \begin{figure}[ht!]
%%     \centering
%%     \resizebox{.48\textwidth}{!}{%
%%     \begin{tabular}{llcl}
%%         \toprule
%%         Group & Subject & \# of modules & Mode\\%
%%         \midrule%
%%         Long   &JenkinsCI Gerrit Trigger Plugin& 1 & \ForkSeq\\%
%%         Medium &Bouncestorage Chaos Http Proxy & 1 & C2\\%
%%         Medium &Javaslang & 1 & C3\\%
%%         Medium &Apache Flink & 1 &\ForkSeq\\%
%%         Medium &Apache Logging Log4J2 & 3 & \ForkSeq{}\\%
%%         Medium &Google Cloud Platform DataflowJavaSDK & 1 & C3\\%
%%         Medium &Hazelcast Jet & 1 & \ForkSeq\\%
%%         Medium &Jankotek MapDB & 1 & C3\\%
%%         Medium &Mapstruct & 1 & \ForkSeq\\%
%%         Medium &Spotify Helios & 3 & \ForkSeq\\%
%%         \bottomrule%
%%     \end{tabular}}
%%     \caption{Subjects with parallelization configurations in build files.}
%%     \label{tab:inspection-table}
%% \end{figure}

%% \begin{figure}[ht!]
%%     \centering
%%     \begin{tabular*}{0.48\textwidth}{@{\extracolsep{\fill}}ccc}
%%         \toprule
%%         \multirow{2}{*}{Group} %1st row, 1st cell
%%             & \multirow{2}{*}{\# \pomf{}}
%% 	    & \# \pomf{} matched\\
%%         % 2nd row - empty cell
%%             & % empty cell
%%             & / total\\%
%%         \midrule%
%% 	Long   & \numPomLong{} & 4 / \numLong{}\\%
%% 	Medium & \numPomMed{} & 6 / \numMed{}\\%
%%         \midrule%
%%         Total % last row, first cell
%%             & \pomMedLong{}
%%             & \numProjectsPar{} / \numMedLong{}\\%
%%         \bottomrule%
%%     \end{tabular*}
%%     \caption{Presence of parallelization settings in build files: the
%%     first column indicates the group of projects according to their
%%     time cost; the second column is the subset of files with parallelization
%%     keywords; the last column indicates the ratio of projects with
%%     parallelism support.}
%%     \label{tab:inspection-table} 
%% \end{figure}
%% \Jbc{rework this... $\rightarrow$} From the \numProjectsPar{} projects
%% identified above, we investigated further the \numPomMatched{}
%% build files with parallel settings.  We analyzed the support and
%% distribution of parallel modes from this subset of projects. To
%% calculate the distribution of parallel modes, we considered only the
%% presence of the mode in at least one of the project settings.  Recall
%% that a build file may contain more than one parallel setting and a
%% project may contain several sub-modules with build files.  In case the
%% value of a parallel option is resolved dynamically (\eg, via
%% command-line argument or system variable) we compute all modes related
%% to the option. For instance, depending on the value, the
%% \CodeIn{parallel} option can be \Seq{} (\CodeIn{none}),
%% \ParClassSeqMeth{} (\CodeIn{classes}), \SeqClassParMeth{},
%% (\CodeIn{methods}), and \ParClassParMeth{} (\CodeIn{all}).
%% Figure~\Fix{fig:freqmodes-static} summarizes our findings.
%% \Fix{Missing conclusion: Fork the most used configuration}
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.32\textwidth]{plots/barplot-modes-static.pdf}
%% 	\caption{\label{fig:freqmodes-static}\Luis{This is wrong, it
%% 	should be \textbf{CF0} instead of \textbf{CL0}}Distribution of parallel modes
%%     identified statically in a subset of \numProjectsPar{} projects.
%%     A project may have support to more than one parallel mode.}
%% \end{figure}

\begin{itemize}
	\item \numRQE{}. \textbf{\RQE{}}
\end{itemize}

To understand why developers prefer not use parallel test execution,
we considered \numNonParallel{} subjects that do not use parallel
configuration as default test execution from Section~\ref{sec:rqC-1} and elaborated a
quick survey to extent the comprehension about why developers do not
use parallel configuration. \Luis{describe how we created the survey}.
We first retrieve the last 20 commits from the project repository and
got the developers e-mails. In total, we sent an e-mail to
\emailsSent{} developers that recently contributed to the 
\numNonParallel{} projects, and we did not found any e-mail from
\Fix{??} projects using \CodeIn{git} tool.
\Luis{Here I want to say that I could not found any valid e-mail using
\textit{git log}}
There were \emailsAnswered{} replies from \emailsProjectsAnswered{}
projects, and \emailsFalseAnswers{} developers was not able to 
explain how the testing works or was not able to answer the survey.

The survey sent was composed by three open questions and one closed
question: (I) How long does it take for test to run in your
environment? Can you briefly define your setup? (II) Do you confirm
that your project does not run in parallel? (III) Select a reason for
not using parallelization: (a) I did not know it was possible; (b) I
was concerned with concurrency issues; (c) I use a continuous
integration server; (d) Some other reason. Please elaborate.

%% \begin{enumerate}
%% 	\item How long does it take for test to run in your
%%		environment?
%%	\item Can you briefly define your setup?
%%	\item Do you confirm that your project does not run in
%%		parallel?
%%	\item Select a reason for not using paralellization:
%%		\begin{enumarate}
%%			\item I did not know it was possible;
%%			\item I was concerned with concurrency issues;
%%			\item I use a continuous integration server;
%%			\item Some other reason.
%%		\end{enumerate}
%% \end{enumerate}

Based on the answers from the developers, we confirmed that most of
the subjects execution time was compatible with the results found at
Section~\ref{sec:rqA}. From Question I, we noticed that \emailsCI{}
developers confirmed the use of an continuous integration server to
run tests, which \emailsDistributed{} of them is using a distributed
continuous integration server as an alternative to parallelism.
From a local development, \emailsLocal{} developers answered that they
run tests in a local machine, and \emailsNotDescribed{} developers did
not described their environment.
From Question II, we noticed that \emailsSequential{} confirmed that
they do not execute tests in parallel, and \emailsParallel{} did not
confirmed, however, the developers confirmed the need of an extra
parameter at the command line to execute in parallel.
From Question III, \emailsA{} of developers do not know that
parallelism is an available configuration for Maven Surefire,
\emailsB{} of developers do not use parallel testing concerned with
concurrency issues, \emailsD{} of developers uses a continuous
integretion server for run tests while working \Luis{here I want to
say that they use it for something like "non-blocking testing" while
developing in a local machine}, \emailsD{} of developers thinks that
it is not worthy the effort, and \emailsNA{} of developers did not
answered the last question, or did not knew an answer.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{plots/survey.pdf}                                                                                                                                                   
     \caption{\Luis{todo}}
\end{figure}


\begin{center}
\fbox{
	\begin{minipage}{8cm}
		\textit{Answering \numRQE{}:~\emph{\Luis{summarize
		findings...}}}
	\end{minipage}
}
\end{center}

\begin{itemize}
    \item \textbf{\RQF{}}
\end{itemize}

\begin{figure}[h!]
\centering
\resizebox{.48\textwidth}{!}{%
\begin{tabular}{lrrrrrr}
\toprule
\emph{Subject} & \Seq{} & \SeqClassParMeth{} & \ParClassSeqMeth{} & \ParClassParMeth{} & \ForkSeq{} & \ForkParMeth{}\\%
\midrule%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 1.47m & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\%
\bottomrule%
\end{tabular}}
\caption{\Fix{Tabela de eficiencia}}
\label{tab:rq6-efficiency}
\end{figure}

\begin{figure}[h!]
\centering
\resizebox{.48\textwidth}{!}{%
\begin{tabular}{lcrrrrr}
\toprule
\emph{Subject} & \emph{\# of tests} & \SeqClassParMeth{} & \ParClassSeqMeth{} & \ParClassParMeth{} & \ForkSeq{} & \ForkParMeth{}\\%
\midrule%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
BounceStorage \textbf{Chaos} HTTP Proxy  & 8765 & 10\% & 10\% & 10\% & 10\% & 10\%\\%
\bottomrule%
\end{tabular}}
\caption{\Fix{Tabela de flakyness}}
\label{tab:rq6-efficiency}
\end{figure}


\begin{center}
\fbox{
\begin{minipage}{8cm}
\textit{Answering \numRQF{}:~\emph{\Jbc{summarize findings...}}}
\end{minipage}
}
\end{center}


\subsection{Threats to Validity}

\Fix{Jean and Luis should write this. This is really important for an
  empirical study.  You need to undertand main sources of threat:
  internal, external, and construct.  Read this document for the
  basics: http://www.statsdirect.com/help/basics/validity.htm.  Read
  this document for an example:
  http://www.cin.ufpe.br/~damorim/publications/perez-etal-icst2017.pdf}

\subsection{Lessons Learned}

\Fix{Marcelo can write this...}

%%To evaluate the distribution of execution time per project, we sorted
%%the test cases by decreasing order of elapsed time and calculated the
%%number of tests executed in 90\% of the total time. Later, we reported
%%the \Fix{balance} of execution time by dividing the number of tests
%%that represents 90\% of the execution time by the number of tests
%%cases. For instance, a balance of 50\% indicates \Fix{...}.  \Fix{We
%%collected the elapsed time from test cases for each generated report.
%%Maven Surefire generates an XML report with execution information
%%(\eg, number of skipped tests and elapsed time) per test suite
%%\Jbc{Should I use the previous sentence as a footnote or should I
%%delete it?}. We noticed that some test cases reported an elapsed time
%%of zero: since the reported time is in milliseconds, some tests may
%%execute in a shorter time. \Fix{..to be continued...}}. Results
%%indicate that \Fix{...}.
%%
%%\begin{figure}[h!]
%%    \centering
%%    \includegraphics[width=0.4\textwidth]{results/plots/balance.pdf}
%%    \caption{\Fix{balance}}
%%\end{figure}

%% \subsection{Answering research question RQ3}
%% \label{sec:rqThree}
%% 
%% \begin{itemize}
%%     \item \RQB
%% \end{itemize}
%% 
%% To evaluate the distribution of CPU and IO intensive test suites from
%% the sample set, we used the command \emph{sar} to monitor the system
%% activity in background while tests ran. \emph{Sar} is a command that
%% collects and reports statistics (\eg, percentage of IO waiting and
%% usage of CPU in user mode) based on the kernel activity and it is
%% highly configurable to collect detailed information (\eg, usage of a
%% specific processor core or percentage of network interface
%% utilization). We configured \emph{sar} to report \Fix{...explain how
%% we executed and what fields we are interested}. \Fix{explain fields}.
%% Figure \Fix{A} shows the distribution of subjects grouped in intervals
%% of \Fix{W}\% of CPU utilization. Results indicates that \Fix{...}
%% 
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.4\textwidth]{results/plots/cpuness.pdf}
%%     \caption{\Fix{cpu usage}}
%% \end{figure}
%% 
%% \Comment{we proposed the definition of \emph{cpuness}
%% computed as the follow: $((user\_t + system\_t) / elapsed\_t) * 100$,
%% where \emph{user\_t} is the elapsed time of execution in \emph{user
%% mode}, \emph{system\_t} is the elapsed in \emph{kernel mode}, and
%% \emph{elapsed\_t} is the elapsed time to finish the execution. We
%% measured the \emph{cpuness} of each regression test \Fix{...elaborate
%% the meaning of cpuness} \Fix{Describe how I measured user, system and
%% "wall" time}.  \Fix{Explain results}.  \Fix{show plots}}
%% 

%%  LocalWords:  RQ occurence parallelization Tradeoffs API readme th
%%  LocalWords:  mvn clearcut escapeinside xleftmargin untestable LTS
%%  LocalWords:  framexleftmargin CPUs Tahr sysstat gh Vagrantfile
%%  LocalWords:  javadoc isolcpus JUnit's JUnitCore Gligoric boxplots
%%  LocalWords:  outliers apache uber chaperone facebookarchive
%%  LocalWords:  linkbench
