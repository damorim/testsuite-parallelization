\section{Objects of Analysis}
\label{sec:subjects}

%% We evaluated the characteristics of test suites in open-source
%% development from a sample set of Java projects from \github{}.  We are
%% interested to evaluate non-trivial test suites from popular projects
%% that are in activity.

%% , which is used on
%% \github{} to indicate appreciation of a user to a
%% project~\cite{github-stars},

We used \github{}'s search API~\cite{githubsearch} to identify
projects that satisfy the following criteria: (1) the primary language
is Java\footnote{In case of projects in multiple languages, the
  \github{} API considers the predominant language as the primary
  language.}, (2) the project has at least 100 stars, (3) the latest
update was on or after January 1st, 2016, and (4) the \emph{readme}
file contains the string \emph{mvn}.  We focused on Java for its
popularity.  Although there is no clearcut limit on the number of
\github{} stars~\cite{github-stars} to define relevant projects, we
observed that 100 was enough to eliminate trivial subjects. The third
criteria serves to skip projects without recent activity. The fourth
criteria is an approximation to find Maven projects.\Comment{ The
  rationale is that if the string \emph{mvn} exists in the
  \emph{readme} file, it may represent a Maven call (\eg, to compile
  or to test the project).} We focused on Maven for its popularity on
Java projects.  Important to highlight that, as of now, the
\github{}'s search API can only reflect contents from README file (not
other code elements); it does not provide a feature to search for
projects containing certain files in the dir structure (\eg{},
\emph{pom.xml}).  Figure~\ref{fig:subject-query} illustrates the query
to the \github{} API as an HTTP request.  

\vspace{1ex}
\begin{figure}[h!]
\centering
\scriptsize
\lstset{
    escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c, numberstyle=\tiny,
    deletekeywords={true}
}
\begin{lstlisting}
https://api.github.com/search/repositories?q=language:java
        +stars:>=100+pushed:>=2016-01-01
        +mvn%20in:readme+sort:stars
\end{lstlisting}
    \caption{\label{fig:subject-query} Query to the \github{} API for
    projects with the following criteria: (1) Java, (2) at least 100
    stars, (3) updated on January 1st, 2016 (or later), (4) contains
    the string \emph{mvn} in the \emph{readme} file. Output is
    paginated in descending order of stars.}
\end{figure}

After obtaining a list of
potential projects, we filtered those containing a \pomf{} file in the
root directory.  A Maven project may contain several sub-modules with
multiple \pomf{} files.\Comment{  We based our methodology to select
experimental subjects on other recent studies in testing and
debugging~\cite{gligoric-etal-issta2015,perez-etal-icst2017}.}
As of March 25th 2017, our search criteria returned \SubjectsGithub{}
subjects. Figure~\ref{fig:subjects} summarizes our sample set. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{plots/subjs.pdf}
    \caption{\label{fig:subjects}We fetched \SubjectsGithub{} popular
    projects hosted on \github{}. From this initial sample, we ignored
    \SubjectsGithubNotMaven{} projects without Maven support and
    \SubjectsGithubNotTestable{} untestable projects\Comment{, and
    \SubjectsGithubFlaky{} projects with flaky tests}. We considered
    \numSubjs{} projects to conduct our study.}
\end{figure}

From \SubjectsGithub{} downloaded projects, \SubjectsGithubNotMaven{}
projects were not Maven or did not have a \pomf{} in the root
directory, \SubjectsGithubNotTestable{} projects were untestable
because of missing dependencies or incompatible testing environment,
for example, and \SubjectsGithubFlaky{} projects were ``flaky''.
\Jbc{Check if this is ok $\rightarrow$} We executed each project's
test suite for three times to identify projects containing flaky tests
and ignored these tests (\ie, using the \CodeIn{@Ignore} annotation
from JUnit) to avoid introducing noise in our experiments.  Our final
set of subjects consists of \numSubjs{} projects.


\subsection{Setup and Replication}
\label{sec:setup}

To run our experiments, we used a Core i7-4790 (3.60 GHz) Intel
processor machine with eight virtual CPUs (four cores with two native
threads each) and 16GB of memory, running Ubuntu 14.04 LTS Trusty Tahr
(64-bit version).  Software settings include \Comment{the Linux
  \emph{sysstat} package to measure performance, }git to fetch
subjects, Java 8, and Maven 3.3.9 to build and test subjects. We used
Python\Comment{ 3.4}, Bash, R and Ruby\Comment{ 2.3} to process the
data and generate plots.  All source artifacts are publicly available
for replication (on request)\Comment{ at \Fix{create gh-pages}}.  This
includes supporting scripts (\eg, the script that test subjects and
generates raw analysis data) and the full list of projects. \Comment{,
  and a \emph{Vagrantfile} to emulate our hardware and all software
  dependencies.}


\section{Evaluation}
\label{sec:eval}

%% We are interested in understanding the prevalence of time-consuming
%% test suites and main sources of execution cost. We want to understand
%% how the execution cost is distributed on test cases within a test
%% suite and how developers approach test execution. Based on that, we
%% study parallelization of testing frameworks and build systems.  More
%% precisely, we investigate how prevalent test parallelization is, the
%% potential for improving execution cost, issues of flakiness that
%% hinders the use of parallelization, and how to address those issues.
%% More specifically, we pose the following research questions:

%% The first research question addresses the prevalence of long-running
%% test suites. We are interested to know if costly test suites are
%% common in open-source projects.  The second research question
%% addresses the relationship of test cases and the overall execution
%% cost: we are interested to investigate how the execution time is
%% distributed among test cases.  In the third research question, we
%% investigate if developers consider low-level parallelism features
%% available out-of-the-box to amortize test execution (see
%% Section~\ref{sec:modes}). In addition, we want to identify what
%% configurations are often used and why they are more popular (if any).
%% The fourth research question addresses the impact of low-level
%% parallelism on test execution from projects in our sample set. We want
%% to identify subjects that already use test parallelization and compare
%% their performance in contrast to sequential execution. In addition, we
%% are interested in evaluating the performance of sequential test suites
%% with different parallelization settings.
%% Finally, the fifth research question discusses the limitations and
%% insights to overcome the pitfalls of parallelization.

%% \Comment{
%%     \Fix{distribution of execution time per test case. For each subject
%%     identified in the first research question, we investigate how
%%     balanced is the cost of the test suite in contrast to the cost of
%%     test cases and if there are subjects where the time cost is mostly
%%     dominated by a small fraction of test cases.} \Fix{The third research
%%     question addresses the distribution of regression tests according
%%     to the use of computational resources.  We are interested in
%%     investigating if regression test suites are CPU intensive and if there
%%     are opportunities to improve performance. The RQ4 addresses}
%%     \Fix{...elaborate...}

%%     The rationale is that if the time cost of a regression test is equally
%%     distributed among test cases, the execution cost could be potentially
%%     improved by running tests in parallel (in contrast to the scenario
%%     where only one test case dominates most of the execution time).
%% }

We pose the following research questions, organized by the dimensions
of analysis we presented in Section~\ref{sec:intro}.


%% Feasibility
\newcommand{\numRQFeasibilityOne}{RQ1}
\newcommand{\RQFeasibilityOne}{How prevalent is the occurence of time-consuming
  test suites\Comment{ in open-source projects}?}

\newcommand{\numRQFeasibilityTwo}{RQ2}
\newcommand{\RQFeasibilityTwo}{How time is distributed across test cases?}

%% Adoption
\newcommand{\numRQAdoptionOne}{RQ3}
\newcommand{\RQAdoptionOne}{How popular is test suite
  parallelization\Comment{ in open-source projects}?}

\newcommand{\numRQAdoptionTwo}{RQ4}
\newcommand{\RQAdoptionTwo}{What are the main reasons that prevent developers
  from using test suite parallelization?}

%% Speedups
\newcommand{\numRQSpeedupOne}{RQ5}
\newcommand{\RQSpeedupOne}{What are the speedups obtained with parallelization
  (in projects that actually use it)?}

\newcommand{\numRQSpeedupTwo}{RQ6}
\newcommand{\RQSpeedupTwo}{How test execution scales with the number of
  available CPUs?}

%% Issues
\newcommand{\numRQIssuesOne}{RQ7}
\newcommand{\RQIssuesOne}{How parallel execution configurations affect testing
  costs and flakiness?}


\setlist[itemize]{leftmargin=1em}
\begin{itemize}
\item Feasibility
  \begin{itemize}
  \item \textbf{\numRQFeasibilityOne.} \RQFeasibilityOne
  \item \textbf{\numRQFeasibilityTwo.} \RQFeasibilityTwo    
  \end{itemize}
\item Adoption
  \begin{itemize}
  \item \textbf{\numRQAdoptionOne.} \RQAdoptionOne    
  \item \textbf{\numRQAdoptionTwo.} \RQAdoptionTwo
  \end{itemize}
\item Speedups
  \begin{itemize}
  \item \textbf{\numRQSpeedupOne.} \RQSpeedupOne
  \item \textbf{\numRQSpeedupTwo.} \RQSpeedupTwo
  \end{itemize}      
\item Issues
  \begin{itemize}
  \item \textbf{\numRQIssuesOne.} \RQIssuesOne    
  \end{itemize}
\end{itemize}

%%\newcommand{\RQFeasibilityTwo}{What is the distribution of CPU and IO bound
%%regression test suites from the sample set?}
%%
%%\newcommand{\RQAdoptionOne}{How uniformly distributed is the execution time
%%across test cases in costly projects?}
%%
%%\newcommand{\RQAdoptionTwo}{How often developers use the parallelism features
%%from build systems to improve runtime performance?}



\subsection{Feasibility}
\label{sec:rqA}
\label{sec:rqB}

\begin{itemize}
    \item \numRQFeasibilityOne{}. \textbf{\RQFeasibilityOne}
\end{itemize}
%\Jbc{The following steps may change $\rightarrow$}

To evaluate prevalence of projects with costly test suites, we
considered the \numSubjs{} testable subjects from
Figure~\ref{fig:subjects}.  Figure~\ref{fig:mvn-execution} illustrates
the script we used to measure time.

We took the following actions to isolate our environment from
measurement noise.  First, we observed that some test tasks called
test-unrelated tasks (\eg, \emph{javadoc} generation and static
analyses) that could interfere in our time measurements.  To address
that potential issue, we inspected Maven execution logs from a sample
including a hundred projects prior to running the script from
Figure~\ref{fig:mvn-execution}.  The tasks we found were ignored from
execution (lines 1-3).  Furthermore, to avoid noise from operating
system events, we configured our workstation to run only essential
services.  The machine was dedicated to our experiments and we
accessed it via SSH. In addition, we configured the \CodeIn{isolcpus}
option from the Linux Kernel \cite{linux-kernel} to isolate six
virtual CPUs to run our experiments, leaving the remaining CPUs to run
OS processes~\cite{isolcpus-use}.  The rationale for this decision is
to prevent context-switching between user processes (running the
experiment) and OS-related processes.  Finally, to make sure our
measurements were fair, we compared timings corresponding to the
sequential execution of tests using Maven with that obtained with
JUnit's default \CodeIn{JUnitCore} runner, invoked from the command
line.  Results were very close.

The main loop (lines 5-11) of the script in
Figure~\ref{fig:mvn-execution} iterates over the list of subjects and
invokes Maven multiple times\Comment{ to isolate cost of running
  tests} (lines 7-9).  It first compiles the source and test files
(line 7), make all dependencies available locally (line 8), and then
runs the tests in offline mode as to skip the package update task,
enabled by default (line 9). After execution, we used a regular
expression on the output log to extract elapsed time (line 10).

%% We executed each project's test suite for
%% \Fix{5} times through Maven and directly through JUnit each project

\input{codes/evaluation}

We ran the test suite for each subject three times, reporting averaged
execution times in three ranges: tests that run within a minute
(\shortg{} group), tests that run in one to five minutes (\medg{}
group), and tests that run in five or more minutes (\longg{}
group). We followed a similar methodology to group projects by time as
Gligoric~\etal{}~\cite{gligoric-etal-issta2015} in their work on
regression test selection.\Comment{ and added the \medg{} group due to
  the variability of the time cost from subjects out of the \shortg{}
  group} Figure~\ref{fig:rq1-barplot} shows the number of projects in
each group.  As expected, \longg{} and \medg{} projects do not occur
as frequently as \shortg{} projects.  However, they do occur in
relatively high numbers.

Figure~\ref{fig:rq1-boxplot} shows cost distribution of test suites in
each group as boxplots.  Note that the y-ranges are different.  The
distribution associated with the \shortg{} group is the most
unbalanced (right skewed)\Comment{ with outliers closed to the \medg{}
  group}.  The test suites in this group ran in 15 or less seconds for
over 75\% of the cases.  Such scenarios constitute the majority of the
cases we analyzed.  Considering the groups \medg{} and \longg{},
however, we found many costly executions.  Nearly 75\% of the projects
from the \medg{} group take over 3.5 minutes to run and nearly 75\% of
the projects from the \longg{} group take $\sim$20 minutes to run.  We
found cases in the \longg{} group were execution takes over 50 minutes
to complete, as can be observed from the outliers (dots) in the plot.

%% the median from the
%% \medg{} group is nearly two minutes and most of the subjects run in
%% less than four minutes; most of the \longg{} group runs in less than
%% 25 minutes but has outliers that require more than 50 minutes to
%% execute.



\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.182\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/barplot-timecost.pdf}
        \caption{\label{fig:rq1-barplot}}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplot-timecost.pdf}
        \caption{\label{fig:rq1-boxplot}}
    \end{subfigure}%
    \caption{(a) Subjects grouped by time cost ($t$): short run ($t <
    1m$), medium run ($1m \le t < 5m$), and long run ($5m \le t$); (b)
    Distribution of time cost by group.}
\end{figure}

%% Figure~\ref{fig:rq1-barplot} is a lower bound estimation of cost
%% because some tests may finish earlier than expected due to existing
%% test failures in the revision we downloaded.

It is important to note that we under-estimated cost in our
experiments for two main reasons.  First, some tests may finish
earlier than expected due to the observed test failures in some of the
revisions we downloaded.  From the \numSubjs{} testable projects,
\numSubjsPass{} successfully executed all tests and \numSubjsFail{}
reported some test failures.  Second, some projects may omit
long-running tests on their default execution. For instance, the
project \CodeIn{apache.maven-surefire} runs all unit tests in a few
seconds.  According to our criteria, this project is to be classified
as \shortg{} but a closer look reveals that only smoke tests are run
by default in this project.  Time-consuming integration and system
tests are only accessible via custom parameters, which we do not
handle in our experimental setup.  We enabled such parameters for this
specific project and observed that testing time goes to nearly 30
minutes.  For simplicity, we considered only the tests executed by
default.

\vspace{1ex}
\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQFeasibilityOne{}:}~\emph{We conclude that
      time-consuming test suites are relatively frequent in
      open-source projects.  We found that \percentMedLongRunning{} of
      the \numSubjs{} projects we analyzed (\ie{}, over 1 in every 5
      projects) take at least 3 minutes to run and
      \percentLongRunning{} take at least 5 minutes to run.\Comment{
        (\ie, \numMedLong{} projects from \medg{} and \longg{}).}}
\end{minipage}
}
\end{center}
\vspace{1ex}


\begin{itemize}
    \item \numRQFeasibilityTwo. \textbf{\RQFeasibilityTwo}
\end{itemize}

Section~\ref{sec:rqA} showed that medium and long-running projects are
not uncommon, accounting to nearly \percentMedLongRunning{} of the
\numSubjs{} projects we analyzed.  Research question \numRQFeasibilityTwo{}
measures the distribution of test costs in test suites as to estimate
(lack of) potential of obtaining speedups with parallelization.  In
the limit, if cost is dominated by a single test from a large test
suite, it is unlikely that parallelization will be beneficial as a
test method is the smallest working unit in test frameworks.

%% However, avoiding
%% frequent context switches is another factor to consider.  For example,
%% assuming there are at least two CPUs available for execution, cost can
%% be cut in half if two tests in a large test suite dominate execution
%% time and these tests are assigned to different CPUs.

%% It is therefore important to
%% speedup regressing testing in open-source projects.\Comment{not only
%%   to huge projects as those from Google~\cite{google-tap,google-ci}
%%   and Microsoft~\cite{prasad-shulte-ieee-microsoft-ci}.}



%% For the case
%% where cost is distributed more evenly across test cases, one expects
%% that speedups will be a function of the number of cores.
%% These contradictory forces, pushing number of tests and cost
%% of each test up and down, make prediction of effectiveness challenging.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-long.pdf}
      \caption{\label{fig:longtcost}Long group.}
    \end{subfigure}\\
    \vspace{2ex}
    \begin{subfigure}{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/testcost-medium.pdf}
      \caption{\label{fig:medtcost}Medium group.}
    \end{subfigure}
    %% \vspace{2ex}
    %% \begin{subfigure}{0.5\textwidth}
    %%   \centering
    %%   \begin{tabular}{rrrr}
    %%     \toprule
    %%     & $\sigma\leq1$ & $1<\sigma\leq5$ & $\sigma\ge5$ \\
    %%     \midrule    
    %%     Long   &  7 & 15 & 12 \\
    %%     Medium & 22 & 19 & 7 \\
    %%     \bottomrule
    %%   \end{tabular}
    %%   \caption{\label{fig:sd}Standard deviation ($\sigma$) of test case running times.}
    %% \end{subfigure}
    \caption{\label{fig:time-distributions}Time distributions.}%
\end{figure}

\sloppy Figures~\ref{fig:longtcost} and~\ref{fig:medtcost} show the
time distribution of individual test cases per project.  We observed
that the average median value of execution cost for a test was
relatively small (dashed horizontal red lines), namely 0.31s for
\medg{} projects and 0.23s for \longg{} projects.  The standard
deviations associated with each distribution were relatively
low.\Comment{ Figure~\ref{fig:sd} shows the number of projects within
  specific ranges of $\sigma$ values.}  We noted a small number of
cases of CPU monopolization.  For example, the highest value of
$\sigma$ occurred in \CodeIn{uber\_chaperone}, a project from the
medium group.  This project contains only 65 tests, 62 of which take
less than 0.5s to run, one of which takes nearly 3s to run, and two of
which take $\sim$40m to run.  For this project, 99.2\% of the
execution cost is dominated by only 3\% of the tests; without these
two costly tests this project would have been classified as
short-running.  A closer inspection in the data indicates that the
project \CodeIn{uber\_chaperone} was a corner case; we did not find
other projects with such extreme time monopolization profile.  Project
\CodeIn{facebookarchive\_linkbench} is also classified as long-running
and has the second highest value of $\sigma$.  For this project,
however, cost is distributed more smoothly across \Fix{529} tests, of
which \Fix{119 (23\%)} take more than \Fix{1s} to run with the rest of
the tests running faster.

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.15\textwidth}
    \centering
    \includegraphics[width=.85\textwidth]{plots/boxplots-testcases.pdf}
    \caption{\label{fig:size-testsuites}Size of test suites.}
  \end{subfigure}
  ~
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{plots/scatter-testcost.pdf}
    \caption{\label{fig:scattercost}Size versus running time of
      test suites.}
  \end{subfigure}
  \caption{\label{fig:time-versus-size}Relating size and time.}%
\end{figure}

%\Mar{$\leftarrow$ show stats to indicate discrimination of
%  two distributions}

Figure~\ref{fig:time-distributions} showed that the average median
times were similar for \medg{} and \longg{}-running test suites.
Results indicate that the difference in overall running times of
projects in those groups was mainly justified by the number of test
cases as opposed to the individual costs of test cases.
Figure~\ref{fig:size-testsuites} shows the difference in the
distribution of test suite sizes across groups.  This figure indicates
that long projects, albeit having a wider inter-quartile range (middle
50\% projects in this group are less predictable), have a higher
median and much higher average number of test cases.  Furthermore, we
noted a strong positive correlation between running time and number of
test on projects in the \longg{} group.  Considering the \medg{}
group, the correlation between these two variables was weak.
Figure~\ref{fig:scattercost} illustrates this correlation.

%% however, weak in
%% the suggesting that saving time in this group with test suite
%% parallelization may be more challenging as relatively fewer tests
%% dominate overall execution time.  Figure~\ref{fig:scattercost} shows
%% these results.

%% This 
%% indication that it is more beneficial to parallelize long projects as
%% cost is spread across many

\vspace{1ex}
\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQFeasibilityTwo{}:}~\emph{Overall, results indicate that
    projects with a very small number of tests monopolizing end-to-end
    execution time were rare.}
\end{minipage}
}
\end{center}
\vspace{1ex}

%% We are interested to know whether
%% most of the execution cost of a subject is dominated by a small subset
%% of test cases or if the cost is nearly equally distributed. 

%% We also evaluated the dispersion of time distributions (one
%% distribution per project) to answer research question \numRQFeasibilityTwo{}.  To
%% measure dispersion \emph{across} projects we used Relative Standard
%% Deviation (RSD)~\cite{everitt-book-stats-2010}.  Note that, if we were
%% to analyze each project in isolation, the standard deviation of a
%% distribution ($\sigma$) would suffice to quantify how dispersed the
%% (time) distribution is.  However, in our case, we would like to be
%% able to compare and summarize dispersion across projects.  The RSD,
%% which is obtained dividing the standard deviation by the mean ($\mu$)
%% of a distribution, provides such normalization effect.  This metric
%% provides a lower bound (zero) but not an upper bound (somewhere close
%% to 1).  The smaller (larger) the value of RSD the more (less) uniform
%% the distribution is.  Consequently, the lower the value of RSD the
%% more parallelizable a test suite should be.

%% \begin{figure}[h!]
%%   \centering
%%   \includegraphics[width=0.5\textwidth]{R/testcost.pdf}  
%%   \caption{\label{fig:relativesd}Distribution of RSD ($\sigma/\mu$)
%%     across projects.}
%% \end{figure}

%% Figure~\ref{fig:relativesd} shows the distribution of RSD across
%% medium and long-running projects.  Results show that the distribution
%% is skewed to the right indicating that test costs are relatively well
%% distributed in most costly projects we analyzed \Fix{$\leftarrow$
%%   confirm}.

%% analyzed the execution time
%% for the \numMedLong{} projects from the \longg{} and \medg{} groups
%% (see Section~\ref{sec:rqA}).
%% For each subject we calculated the
%% relative standard deviation of the test cases: we collected the
%% elapsed time of each individual test, calculated the standard
%% deviation, and divided by the mean. \Jbc{I need to clarify the
%%   relationship "well/bad-balanced" regression test and relative
%%   standard deviation}

%% Results indicated that \Fix{...elaborate...}. \Jbc{We may identify
%% different groups of subjects}\Fix{TODO: collect data + compute the
%% statistic, create a scatter plot to identify groups of subjects}

%% Regression tests that are well distributed may benefit from
%% parallelism since more tests executes at the same time while the
%% opposite scenario may require a different approach. In the later
%% scenario, executing tests in parallel may have insignificant impact
%% since a small subset of test cases dominates the execution.}


\subsection{Adoption}
\label{sec:rqC}
\label{sec:rqE}

The dimension adoption focuses on the usage of parallelism in
open-source projects.  It evaluates (\numRQAdoptionOne) how often open-source
projects use parallelization schemes and (\numRQAdoptionTwo) how developers
involved in costly projects, not using parallelization, perceive this
technology.

\begin{itemize}
    \item \numRQAdoptionOne. \textbf{\RQAdoptionOne{}}
\end{itemize}

To answer \numRQAdoptionOne{}, we selected all projects from the \medg{} and
\longg{} groups, \ie, projects that ran in at least one minute.  This
set includes \numMedLong{} projects (see Section~\ref{sec:rqA}).  We
looked for dynamic and static manifestations of parallelism.

%% The
%% following section report results for each of these cases.

\vspace{1ex}
\subsubsection{Dynamic checking}
\label{sec:rqC-1}

To find dynamic evidence of parallelism, we ran the test suites from
our set of \numMedLong{} projects to output all key-value pairs of
Maven parameters.  To that end, we used the option~\CodeIn{-X} to
produce debug output and the option~\CodeIn{-DskipTests} to skip
execution of tests.  We skipped execution of tests as we observed from
sampling that only bootstrapping the Maven process suffices to infer
which parallel configuration modes it will use to actually run the
tests.  It is also important to point that we used the default
configurations specified in the project.  We inferred parallel
configurations by searching for certain configuration parameters in
log files. According to Maven's
documentation~\cite{maven-surefire-plugin}, a parallel configuration
depends either on (1) the parameter \CodeIn{parallel} to define the
parallelism mode within a JVM followed by the parameter
\CodeIn{threadCount} or (2) the parameter
\CodeIn{forkCount}\footnote{This parameter is named \CodeIn{forkMode}
  in old versions of Maven Surefire} to define the number of forked
JVMs.  As such, we captured, for each project, all related key-value
pairs of Maven parameters and mapped those pairs to one of the
possible parallelization modes.  For instance, if a given project
contains a module with the parameter
\CodeIn{<forkCount>1C</forkCount>}, the possible classifications are
\ForkSeq{} or \ForkParMeth{}, depending on the presence and the value
of the parameter \CodeIn{parallel}.  If the parameter
\CodeIn{parallel} is set to \CodeIn{methods} the detected mode will be
\ForkParMeth{}.  Large projects may contain several test suites
distributed on different Maven modules potentially using different
configurations.  For those cases, we collected the Maven output from
each module discarding duplicates as to avoid inflating counts for
configuration modes that appear in several modules of the same
project. For instance, if a project contains two modules using the
same configuration, we counted only one occurrence.


\begin{wrapfigure}{r}[0pt]{0pt}%0.525\linewidth
  \footnotesize
  %  \small
  \centering
  \setlength{\tabcolsep}{2.5pt}
%    \resizebox{.48\textwidth}{!}{%
    \begin{tabular}{lrr}
        \toprule
        \emph{Subject} & \emph{\# of modules} & \emph{Mode}\\%
        \midrule%
        \Comment{BounceStorage }Chaos\Comment{ HTTP Proxy} & 1/1 &  \ParClassSeqMeth{}\\%
        \Comment{Apache }Flink & 74/74 & \ForkSeq{} \\%        
        \Comment{JenkinsCI }Gerrit\Comment{ Trigger Plugin} & 1/1 & \ForkSeq{}\\%
        \Comment{Spotify }Helios & 8/8 & \ForkSeq{}\\%
        Javaslang & 3/3 & \ParClassParMeth{}\\%
        Jcabi\Comment{ Github} & 1/1 & \ParClassParMeth{}\\%        
        \Comment{Hazelcast }Jet & 7/14 & \ForkSeq{}\\%
        \Comment{Apache Logging }Log4J2 & 25/31 & \ForkSeq{}\\%
        \Comment{Jankotek }MapDB & 1/2 & \ParClassParMeth{}\\%        
        \bottomrule%
    \end{tabular}
    \caption{Subjects with parallel test execution enabled by
    default.}
    \label{tab:freqmodes-dynamic}
\end{wrapfigure}
Figure~\ref{tab:freqmodes-dynamic} shows the projects we idendified
where parallelism is enabled by default in Maven.  Column
``\emph{Subject}'' indicates the name of the project, column
``\emph{\# of modules}'' indicates the fraction of modules containing
tests that use the configuration of parallelism mentioned in column
``\emph{Mode}''.  We note that, considering these projects, the
modules that do not use the configuration cited use the sequential
configuration \Seq{}.  For example, six modules (=31-25) from Log4J2
use sequential configuration.

It came as a surprise the observation that
no project used distinct configurations in their modules. Considering
our set of \numMedLong{} projects, we found that only
\textbf{\numProjectsPar{}} of those projects had parallelism enabled
by default, with only configurations \ParClassSeqMeth{},
\ParClassParMeth{}, and \ForkSeq{} being used.  Configurations
\ParClassParMeth{} and \ForkSeq{} were the most popular among these
cases.  Note that these results under-approximate real usage of
parallelism as we used default parameters in our scripts to spawn the
Maven process.  That decision could prevent execution of particular
test modules.
%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.32\textwidth]{plots/barplot-modes-dynamic.pdf}
%    \caption{\label{fig:freqmodes-dynamic}\Fix{fix
%    caption}Distribution of parallel modes identified dynamically in a
%    subset of \numProjectsPar{} projects.  A project may have support
%    to more than one parallel mode. Also, a project may run only a
%    subset tests in parallel by default.}
%\end{figure}

%% Recall that some projects can use parallel execution that is only
%% activated when developers pass certain parameters to the build
%% process. For instance, it is possible to create in Maven multiple
%% configurations in the same build file and select dynamically which one
%% should be used.  

\subsubsection{Static checking}
\label{sec:rqC-2}
Given the inherent limitation of dynamic monitoring to find evidence
of parallelism, we also looked for indication of parallelism in build
files\Comment{ in the same sample set of \numMedLong{} projects}.  We
parsed all \emph{pom.xml} files under the project's directory and used
the same approach as in our previous analysis to classify
configurations.  We noticed initially that our approach was unable to
infer the configuration mode for cases where the decision depends on
the input (\eg,
\CodeIn{<parallel>\$\{parallel.type\}</parallel>}). For these
projects, the tester needs to provide additional parameters in the
command line to enable parallelization (\eg, \CodeIn{mvn test
  -Dparallel.type=classesAndMethods}). To handle those case, we
considered all possible values for the parameter (in this case,
\CodeIn{\$\{parallel.type\}}).  It is also important to note that this
approach is not immune to false negatives, which can ocurr when
\emph{pom.xml} files are encapsulated in jar files or downloaded from
the network.  Consequently, this static checking is complementary to
the dynamic checking, previously presented.

Overall, we found, using this methodology, ten projects that use
parallelism.  Compared to the set of projects listed in
Figure~\ref{tab:speedup}, we found two new projects, namely:
\CodeIn{Google Cloud\Comment{ Platform} DataflowJavaSDK} (using
configuration C3) and \CodeIn{Mapstruct} (using configuration
\ForkSeq{}).  Curiously, we also found that project \CodeIn{Jcabi} was
not detected using this methodology.  That happened because this
project loads its \emph{pom.xml} file from a jar file that we did not
check.  Considering the static and dynamic methods together, we found
a total of 11 distinct projects using parallelism, corresponding to
the union of the two subject sets.

\vspace{1ex}
\begin{center}
\fbox{
  \begin{minipage}{8cm}
      \textit{Answering \numRQAdoptionOne{}:}~\emph{Results indicate that test
        suite parallelization is underused.  Overall, only
        \percentParallel{} of costly projects (11 out of \numMedLong)
        use parallelism.}
  \end{minipage}
}
\end{center}
\vspace{1ex}

%False positive can happen because of comments, for instance.  
%To eliminate the cases of false positives and also to categorize 
%true positive cases, we complemented the initial mining step with a 
%manual inspection of files.
%% settings); the second step (inspection) consists in a manual
%% inspection to confirm the presence of parallelism settings in the
%% build file and classify them according to the parallelism level.
%% Figure \Fix{removed} describes the discovery step: we list the paths
%% of all build files and filter only the files that contain any of the

%% Figure~\ref{tab:inspection-table} summarizes our results.
%% \Fix{The first column indicates the group of projects according to
%% their time cost.  The second column indicates the number of build
%% files per group.  The last column indicates the ratio of projects with
%% parallelization settings.  From the \numMedLong{} subjects, we found
%% \pomMedLong{} \pomf{} files.  The \numPomMatched{} configurations are
%% distributed across \numProjectsPar{} projects from our sample.}

%% % \emph{From these results we found that $\sim$51\% of medium and
%% % long-running projects do not use parallel features to run test
%% % suites.}\Mar{please make it consistent with research
%% % question}\Mar{explain this is over(under)-estimated...}
%% \begin{figure}[ht!]
%%     \centering
%%     \resizebox{.48\textwidth}{!}{%
%%     \begin{tabular}{llcl}
%%         \toprule
%%         Group & Subject & \# of modules & Mode\\%
%%         \midrule%
%%         Long   &JenkinsCI Gerrit Trigger Plugin& 1 & \ForkSeq\\%
%%         Medium &Bouncestorage Chaos Http Proxy & 1 & C2\\%
%%         Medium &Javaslang & 1 & C3\\%
%%         Medium &Apache Flink & 1 &\ForkSeq\\%
%%         Medium &Apache Logging Log4J2 & 3 & \ForkSeq{}\\%
%%         Medium &Google Cloud Platform DataflowJavaSDK & 1 & C3\\%
%%         Medium &Hazelcast Jet & 1 & \ForkSeq\\%
%%         Medium &Jankotek MapDB & 1 & C3\\%
%%         Medium &Mapstruct & 1 & \ForkSeq\\%
%%         Medium &Spotify Helios & 3 & \ForkSeq\\%
%%         \bottomrule%
%%     \end{tabular}}
%%     \caption{Subjects with parallelization configurations in build files.}
%%     \label{tab:inspection-table}
%% \end{figure}

%% \begin{figure}[ht!]
%%     \centering
%%     \begin{tabular*}{0.48\textwidth}{@{\extracolsep{\fill}}ccc}
%%         \toprule
%%         \multirow{2}{*}{Group} %1st row, 1st cell
%%             & \multirow{2}{*}{\# \pomf{}}
%% 	    & \# \pomf{} matched\\
%%         % 2nd row - empty cell
%%             & % empty cell
%%             & / total\\%
%%         \midrule%
%% 	Long   & \numPomLong{} & 4 / \numLong{}\\%
%% 	Medium & \numPomMed{} & 6 / \numMed{}\\%
%%         \midrule%
%%         Total % last row, first cell
%%             & \pomMedLong{}
%%             & \numProjectsPar{} / \numMedLong{}\\%
%%         \bottomrule%
%%     \end{tabular*}
%%     \caption{Presence of parallelization settings in build files: the
%%     first column indicates the group of projects according to their
%%     time cost; the second column is the subset of files with parallelization
%%     keywords; the last column indicates the ratio of projects with
%%     parallelism support.}
%%     \label{tab:inspection-table} 
%% \end{figure}
%% \Jbc{rework this... $\rightarrow$} From the \numProjectsPar{} projects
%% identified above, we investigated further the \numPomMatched{}
%% build files with parallel settings.  We analyzed the support and
%% distribution of parallel modes from this subset of projects. To
%% calculate the distribution of parallel modes, we considered only the
%% presence of the mode in at least one of the project settings.  Recall
%% that a build file may contain more than one parallel setting and a
%% project may contain several sub-modules with build files.  In case the
%% value of a parallel option is resolved dynamically (\eg, via
%% command-line argument or system variable) we compute all modes related
%% to the option. For instance, depending on the value, the
%% \CodeIn{parallel} option can be \Seq{} (\CodeIn{none}),
%% \ParClassSeqMeth{} (\CodeIn{classes}), \SeqClassParMeth{},
%% (\CodeIn{methods}), and \ParClassParMeth{} (\CodeIn{all}).
%% Figure~\Fix{fig:freqmodes-static} summarizes our findings.
%% \Fix{Missing conclusion: Fork the most used configuration}
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.32\textwidth]{plots/barplot-modes-static.pdf}
%% 	\caption{\label{fig:freqmodes-static}\Luis{This is wrong, it
%% 	should be \textbf{CF0} instead of \textbf{CL0}}Distribution of parallel modes
%%     identified statically in a subset of \numProjectsPar{} projects.
%%     A project may have support to more than one parallel mode.}
%% \end{figure}


\begin{itemize}
	\item \numRQAdoptionTwo{}. \textbf{\RQAdoptionTwo{}}
\end{itemize}

To answer this research question we surveyed developers involved in a
selection of projects from our benchmark with time-consuming test
suites.  The goal of the survey is to better comprehend developer's
attitude towards the use of parallelism as a mechanism to speedup
regression testing.  We surveyed developers from a total of
\emailsProjects{} projects.  From the initial list of \numMedLong{}
project, we discarded 11 projects that we knew a priori used
parallelization, and \discartedProjects{} projects that we could not find
developer's emails from commit logs.  From this list of projects, we
mined potential participants for our study.  More precisely, we
searched for developer's name and email from the last 20 commits to
the corresponding project repository.  Using this approach, we
identified a total of \emailsSent{} eligible participants.  Finally,
we sent plain-text e-mails, containing the survey, to those developers.  In
total, \emailsAnswered{} developers replied but we discarded
\emailsFalseAnswers{} replies with subjective answers.  Considering
projects covered by the answers, a total of \emailsProjectsAnswered{}
projects (\percEmailsProjectsAnswered{} of the total) were represented
in those replies.  Note that multiple developers on each project
received emails.  We sent the following set of questions to
developers:

\begin{enumerate}
\item How long does it take for tests to run in your environment? Can
  you briefly define your setup?
\item Do you confirm that your regression test suite does *not* run in parallel?
\item\label{questionThree} Select a reason for not using parallelization:
  \begin{enumerate}[label=\alph*)]
  \item I did not know it was possible
  \item I was concerned with concurrency issues
  \item I use a continuous integration server
  \item Some other reason. Please elaborate.
  \end{enumerate}
\end{enumerate}

%% \begin{enumerate}
%% 	\item How long does it take for test to run in your
%%		environment?
%%	\item Can you briefly define your setup?
%%	\item Do you confirm that your project does not run in
%%		parallel?
%%	\item Select a reason for not using paralellization:
%%		\begin{enumarate}
%%			\item I did not know it was possible;
%%			\item I was concerned with concurrency issues;
%%			\item I use a continuous integration server;
%%			\item Some other reason.
%%		\end{enumerate}
%% \end{enumerate}
%% One of the goals of the first questions is to identify potential
%% discrepancies between our experimental environment and the environment
%% of developers.  Overall, we found that \Fix{...}

Considering question 1, we confirmed that execution time was
compatible with the results we reported in Section~\ref{sec:rqA}.
Furthermore, \emailsCI{} of the participants indicated the use of
Continuous Integration (CI) to run tests, with \emailsDistributed{} of
these participants reporting that test suites are modularized and
those modules are tested independetly in CI servers through different
parameters.  Those participants argumented that such practice helps to
reduce time to observe test failures, which is the goal of speeding up
regression testing.  A total of \emailsLocal{} participants answered
that they do run tests in their local machines.  Note, however, that
CI does not preclude low-level parallelization.  For example,
installations of open-source CI tools (\eg{}, Jenkins~\cite{jenkins})
in dedicated servers would benefit from running tests faster through
low-level test suite parallelization.

% \emailsNotDescribed{} developers did not described their environment.

Considering question 2, the answers we collected indicated, to our
surprise, that six of the \emailsProjectsAnswered{} projects execute
tests in parallel.  This mismatch is justified by cases where neither
of our checks (static or dynamic) could detect presence of
parallelism.  A closer look at these projects revealed that one of
them contained a \emph{pom.xml} file encapsulated in a jar file
(similar case as reported in Section~\ref{sec:rqC-2}), in one of the
projects the particpant considered that distributed CI was a form of
parallelism, and in four projects the team preferred to implement
parallelization instead of using existing features from the testing
framework and the build system~---~in two projects the team
implemented concurrency control with custom JUnit test runners and in
two other projects the team implemented concurrency within test
methods.  Note that, considering these four extra cases (ignored two
distributed CI cases), the usage of parallelization increases from
\percentParallel{} to \percentParallelUpdated{}.  We do not consider
this change significant enough to modify our conclusion about
practical adoption of parallelization (\numRQAdoptionOne{}).

%% , one runs a manually created Thread to run some
%% tests, and the other runs in parallel by using Java 8 collection
%% streams, that allows the developers to iterate over a list in
%% parallel.

%% did not confirmed, however,
%% the developers confirmed the need of an extra parameter at the command
%% line to execute in parallel.

Considering question 3, the distribution of answers was as follows.  A
total of \emailsA{} of the \emailsProjectsAnswered{} developers who
answered the survey did not know that parallelism was available in
Maven (option ``a''), \emailsB{} of developers mentioned that they did
not use parallelism concerned with possible concurrency issues (option
``b''), \emailsD{} of developers mentioned that continuous integration
services sufficed to provide timely feedback while running only smoke
\begin{wrapfigure}{r}[0pt]{0pt}%0.525\linewidth
    \centering
    \includegraphics[width=.18\textwidth]{plots/survey.pdf}
    \caption{\label{fig:rq5-answers}Summary of developer's answers to
      survey question~\ref{questionThree}.}
\end{wrapfigure}
tests (\ie{}, short-running tests) locally (option ``c'')\Comment{here
  I want to say that they use it for something like "non-blocking
  testing" while developing in a local machine}, and \emailsD{} of
developers who provided an alternative answer (option ``d'') mentioned
that using parallelism was not worth the effort of preparing the test
suites to take advantage of available processing power.  A total of
\emailsNA{} of participants did not answer the last question of the
survey.  The pie chart in Figure~\ref{fig:rq5-answers} 
summarizes the distribution of answers.

\begin{center}
\fbox{
	\begin{minipage}{8cm}
	  \textit{Answering \numRQAdoptionTwo{}:}~\emph{Results suggest that dealing
       with concurrency issues (\ie{}, the extra work to organize test
       suite to safely explore concurrency) was the principal reason
       for developers not investing in parallelism.  Other reasons
       included availability of continuous integration services and
       unfamiliarity with the technology.}
	\end{minipage}
}
\end{center}

\subsection{Speedups}
\label{sec:rqD}

\begin{itemize}
    \item \numRQSpeedupOne{}. \textbf{\RQSpeedupOne}
\end{itemize}

To answer \numRQSpeedupOne{}, we considered the \numProjectsPar{}
subjects from our benchmark that use parallelization \emph{by default}
(see Figure~\ref{tab:freqmodes-dynamic}).  We compared running times
with parallelization~---~configured by project owners~---~and without
parallelization.

%% In those projects, parallelization is active without
%% passing any extra parameters.  Section~\ref{sec:rqC-1} describes in
%% detail the methodology we used to find these subjects.
%and
%Section~\ref{seq:rq6-tradeoffs}, we verified that both
%% executions produce the same outcome to eliminate noise from failing
%% tests.  To compute the speedup, we divide the time obtained in the
%% sequential execution by the time obtained from the default execution.
%% For instance, if a project runs the tests sequentially in $10m$ and
%% the same execution runs in $5m$ with parallelization enabled (default
%% execution), the speedup is two.

Figure~\ref{tab:speedup} summarizes results.  Lines are sorted by
project names.  Columns ``\emph{Group}'' and
``\emph{Name}'' indicate, respectively, the group and the name of the
subject.  Column ``$T_s$'' shows sequential execution time and column
``$T_p$'' shows parallel execution time. Column ``$T_s/T_p$'' shows
speedup or slowdown.  As usual, a ratio above 1x denotes speedup
and a ratio below 1x denotes slowdown.

\begin{figure}[h!]
\centering
\resizebox{.41\textwidth}{!}{%
  \scriptsize
\begin{tabular}{llrrr}
\toprule
\emph{Group} & \emph{Subject} & \multicolumn{1}{c}{$T_s$} & \multicolumn{1}{c}{$T_p$} & $T_s/T_p$ \\%
\midrule%
Medium & \Comment{BounceStorage }Chaos\Comment{ HTTP Proxy} & 1.51m & 1.47m & 
    \cellcolor{lightgray}1.01x\\%
Medium &\Comment{ Apache }Flink& 11.79m & 2.57m & 4.59x\\%
Long &\Comment{ Jenkins CI }Gerrit\Comment{ Plugin} & 51.19m & 40.31m &  1.26x\\%
Medium &\Comment{ Spotify }Helios& 4.46m & 1.63m & 2.73x\\%  
Medium &Javaslang& 2.18m & 1.82m & 1.19x\\%
Medium &Jcabi\Comment{ GitHub} & 2.76m & 0.30m &
    \cellcolor{lightgray}9.2x\\%
Medium &\Comment{ Hazelcast }Jet& 8.26m & 3.67m & 2.25x\\%
Long &\Comment{ Apache }Log4J2& 8.24m & 8.21m & \cellcolor{lightgray}1.00x\\%
Long &\Comment{ Jankotek }MapDB& 10.06m & 8.58m & 1.17x\\%
\midrule
\textbf{average} &  &  &  & \avgSpeedup{}x\\
\bottomrule%
\end{tabular}}
\caption{\label{tab:speedup}Speedup (or slowdown) of parallel
  execution ($T_p$) over sequential execution ($T_s$).  Default
  parallel configuration of Maven is used.  Highest slowdown/speedup
  appears in gray color.}
\end{figure}

Results indicate that, on average, parallel execution was
\avgSpeedup{} times faster compared to sequential execution.  Three
cases worth special attention: \CodeIn{Chaos}, \CodeIn{Jcabi} and
\CodeIn{Log4J2}.  No significant speedup was observed in
\CodeIn{Chaos}, a project with only three test classes, of which one
monopolizes the bulk of test execution time.  This project uses
configuration \ParClassSeqMeth{}, which runs test classes in parallel
and test methods, declared in each class, sequentially.  Consequently,
speedup cannot be obtained as the cost of the single expensive test
class cannot be broken down with the selected configuration.  Although
project \CodeIn{Jcabi} also uses configuration \ParClassSeqMeth{},
results obtained are very different compared to \CodeIn{Chaos}.  The
speedup observed in \CodeIn{Jcabi} was the highest amongst all
projects.  This project contains test classes with a small number of
test methods and several methods in those classes are time-consuming.
As result, the CPUs available for testing are kept occupied for the
most part during test execution.  Finally, we note that parallel
execution in \CodeIn{Log4J2} was innefective.  We found that Maven
invokes several test modules in this project but the test modules that
dominate execution time run sequentially by default.

%% The third, runs parallel configuration in
%% \Fix{80\%} of the project modules, however, the test time is dominated
%% by one of the sequentially running modules.  \Luis{$\leftarrow$ rework
%%   this} \Fix{falar sobre o resultado geral dos speedups - elaborar
%%   menor e maior speedup... acho que so vale a pena discutir quando
%%   tiver conviccao dos 2 casos}

\begin{center}
\fbox{
  \begin{minipage}{8cm}
    \textit{Answering \numRQSpeedupOne{}:}~\emph{Considering the
      machine setup we used, the average speedup observed with default
      configurations of parallelization was \avgSpeedup{}x.}
  \end{minipage}
}
\end{center}

\begin{itemize}
    \item \numRQSpeedupTwo{}. \textbf{\RQSpeedupTwo}
\end{itemize}

\newcommand{\subjectScalability}{MapDB}

This experiment evaluates the impact of making available to the build
system a growing number of CPUs for testing.  For that reason, we used
a more poweful machine. \Fix{...describe machine...}  We focused on
configuration \ForkSeq{}
\begin{wrapfigure}{r}[0pt]{0pt}%0.525\linewidth
    \centering
    \includegraphics[width=.2\textwidth]{R/scalability/scalability.pdf}
    \caption{\label{fig:scalability}Scalability.}
\end{wrapfigure}
as the goal of this experiment is to evaluate the impact of spawning a
growing number of JVMs, running on different CPUs.  Furthremore, we
selected the subject \subjectScalability{} as it represents the case
of a long-running test suite (see Figure~\ref{tab:speedup}) with test
cases distributed across many test classes (\Fix{X} test classes for
\subjectScalability{}).  Note that a test class is the smallest unit
that can be used to spawn a test job on a JVM and that we have no
control over which test classes will be assigned to which JVM that the
build system forks.

Figure~\ref{fig:scalability} shows the reduction of running times as
more CPUs contribute to the execution.  \Fix{elaborate...}

\begin{center}
\fbox{
  \begin{minipage}{8cm}
    \textit{Answering \numRQSpeedupTwo{}:}~\emph{...}
  \end{minipage}
}
\end{center}

\subsection{Issues}
\label{sec:rq6-tradeoffs}

This dimension assesses the impact of using distinct parallel
configurations on test flakiness (negative impact) and speedup
(positive impact).  Furthermore, note that Section~\ref{sec:rqD}
evaluated speedup in isolation, focused on projects configured to use
parallelism by default.

\begin{itemize}
    \item \numRQIssuesOne{}. \textbf{\RQIssuesOne{}}
\end{itemize}


%% The intuition is that efficiency and flakiness are inversely
%% proportional in some cases: if too many tests depends on the state of
%% a single external resource, several tests are likely to fail as the
%% degree of concurrency increases by exploiting maximum CPU usage.

%% Table header macros
\newcommand{\subcolA}{$\Uparrow_\text{speed}$}
\newcommand{\subcolB}{$\%_\text{fail}$}
\newcommand{\colheader}[1]{\multicolumn{2}{c}{\emph{#1}}}
\newcommand{\blankentry}{\entry{-}{-}}
\newcommand{\subcol}{\subcolA{} & \subcolB{}}
\newcommand{\entry}[2]{#1 & #2}

\begin{figure*}[t]
\centering
\small
\begin{tabular}{l|rr|rrrrrrrrrr}
\toprule
\multirow{2}{*}{\emph{Subject (module)}} & \multicolumn{2}{c|}{\emph{\Seq}} &
    \colheader{\SeqClassParMeth} & \colheader{\ParClassSeqMeth} &
    \colheader{\ParClassParMeth} & \colheader{\ForkSeq} &
    \colheader{\ForkParMeth} \\ %\cline{2-12}
    & $T_\text{cost} $ & $\mathit{N}$ & \subcol{} & \subcol{} & \subcol{} & \subcol{}
    & \subcol{}\\%
\midrule%
Jankotek MapDB  & \entry{8.2m}{5324} & \entry{1.52x}{0\%} & \entry{2.73x}{0\%} & \entry{4.82x}{0.05\%}   & \entry{}{} & \entry{}{}\\%
Stripe Java     & \entry{4.3m}{302}  & \entry{4.78x}{6.31\%} & \entry{3.31x}{7.31\%} & \entry{21.50x}{14.95\%} & \entry{}{} & \entry{}{}\\%

Facebook Linkbench    & \entry{4.3m}{98}  & \entry{1.00x}{0\%} & \entry{1.65x}{1.02\%} & \entry{1.59x}{1.02\%} & \entry{}{} & \entry{}{}\\%

AWS SDK Java (\CodeIn{core})  & \entry{3.7m}{847}  & \entry{1.95x}{2.24\%} & \entry{2.47x}{2.77\%} & \entry{3.70x}{4.01\%} & \entry{}{} & \entry{}{}\\%
JCTools (\CodeIn{core})       & \entry{3.6m}{690}  & \entry{4.50x}{0\%} & \entry{3.60x}{0\%} & \entry{18.00x}{0\%} & \entry{}{} & \entry{}{}\\%

GoogleCloud Dataflow Java (\CodeIn{sdk}) & \entry{1.6m}{3345}  & \entry{1.23x}{1.67\%} & \entry{2.67x}{1.05\%} & \entry{0.80x}{5.35\%} & \entry{}{} & \entry{}{}\\%

Javaslang (\CodeIn{core})     & \entry{1.1m}{17513}  & \entry{1.38x}{0\%} & \entry{1.83x}{0\%} & \entry{1.38x}{0\%} & \entry{}{} & \entry{}{}\\%
%%  & \entry{x.xm}{0}  & \entry{}{\%} & \entry{}{\%} & \entry{}{\%} & \entry{}{} & \entry{}{}\\%
%%OpenMRS Core (\CodeIn{api})  & \entry{15.4m}{3436} & \blankentry{}        & \blankentry{}        & \blankentry{} & \entry{1.5x}{0\%} & \entry{1.7x}{0}\\%
%%Apache Flume (\CodeIn{core}) &  \entry{7.7m}{392}  & \blankentry{}        & \blankentry{}        & \blankentry{}       & \entry{0.9x}{0\%} & \blankentry{} \\%
%%Facebook Archive Linkbench   &  \entry{4.5m}{98}   & \entry{1.0x}{0.2\%}  & \entry{1.6x}{1.2\%}       & \entry{1.0x}{0\%}  & \entry{1.7x}{0.5\%}  & \entry{1.7x}{0.2\%}\\%
%%AWS SDK Java (\CodeIn{core}) &  \entry{3.8m}{847}  & \multicolumn{6}{c}{\Fix{requires investigation}} & \entry{2.2x}{0.1\%} & \entry{3.2x}{2.0\%}\\%

%% OpenMRS Core (\CodeIn{api})  & \entry{15.4m}{3436} & \blankentry{}        & \blankentry{}        & \blankentry{} & \entry{1.5x}{0\%} & \entry{1.7x}{0}\\%
%% Jankotek MapDB               &  \entry{9.9m}{5218} & \multicolumn{6}{c}{\cellcolor{lightgray}\emph{JVM Crash}} & \entry{1.5x}{0\%} & \entry{1.7x}{0\%}\\%
%% Apache Flume (\CodeIn{core}) &  \entry{7.7m}{392}  & \blankentry{}        & \blankentry{}        & \blankentry{}       & \entry{0.9x}{0\%} & \blankentry{} \\%
%% Apache Giraph (\CodeIn{core})&  \entry{7.2m}{236}  & \entry{2.1x}{5.1\%}  & \colheader{\cellcolor{lightgray}timeout}   & \entry{1.0x}{0\%} & \entry{1.0x}{0}\% & \colheader{\cellcolor{lightgray}JVM Crash}\\%
%% Facebook Archive Linkbench   &  \entry{4.5m}{98}   & \entry{1.0x}{0.2\%}  & \entry{1.6x}{1.2\%}       & \entry{1.0x}{0\%}  & \entry{1.7x}{0.5\%}  & \entry{1.7x}{0.2\%}\\%
%% Stripe Java                  &  \entry{4.2m}{302}  & \entry{4.2x}{5.2\%}  & \entry{3.5x}{5.7\%}       & \entry{4.2x}{6.3\%} & \entry{1.0x}{0.3\%} & \entry{4.2x}{5.7\%}\\%
%% AWS SDK Java (\CodeIn{core}) &  \entry{3.8m}{847}  & \multicolumn{6}{c}{\Fix{requires investigation}} & \entry{2.2x}{0.1\%} & \entry{3.2x}{2.0\%}\\%

%% Jenkins CI Github            & \entry{Xm}{Z}       & \entry{x}{\%}          & \entry{x}{\%} & \entry{x}{\%} & \entry{x}{\%} & \entry{x}{\%}\\%
%% Jenkins CI Docker Workflow   & \entry{Xm}{Z}       & \entry{x}{\%}          & \entry{x}{\%} & \entry{x}{\%} & \entry{x}{\%} & \entry{x}{\%}\\%
%% Hazelcast                    & \entry{Xm}{Z}       & \entry{x}{\%}          & \entry{x}{\%} & \entry{x}{\%} & \entry{x}{\%} & \entry{x}{\%}\\%

\bottomrule%
\end{tabular}
\caption{\Fix{Tabela de eficiencia e flakiness}}
\label{tab:rq6-table}
\end{figure*}

To answer this research question, we conducted an experiment involving
the six parallel configurations from Section~\ref{sec:modes}  and the
top 10 long-running test modules from our sample set. The rationale
for this selection criteria was to maximize the chances of observing
speedup and flakiness, assuming that long-running tests also have many
tests. Indeed, we confirmed that test suites in these projects contain
at least 236 tests ($1,662.7$ in average). Also, we preferred to compare the
effect of a configuration over a single test module for projects with
multiple test modules.  Recall that large projects may contain several
test modules, and these modules may contain distinct characteristics
that could favor one configuration and not others; therefore, it would
be necessary to check each module individually for a fair comparison.
We are interested in understanding how efficiency (\ie, testing cost)
and flakiness (\ie, failing tests) are affected when we run a test
suite with different parallel configurations.  Recall that increased
resource contention obtained with parallelism can lead to concurrency
issues such as data races.  Flakiness and speedup are contradictory
forces that drive configuration selection.  We used as a \emph{control
group} (\ie, baseline) the sequential execution of each subject's
tests. Notice that for measuring flakiness, we have to consider only
tests that failed due to the concurrency in the parallel execution.
For that reason, we re-executed the tests in sequence ten times and
carefully verified that there are only passing tests in our baseline.
We considered ten as the number of re-executions based on the approach
used by Google reported in a previous work on test
flakiness~\cite{luo-etal-fse2014}.  To obtain parallel configurations,
we implemented a script that takes a subject and a configuration (\eg,
\ForkSeq{}) as inputs, and the script outputs the modified version of
the subject with the desired configuration. The workflow consists in
copying the project directory to a new directory, finding all existing
build files (\ie, \pomf{} files), and modifying all existing Maven
Surefire configurations with new values for the parameters
\CodeIn{parallel}, \CodeIn{forkCount}, and \CodeIn{threadCount} using
an XPath~\cite{xpath} library to manipulate XML documents. For
configurations with forked JVMs enabled (\ie, \ForkSeq{} and
\ForkParMeth{}), we changed the \CodeIn{forkCount} with the value
\CodeIn{1C} (\ie, one JVM per core).  To adjust the pool of threads
for parallelism within a JVM, we changed the parameter
\CodeIn{threadCount} with \Jbc{should we consider 2 as it is the
number of native threads per core OR 6 as it represents 3 Cores * 2
native threads?}.  To run the subjects and their respective variations
(\ie, the modified versions according to the parallelism
configuration), we used a similar approach as described in


Figure~\ref{fig:mvn-execution} except that we added a timeout of one
hour to run the tests. We used the \CodeIn{timeout}
command~\cite{timeout-cmd} to monitor the execution, and we configured
the command to dispatch a \emph{kill} signal if the test execution
exceeds the time limit. We imposed this time constraint to avoid
hanging indefinitely the experiment execution due to some thread
contention that may occur (\eg, deadlock). Finally, we saved each
execution log and XML test reports generated by Maven to collect the
execution time, the number of failing tests, and for reference to
analyze and diagnose outliers in our results. For efficiency, we
reported the speedup (\ie, $\Uparrow_\text{speed} = T_{\text{s}} /
T_{\text{p}}$) in average, and for flakiness, we reported the rate of
failing tests (\ie, $\mathit{\%_\text{fail} = failures / tests}$) in
average.  Figure~\ref{tab:rq6-table} summarizes the obtained results
ordered by the time cost (\ie, $T_\text{cost}$). \Fix{Remember to
explain efficiency cutoff, JVM crashes and timeout}

\Jbc{Lembrar que investigar o slowdown de Dataflow... os logs indicam que varios
dos testes que quebraram tentavam fazer uma autenticacao em algum servico.
Possivelmente o slowdown se deve ao tempo de resposta do servico quando
a autentiticao falha}

%% \begin{figure}[h!]
%% \centering
%% \resizebox{.48\textwidth}{!}{%
%% \begin{tabular}{lcrrrrr}
%% \toprule
%% \emph{Subject} & \emph{\# of tests} & \emph{\SeqClassParMeth{}} & \emph{\ParClassSeqMeth{}} & \emph{\ParClassParMeth{}} & \emph{\ForkSeq{}} & \emph{\ForkParMeth{}}\\%
%% \midrule%
%% Linkedin Pinot & 356 & - & - & - & - & -\\%
%% %% Jenkins CI Github Plugin & - & - & - & - & 0\% & -\\%
%% %% Kite SDK & - & - & - & - & - & -\\%
%% %% \Fix{!?} Apache Giraph & 327 & - & - & - & - & -\\%
%% %% OpenMRS Core & - & - & - & - & - & -\\%
%% %% Jenkins CI Docker Workflow Plugin & - & - & - & - & 0\% & -\\%
%% %% \Fix{Flaky} Apache Eagle & - & - & - & - & - & -\\%
%% %%Geotools & 7701 & - & - & - & - & -\\%
%% %%Kuromoji & 672 & - & - & - & - & -\\%
%% %%Atomix & 99 & - & - & - & - & -\\%
%% %%\Fix{Snazy OHC} & - & - & - & - & - & -\\%
%% %%\Fix{RoaringBitmap} & - & - & - & - & - & -\\%
%% \bottomrule%
%% \end{tabular}}
%% \caption{\Fix{Tabela de flakiness}}
%% \label{tab:rq6-flaky}
%% \end{figure}

\Fix{Elaborate results from efficiency}

\Fix{Elaborate results from flakiness}

\Fix{highlight special cases}

\Fix{Draw conclusions}

\begin{center}
\fbox{
\begin{minipage}{8cm}
\textit{Answering \numRQIssuesOne{}:~\emph{\Jbc{summarize findings...}}}
\end{minipage}
}
\end{center}

%%  LocalWords:  RQ occurence parallelization Tradeoffs API readme th
%%  LocalWords:  mvn clearcut escapeinside xleftmargin untestable LTS
%%  LocalWords:  framexleftmargin CPUs Tahr sysstat gh Vagrantfile
%%  LocalWords:  javadoc isolcpus JUnit's JUnitCore Gligoric boxplots
%%  LocalWords:  outliers apache uber chaperone facebookarchive
%%  LocalWords:  linkbench priori
