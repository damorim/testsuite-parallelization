\section{Evaluation}
\label{sec:eval}

We are interested in understanding the prevalence of time-consuming
test suites and main sources of cost. We want to understand how the
execution cost is distributed on test cases within a test suite and
how developers approach test execution. Based on that, we study
parallelization of testing frameworks.  More precisely, we investigate
how prevalent test parallelization is, the potential for improving
execution cost, issues of flakiness that hinders the use of
parallelization, and how to address those issues.  More specifically,
we pose the following research questions:

\newcommand{\numRQA}{RQ1}
\newcommand{\numRQB}{RQ2}
\newcommand{\numRQC}{RQ3}
\newcommand{\numRQD}{RQ4}
%%\newcommand{\numRQE}{RQ5}
\newcommand{\numRQF}{RQ5}

\newcommand{\RQA}{How prevalent is the occurence of time-consuming
  regression test suites in open-source projects?}
\newcommand{\RQB}{How the time cost is distributed across test cases?}
\newcommand{\RQC}{How often parallelization settings appear in build
  files?}
\newcommand{\RQD}{What is the impact of parallelization?}
%%\newcommand{\RQE}{What factors contribute to improve performance
%%  through parallelization?\Jbc{Previously as "When does
%%    parallelization works and when it does not so great?"}}
\newcommand{\RQF}{What are the limitations of low-level parallelism?}

\begin{itemize}
    \item \textbf{\numRQA.} \RQA
    \item \textbf{\numRQB.} \RQB
    \item \textbf{\numRQC.} \RQC
    \item \textbf{\numRQD.} \RQD
%%    \item \textbf{\numRQE.} \RQE
    \item \textbf{\numRQF.} \RQF
\end{itemize}

%%\newcommand{\RQB}{What is the distribution of CPU and IO bound
%%regression test suites from the sample set?}
%%
%%\newcommand{\RQC}{How uniformly distributed is the execution time
%%across test cases in costly projects?}
%%
%%\newcommand{\RQD}{How often developers use the parallelism features
%%from build systems to improve runtime performance?}

The first research question addresses the prevalence of long-running
test suites. We are interested to know if costly test suites are
common in open-source projects.  The second research question
addresses the relationship of test cases and the overall execution
cost: we are interested to investigate how the time cost is
distributed among test cases.  In the third research question, we are
in understanding if developers are aware of low-level parallelism
features available out-of-the-box through build systems (\eg,
configuring Maven to use JUnit with multiple threads to speedup test
execution). In addition, we want to identify which configurations are
often used and why they more popular (if any).  The fourth research
question addresses the impact of low-level parallelism on the
regression tests from our sample set. For subjects that use
parallelization settings, we are interested to compare the execution
performance when these settings are activated and deactivated (\ie,
tests run sequentially).
%%\Jbc{This RQ has to be reworked:} \Fix{The fifth research question
%%addresses the characteristics of regression tests from the previous
%%experiment.  More specifically, we want to investigate if there is a
%%relation between the balance of test execution (\ie, how uniformly
%%distributed is the execution time across tests cases) and the usage of
%%computational resources (\ie, if tests are mostly CPU or IO intense)
%%that impacts the effectiveness of parallelization}.
Finally, the fifth research question discusses the limitations and
insights to overcome the pitfalls of parallelization.

\Comment{
    \Fix{distribution of execution time per test case. For each subject
    identified in the first research question, we investigate how
    balanced is the cost of the test suite in contrast to the cost of
    test cases and if there are subjects where the time cost is mostly
    dominated by a small fraction of test cases.} \Fix{The third research
    question addresses the distribution of regression tests according
    to the use of computational resources.  We are interested in
    investigating if regression test suites are CPU intensive and if there
    are opportunities to improve performance. The RQ4 addresses}
    \Fix{...elaborate...}

    The rationale is that if the time cost of a regression test is equally
    distributed among test cases, the execution cost could be potentially
    improved by running tests in parallel (in contrast to the scenario
    where only one test case dominates most of the execution time).
}

\subsection{Subjects}
\label{sec:subjects}

We evaluated the characteristics of regression tests in open-source
development from a sample set of Java projects from \github{}.  We are
interested to evaluate non-trivial test suites on popular projects
that are in activity. We used the \github{}'s search
API~\cite{githubsearch} to fetch Java projects according to the
following criteria: (1) the primary language must be Java\footnote{In
case of projects in multiple languages, the \github{} API considers
the predominant language as the primary language.}; (2) the project
has at least 100 stars; (3) latest update on (or after) January 1st,
2016; (4) the \emph{readme} file contains the string \emph{mvn}.

On \github{}, when a user adds a star to a project, s/he demonstrated
appreciation and bookmarked it for later
reference~\cite{github-stars}.  Although there is not a specific range
for the number of stars, our criteria is an estimation to avoid
trivial subjects: we assumed that \github{} users are likely to
demonstrate interest on well-tested projects. The third criteria is a
constraint to skip projects without recent activity. The fourth
criteria is an approximation to find projects with Maven support. The
rationale is that if the string \emph{mvn} exists in the \emph{readme}
file, it may represent a Maven call (\eg, to compile or test the
project). We used Maven as a reference due to its popularity on Java
projects and to automate our evaluation scripts.
Figure~\ref{fig:subject-query} illustrates the query as an HTTP
request.

\begin{figure}[h!]
\centering
\scriptsize
\lstset{
    escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c, numberstyle=\tiny,
    deletekeywords={true}
}
\begin{lstlisting}
https://api.github.com/search/repositories?q=language:java
        +stars:>=100+pushed:>=2016-01-01
        +mvn%20in:readme+sort:stars
\end{lstlisting}
    \caption{\label{fig:subject-query} Query to the \github{} API for
    projects with the following criteria: (1) Java, (2) at least 100
    stars, (3) updated on January 1st, 2016 (or later), (4) contains
    the string \emph{mvn} in the \emph{readme} file. Output is
    paginated in descending order of stars.}
\end{figure}

A Maven project may contain several sub-modules with multiple \pomf{}
files. We considered only projects with a \pomf{} file located in the
root directory.  As of March 25th 2017, our search criteria returned
\SubjectsGithub{} subjects.  From the \SubjectsGithub{} downloaded
projects, \SubjectsGithubNotMaven{} were not Maven projects (or did
not have a \pomf{} in the root directory) and
\SubjectsGithubNotTestable{} were in an untestable revision (\eg,
missing dependencies or incompatible testing environment). To ensure
that our sample set is stable, we retested the remaining subjects to
eliminate flaky tests (total of \SubjectsGithubFlaky{} subjects). Our
final set consists in \numSubjs{} testable subjects.

\subsection{Setup and Replication}
\label{sec:setup}

To run our experiments, we used a Core i7-4790 (3.60 GHz) Intel
processor machine with eight virtual CPUs (four cores with two native
threads each) and 16GB of memory, running Ubuntu 14.04 LTS Trusty Tahr
(64-bit version). Software settings include \Comment{the Linux
\emph{sysstat} package to measure performance, }git to fetch subjects,
Java 8 and Maven 3.3.9 to build and test subjects. Our evaluation
scripts depends on Python 3.4 and Bash to execute and R to analyze the
data. For replication, all source artifacts are publicly available at
\Fix{create gh-pages}, including supporting scripts (\eg, the script
that test subjects and generates the raw data), and the full list of
projects. \Comment{, and a \emph{Vagrantfile} to emulate our hardware
and all software dependencies.}

\subsection{Answering research question \numRQA{}}
\label{sec:rqA}

\begin{itemize}
    \item \emph{\RQA}
\end{itemize}

To evaluate the frequency of time-consuming regression tests, we
considered the \numSubjs{} subjects from Section~\ref{sec:subjects}
and compared the elapsed time to run their tests.
Figure~\ref{fig:mvn-execution} shows the commands used in our script
to test each subject. The main loop (lines 5-11) iterates over the
list of subjects and invokes Maven in separated steps (lines 7-9). To
avoid inflating the measured time we executed Maven in different
steps: we first compiled the source and test files (line 7), made all
dependencies available locally (line 8) and later, we ran the tests in
offline mode (line 9) to bypass package updates. After the execution,
we used a regular expression on the output log to extract the elapsed
time (line 10). Before collecting the time cost, we executed all
subjects and randomly selected 100 logs to inspect and identify
non-related tasks (\eg, \emph{javadoc} generation and static analysis)
to ignore during the experiment (lines 1-3).  Also, measured the
approximated overhead from the build system after skipping non-related
tasks. We identified the generated test reports and compared the
difference between the elapsed time reported from Maven and the sum of
all test cases executed.
To avoid noise from operating system events, we used a dedicated
server remotely via SSH with the operating system running only
essential services (\eg, the SSH server). In addition, we configured
the \CodeIn{isolcpus} option from the Linux Kernel \cite{linux-kernel}
to isolate six virtual CPUs to execute our experiment and the
remaining CPUs to run the operating system. By isolating a set of
virtual CPUs, we prevent context-switching from experiment processes
and OS-related processes.

\input{codes/evaluation}

\Comment{\Jbc{Ensure to describe that: subjects that could not run the tests
due to incompatible testing environment were removed (likely to happen
for subjects that rely on database systems and other external
factors), the exit status for all executions are consistent and I
removed flaky projs}}

We ran each subject's tests three times and grouped them according to
the time cost (in average): tests that run in less than a minute
(\shortg{} group), tests than run in one minute and less than five
minutes (\medg{} group), and tests that run in five (or more) minutes
(\longg{} group). We based our classification from a previous work
\cite{gligoric-etal-issta2015} and added the \medg{} group due to the
variability of the time cost from subjects out of the \shortg{} group.
\Jbc{these results will be updated $\rightarrow$}
Figure~\ref{fig:rq1-boxplot} shows the time cost distribution of each
group: the \shortg{} group is the most right skewed distribution with
outliers closed to the \medg{} group and 50\% of the subjects run in
less than 15 seconds; the median from the \medg{} group is nearly two
minutes and most of the subjects run in less than four minutes; most
of the \longg{} group runs in less than 25 minutes but has outliers
that require more than 50 minutes to execute.
Figure~\ref{fig:rq1-barplot} summarizes the proportion of subjects
from each group.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/barplot-timecost.pdf}
        \caption{\label{fig:rq1-barplot}}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplot-timecost.pdf}
        \caption{\label{fig:rq1-boxplot}}
    \end{subfigure}%
    \caption{(a) Subjects grouped by time cost ($t$): short run ($t <
    1m$), medium run ($1m \le t < 5m$), and long run ($5m \le t$); (b)
    Distribution of time cost by group.}
\end{figure}

\Jbc{these results will be updated $\rightarrow$}
It is important to mention that Figure~\ref{fig:rq1-barplot}
represents a lower bound estimation for time cost because some
projects omit long-running tests on their default execution. For
instance, it is necessary to invoke Maven with additional parameters
to run integration tests from project \CodeIn{apache.maven-surefire}
(nearly 30 min in our environment).  Also, some tests may finish
earlier than expected due to existing bugs on the ongoing revision.
From the \numSubjs{} compiled projects, 286 successfully executed
all tests and 118 reported test failures.

\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQA{}:}~\emph{Given that
    \percentMedLongRunning{} of the projects are costly (\ie,
    \numMedLong{} projects from \medg{} and \longg{}), we concluded
    that time-consuming regression test suites are relatively frequent
    in open-source projects.}
\end{minipage}
}
\end{center}

\subsection{Answering research question \numRQB{}}
\label{sec:rqB}

\begin{itemize}
    \item \emph{\RQB}
\end{itemize}

\Jbc{pending updates! $\rightarrow$}
Section~\ref{sec:rqA} showed that medium and long-running projects are
not uncommon; they account to nearly \percentMedLongRunning{} of the
\numSubjs{} projects we analyzed.  However, parallelism by itself does
not assure speedups on those projects.  One sensible factor that is
important to the effectiveness of low-level parallelism is the
distribution of test costs in the test suite.  In the limit, if cost
is dominated by a single test, it is unlikely that parallelization
will be beneficial as a test method is the smallest working unit in
test frameworks (see Section~\ref{sec:frameworks}).

\begin{figure}[ht]
    \centering

    \begin{subfigure}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{R/testcost-med.pdf}
      \caption{\label{fig:medtcost}Medium}
    \end{subfigure}\\
    
    \begin{subfigure}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{R/testcost-long.pdf}
      \caption{\label{fig:longtcost}Long}
    \end{subfigure}%
    
    \begin{subfigure}{0.5\textwidth}
      \centering
      \begin{tabular}{rrrr}
        \toprule
        & $\sigma\leq1$ & $1<\sigma\leq5$ & $\sigma\ge5$ \\
        \midrule    
        Medium & 17 & 20 & 7 \\
        Long   &  9 & 14 & 5 \\
        \bottomrule
      \end{tabular}
      \caption{\label{fig:sd}...\Fix{Looks like there are missing
        projects...}}    
    \end{subfigure}%

    \caption{\label{fig:time-distributions}Distribution of time cost per project.}    
\end{figure}

\sloppy Figures~\ref{fig:medtcost} and~\ref{fig:longtcost} show the
time distribution of tests per project.  We observed that the average
median value of execution cost for a test was relatively small (dashed
horizontal red lines), namely 0.41s for \medg{} projects and 0.23s for
\longg{} projects.  Note that correlation between cost of test suites
and cost of their individual tests is not to be expected.  The
standard deviation ($\sigma$) associated with each ditribution was
relatively low.  Figure~\ref{fig:sd} shows the number of projects
within specific ranges of $\sigma$ values.  We also observed that
projects with very high standard deviations may or may not be
indicative of monopolization of execution by a small number of tests.
For example, the highest value of $\sigma$ ocurred in project
\CodeIn{wildfly-swarm}, whose test suite was classified as
long-running.  This project contains only 65 tests, 62 of which take
less than 0.5s to run, 1 of which takes nearly 3s to run, and 2 of
which take 40m to run.  This is an example where test suite
parallelization may not be very beneficial as $>$99\% execution cost
is dominated by only 3\% of the tests.  Without these two tests this
project would have been classified as short-running.  A closer
inspection in the data indicates that the project
\CodeIn{wildfly-swarm} was an extreme case; we did not find projects
with similar characteristics of time monopolization.  Project
\CodeIn{jenkinsci.gerrit-trigger-plugin} is also classified as
long-running and has the second highest value of $\sigma$.  For this
project, however, cost is distributed more smoothly across 529 tests,
of which 119 take more than 1s to run.  


\begin{center}
\fbox{
\begin{minipage}{8cm}
    \textit{Answering \numRQB{}:}~\emph{Overall, results indicate that
    projects with a very small number of tests monopolizing end-to-end
    execution time were rare.}
\end{minipage}
}
\end{center}

%% We are interested to know whether
%% most of the execution cost of a subject is dominated by a small subset
%% of test cases or if the cost is nearly equally distributed. 

%% We also evaluated the dispersion of time distributions (one
%% distribution per project) to answer research question \numRQB{}.  To
%% measure dispersion \emph{across} projects we used Relative Standard
%% Deviation (RSD)~\cite{everitt-book-stats-2010}.  Note that, if we were
%% to analyze each project in isolation, the standard deviation of a
%% distribution ($\sigma$) would suffice to quantify how dispersed the
%% (time) distribution is.  However, in our case, we would like to be
%% able to compare and summarize dispersion across projects.  The RSD,
%% which is obtained dividing the standard deviation by the mean ($\mu$)
%% of a distribution, provides such normalization effect.  This metric
%% provides a lower bound (zero) but not an upper bound (somewhere close
%% to 1).  The smaller (larger) the value of RSD the more (less) uniform
%% the distribution is.  Consequently, the lower the value of RSD the
%% more parallelizable a test suite should be.



%% \begin{figure}[h!]
%%   \centering
%%   \includegraphics[width=0.5\textwidth]{R/testcost.pdf}  
%%   \caption{\label{fig:relativesd}Distribution of RSD ($\sigma/\mu$)
%%     across projects.}
%% \end{figure}

%% Figure~\ref{fig:relativesd} shows the distribution of RSD across
%% medium and long-running projects.  Results show that the distribution
%% is skewed to the right indicating that test costs are relatively well
%% distributed in most costly projects we analyzed \Fix{$\leftarrow$
%%   confirm}.

%% analyzed the execution time
%% for the \numMedLong{} projects from the \longg{} and \medg{} groups
%% (see Section~\ref{sec:rqA}).
%% For each subject we calculated the
%% relative standard deviation of the test cases: we collected the
%% elapsed time of each individual test, calculated the standard
%% deviation, and divided by the mean. \Jbc{I need to clarify the
%%   relationship "well/bad-balanced" regression test and relative
%%   standard deviation}

%% Results indicated that \Fix{...elaborate...}. \Jbc{We may identify
%% different groups of subjects}\Fix{TODO: collect data + compute the
%% statistic, create a scatter plot to identify groups of subjects}

%% \Jbc{I shouldn't introduce parallelization arguments here but we have
%% to address this at some point:
%% Regression tests that are well distributed may benefit from
%% parallelism since more tests executes at the same time while the
%% opposite scenario may require a different approach. In the later
%% scenario, executing tests in parallel may have insignificant impact
%% since a small subset of test cases dominates the execution.}

\subsection{Answering research question \numRQC{}}
\label{sec:rqC}

\begin{itemize}
    \item \emph{\RQC}
\end{itemize}

To evaluate the presence of parallelization settings (see Section
\ref{sec:modes}), we considered all projects classified as \medg{} and
\longg{}-running, a total of \numMedLong{} subjects (see
Figure~\ref{fig:rq1-barplot}). To reduce noise we did not consider
projects with \shortg{}-running test suites as we observed that
\percentShortSequential{} of those projects do not use parallelism;
including them would favor sequential execution in our analysis.

To answer \numRQC{} we initially mined build files (\ie, \pomf{}) that
contain parallelization keywords, namely \CodeIn{forkMode},
\CodeIn{forkCount}, and \CodeIn{parallel}.  According to the Maven
surefire documentation\Fix{cite surefire doc}, any configuration to
run tests in parallel must contain one of these keywords. This mining
step, however, is not sound as a matching project may not indicate a
project with a valid configuration for running tests in parallel.
False positive can happen because of comments, for instance.  
To eliminate the cases of false positives and also to categorize true
positive cases, we complemented the initial mining step with a manual
inspection of files.


%% settings); the second step (inspection) consists in a manual
%% inspection to confirm the presence of parallelism settings in the
%% build file and classify them according to the parallelism level.
%% Figure \Fix{removed} describes the discovery step: we list the paths
%% of all build files and filter only the files that contain any of the

Figure~\ref{tab:inspection-table} summarizes our results. The first
column indicates the group of projects according to their time cost.
The second column indicates the number of build files per group.  The
third column is a subset from the previous column and represents the
number of build files found in the mining step (including
false-positives highlighted in parentheses). The last column indicates
the ratio of projects with parallelization settings.

From the \numMedLong{} subjects, we found \pomMedLong{} \pomf{} files
where \numPomMatched{} of those files match one of the keywords
mentioned above.  From these \numPomMatched{} build files, we
eliminated six false-positives: two were names of project modules
(\eg, \emph{camel-cookbook-parallel-processing}), one was part of a
comment, and three were commented configurations. The
\numPomMatchedValid{} build files with valid configurations are
distributed across \numProjectsPar{} projects from our sample.
\emph{From these results we found that $\sim$51\% of medium and
long-running projects do not use parallel features to run test
suites.}\Mar{please make it consistent with research
question}\Mar{explain this is over(under)-estimated...}

\begin{figure}[ht!]
    \centering
    \begin{tabular*}{0.48\textwidth}{@{\extracolsep{\fill}}cccc}
        \toprule
        \multirow{2}{*}{Group} %1st row, 1st cell
            & \multirow{2}{*}{\# \pomf{}}
            & \# \pomf{} matched
            & \# projects matched\\%
        % 2nd row - empty cell
            & % empty cell
            & (false-positives)
            & / total\\%
        \midrule%
        Long   & 1613 & 79 (1) & 19 / \numLong{}\\%
        Medium & 1238 & 30 (5) & 22 / \numMed{}\\%
        \midrule%
        Total % last row, first cell
            & \pomMedLong{}
            & \numPomMatched{} (6)
            & \numProjectsPar{} / \numMedLong{}\\%
        \bottomrule%
    \end{tabular*}
    \caption{Presence of parallelization settings in build files: the
    first column indicates the group of projects according to their
    time cost; the second column indicates the number of build files;
    the third column is the subset of files with parallelization
    keywords; the last column indicates the ratio of projects with
    parallelism support.}
    \label{tab:inspection-table} 
\end{figure}

\subsubsection{Distribution of parallel modes}

From the \numProjectsPar{} projects identified above, we investigated
further the \numPomMatchedValid{} build files with parallel settings.
We analyzed the support and distribution of parallel modes from this
subset of projects. To calculate the distribution of parallel modes,
we considered only the presence of the mode in at least one of the
project settings.  Recall that a build file may contain more than one
parallel setting and a project may contain several sub-modules with
build files.  In case the value of a parallel option is resolved
dynamically (\eg, via command-line argument or system variable) we
compute all modes related to the option. For instance, depending on
the value, the \CodeIn{parallel} option can be \Seq{} (\CodeIn{none}),
\ParClassSeqMeth{} (\CodeIn{classes}), \SeqClassParMeth{},
(\CodeIn{methods}), and \ParClassParMeth{} (\CodeIn{all}).
Figure~\ref{fig:freqmodes} summarizes our findings. \Jbc{Given that we
checked each configuration, we could elaborate how these settings are
used.} \Fix{Missing conclusion: Fork the most used configuration}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.32\textwidth]{plots/parallel-modes.pdf}
    \caption{\label{fig:freqmodes}Distribution of parallel modes
    identified in a subset of \numProjectsPar{} projects (unused modes
    omitted). A single project may have support to more than one
    parallel mode.}
\end{figure}

\subsection{Answering research question \numRQD{}}
\label{sec:rqD}

\begin{itemize}
    \item \emph{\RQD}
\end{itemize}

To answer \numRQD{}, we considered the \numProjectsPar{} subjects with
parallelization settings identified in \numRQC{} and compared their
time cost with sequential execution. \Jbc{should have a sentence to
motivate this RQ} 
\Jbc{explain methodology}.

\Fix{---------------------}

%%To evaluate the distribution of execution time per project, we sorted
%%the test cases by decreasing order of elapsed time and calculated the
%%number of tests executed in 90\% of the total time. Later, we reported
%%the \Fix{balance} of execution time by dividing the number of tests
%%that represents 90\% of the execution time by the number of tests
%%cases. For instance, a balance of 50\% indicates \Fix{...}.  \Fix{We
%%collected the elapsed time from test cases for each generated report.
%%Maven Surefire generates an XML report with execution information
%%(\eg, number of skipped tests and elapsed time) per test suite
%%\Jbc{Should I use the previous sentence as a footnote or should I
%%delete it?}. We noticed that some test cases reported an elapsed time
%%of zero: since the reported time is in milliseconds, some tests may
%%execute in a shorter time. \Fix{..to be continued...}}. Results
%%indicate that \Fix{...}.
%%
%%\begin{figure}[h!]
%%    \centering
%%    \includegraphics[width=0.4\textwidth]{results/plots/balance.pdf}
%%    \caption{\Fix{balance}}
%%\end{figure}

%% \subsection{Answering research question RQ3}
%% \label{sec:rqThree}
%% 
%% \begin{itemize}
%%     \item \RQB
%% \end{itemize}
%% 
%% To evaluate the distribution of CPU and IO intensive test suites from
%% the sample set, we used the command \emph{sar} to monitor the system
%% activity in background while tests ran. \emph{Sar} is a command that
%% collects and reports statistics (\eg, percentage of IO waiting and
%% usage of CPU in user mode) based on the kernel activity and it is
%% highly configurable to collect detailed information (\eg, usage of a
%% specific processor core or percentage of network interface
%% utilization). We configured \emph{sar} to report \Fix{...explain how
%% we executed and what fields we are interested}. \Fix{explain fields}.
%% Figure \Fix{A} shows the distribution of subjects grouped in intervals
%% of \Fix{W}\% of CPU utilization. Results indicates that \Fix{...}
%% 
%% \begin{figure}[h!]
%%     \centering
%%     \includegraphics[width=0.4\textwidth]{results/plots/cpuness.pdf}
%%     \caption{\Fix{cpu usage}}
%% \end{figure}
%% 
%% \Comment{we proposed the definition of \emph{cpuness}
%% computed as the follow: $((user\_t + system\_t) / elapsed\_t) * 100$,
%% where \emph{user\_t} is the elapsed time of execution in \emph{user
%% mode}, \emph{system\_t} is the elapsed in \emph{kernel mode}, and
%% \emph{elapsed\_t} is the elapsed time to finish the execution. We
%% measured the \emph{cpuness} of each regression test \Fix{...elaborate
%% the meaning of cpuness} \Fix{Describe how I measured user, system and
%% "wall" time}.  \Fix{Explain results}.  \Fix{show plots}}
%% 
